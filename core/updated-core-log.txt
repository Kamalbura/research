PROJECT STRUCTURE AND PYTHON FILES LOG
================================================================================
Root Directory: C:\Users\burak\Desktop\research\core
Output File: C:\Users\burak\Desktop\research\core\project_structure_20251009_070230.txt
Generated: 2025-10-09 07:02:30
================================================================================

================================================================================
DIRECTORY TREE STRUCTURE
================================================================================
Root Directory: C:\Users\burak\Desktop\research\core
Generated: 2025-10-09 07:02:30

├── __pycache__/
│   ├── __init__.cpython-311.pyc (290 bytes)
│   ├── __init__.cpython-313.pyc (273 bytes)
│   ├── aead.cpython-311.pyc (18,997 bytes)
│   ├── aead.cpython-313.pyc (14,394 bytes)
│   ├── async_proxy.cpython-311.pyc (57,100 bytes)
│   ├── async_proxy.cpython-313.pyc (48,055 bytes)
│   ├── config.cpython-311.pyc (10,160 bytes)
│   ├── config.cpython-313.pyc (9,306 bytes)
│   ├── handshake.cpython-311.pyc (19,069 bytes)
│   ├── handshake.cpython-313.pyc (17,097 bytes)
│   ├── logging_utils.cpython-311.pyc (6,243 bytes)
│   ├── logging_utils.cpython-313.pyc (5,872 bytes)
│   ├── policy_engine.cpython-311.pyc (11,421 bytes)
│   ├── policy_engine.cpython-313.pyc (9,972 bytes)
│   ├── power_monitor.cpython-313.pyc (49,317 bytes)
│   ├── project_config.cpython-313.pyc (188 bytes)
│   ├── run_proxy.cpython-311.pyc (35,491 bytes)
│   ├── run_proxy.cpython-313.pyc (30,777 bytes)
│   ├── suites.cpython-311.pyc (15,590 bytes)
│   ├── suites.cpython-313.pyc (11,653 bytes)
│   └── temp-file.cpython-313.pyc (20,538 bytes)
├── __init__.py (121 bytes)
├── aead.py (14,240 bytes)
├── async_proxy.py (47,990 bytes)
├── config.py (13,294 bytes)
├── handshake.py (13,635 bytes)
├── logging_utils.py (2,957 bytes)
├── policy_engine.py (7,034 bytes)
├── power_monitor.py (42,867 bytes)
├── project_config.py (168 bytes)
├── project_structure_20251007_183028.txt (195,097 bytes)
├── project_structure_20251009_061346.txt (191,845 bytes)
├── project_structure_20251009_070230.txt (0 bytes)
├── run_proxy.py (27,151 bytes)
├── suites.py (15,926 bytes)
├── temp-file.pyd (18,859 bytes)
└── updated-core-log.txt (192,319 bytes)


================================================================================
PYTHON FILE CONTENTS
================================================================================

Found 11 Python files:
   1. __init__.py
   2. aead.py
   3. async_proxy.py
   4. config.py
   5. handshake.py
   6. logging_utils.py
   7. policy_engine.py
   8. power_monitor.py
   9. project_config.py
  10. run_proxy.py
  11. suites.py

--------------------------------------------------------------------------------

FILE 1/11: __init__.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\__init__.py
Size: 121 bytes
Modified: 2025-09-24 05:23:26
------------------------------------------------------------
"""
PQC Drone-GCS Secure Proxy Core Package.

Provides post-quantum cryptography secure communication components.
"""

============================================================

FILE 2/11: aead.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\aead.py
Size: 14,240 bytes
Modified: 2025-10-09 06:19:22
------------------------------------------------------------
"""
AEAD framing for PQC drone-GCS secure proxy.

Provides authenticated encryption (AES-256-GCM) with wire header bound as AAD,
deterministic 96-bit counter IVs, sliding replay window, and epoch support for rekeys.
"""

import struct
from dataclasses import dataclass
from typing import Optional, Tuple

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
try:
    from cryptography.hazmat.primitives.ciphers.aead import ChaCha20Poly1305
except ImportError:  # pragma: no cover - ChaCha unavailable on very old crypto builds
    ChaCha20Poly1305 = None
from cryptography.exceptions import InvalidTag

try:  # Optional dependency installed in gcs-env for PQC evaluation
    import ascon  # type: ignore
except ImportError:  # pragma: no cover - ASCON not installed
    ascon = None

from .config import CONFIG
from .suites import header_ids_for_suite


# Exception types
class HeaderMismatch(Exception):
    """Header validation failed (version, IDs, or session_id mismatch)."""
    pass


class AeadAuthError(Exception):
    """AEAD authentication failed during decryption."""
    pass


class ReplayError(Exception):
    """Packet replay detected or outside acceptable window."""
    pass


class SequenceOverflow(Exception):
    """Sender sequence space exhausted for current epoch."""
    pass


# Constants
HEADER_STRUCT = "!BBBBB8sQB"
HEADER_LEN = 22
# IV is still logically 12 bytes (1 epoch + 11 seq bytes) but is NO LONGER transmitted on wire.
# Wire format: header(22) || ciphertext+tag
IV_LEN = 0  # length of IV bytes present on wire (0 after optimization)


class _AsconCipher:
    """Wrapper to present ASCON-128 with the cryptography AEAD interface."""

    __slots__ = ("_key",)

    def __init__(self, key: bytes):
        self._key = key

    def encrypt(self, nonce: bytes, data: bytes, aad: bytes) -> bytes:
        return ascon.encrypt(self._key, nonce, aad, data)  # type: ignore[arg-type]

    def decrypt(self, nonce: bytes, data: bytes, aad: bytes) -> bytes:
        plaintext = ascon.decrypt(self._key, nonce, aad, data)  # type: ignore[arg-type]
        if plaintext is None:
            raise InvalidTag("ascon authentication failed")
        return plaintext


def _canonicalize_aead_token(token: str) -> str:
    candidate = token.lower()
    if candidate not in {"aesgcm", "chacha20poly1305", "ascon128"}:
        raise NotImplementedError(f"unknown AEAD token: {token}")
    return candidate


def _instantiate_aead(token: str, key: bytes) -> Tuple[object, int]:
    """Return AEAD primitive and required nonce length for the suite token."""

    normalized = _canonicalize_aead_token(token)

    if normalized == "aesgcm":
        if len(key) != 32:
            raise NotImplementedError("AES-GCM requires 32-byte key material")
        return AESGCM(key), 12

    if normalized == "chacha20poly1305":
        if ChaCha20Poly1305 is None:
            raise NotImplementedError("ChaCha20-Poly1305 not available in cryptography build")
        if len(key) != 32:
            raise NotImplementedError("ChaCha20-Poly1305 requires 32-byte key material")
        return ChaCha20Poly1305(key), 12

    if normalized == "ascon128":
        if ascon is None:
            raise NotImplementedError("ascon module not installed")
        if len(key) < 16:
            raise NotImplementedError("ASCON-128 requires at least 16 bytes of key material")
        return _AsconCipher(key[:16]), 16

    raise NotImplementedError(f"unsupported AEAD token: {token}")


def _build_nonce(epoch: int, seq: int, nonce_len: int) -> bytes:
    base = bytes([epoch & 0xFF]) + seq.to_bytes(11, "big")
    if nonce_len == 12:
        return base
    if nonce_len > 12:
        return base + b"\x00" * (nonce_len - 12)
    raise NotImplementedError("nonce length must be >= 12 bytes")


@dataclass(frozen=True)
class AeadIds:
    kem_id: int
    kem_param: int
    sig_id: int
    sig_param: int

    def __post_init__(self):
        for field_name, value in [("kem_id", self.kem_id), ("kem_param", self.kem_param), 
                                  ("sig_id", self.sig_id), ("sig_param", self.sig_param)]:
            if not isinstance(value, int) or not (0 <= value <= 255):
                raise NotImplementedError(f"{field_name} must be int in range 0-255")


@dataclass
class Sender:
    version: int
    ids: AeadIds
    session_id: bytes
    epoch: int
    key_send: bytes
    aead_token: str = "aesgcm"
    _seq: int = 0

    def __post_init__(self):
        if not isinstance(self.version, int) or self.version != CONFIG["WIRE_VERSION"]:
            raise NotImplementedError(f"version must equal CONFIG WIRE_VERSION ({CONFIG['WIRE_VERSION']})")
        
        if not isinstance(self.ids, AeadIds):
            raise NotImplementedError("ids must be AeadIds instance")
        
        if not isinstance(self.session_id, bytes) or len(self.session_id) != 8:
            raise NotImplementedError("session_id must be exactly 8 bytes")
        
        if not isinstance(self.epoch, int) or not (0 <= self.epoch <= 255):
            raise NotImplementedError("epoch must be int in range 0-255")
        
        if not isinstance(self.key_send, bytes):
            raise NotImplementedError("key_send must be bytes")
        
        if not isinstance(self._seq, int) or self._seq < 0:
            raise NotImplementedError("_seq must be non-negative int")

        self._aead_token = _canonicalize_aead_token(self.aead_token)
        self._cipher, self._nonce_len = _instantiate_aead(self._aead_token, self.key_send)

    @property
    def seq(self):
        """Current sequence number."""
        return self._seq

    def pack_header(self, seq: int) -> bytes:
        """Pack header with given sequence number."""
        if not isinstance(seq, int) or seq < 0:
            raise NotImplementedError("seq must be non-negative int")
        
        return struct.pack(
            HEADER_STRUCT,
            self.version,
            self.ids.kem_id,
            self.ids.kem_param, 
            self.ids.sig_id,
            self.ids.sig_param,
            self.session_id,
            seq,
            self.epoch
        )

    def encrypt(self, plaintext: bytes) -> bytes:
        """Encrypt plaintext returning: header || ciphertext + tag.

        Deterministic IV (epoch||seq) is derived locally and NOT sent on wire to
        reduce overhead (saves 12 bytes per packet). Receiver reconstructs it.
        """
        if not isinstance(plaintext, bytes):
            raise NotImplementedError("plaintext must be bytes")
        
        # Check for sequence overflow - header uses uint64, so check that limit
        # Bug #6 fix: Allow full uint64 range (0 to 2^64-1)
        if self._seq >= 2**64:
            raise SequenceOverflow("packet_seq overflow; rekey/epoch bump required")
        
        # Pack header with current sequence
        header = self.pack_header(self._seq)

        iv = _build_nonce(self.epoch, self._seq, self._nonce_len)

        try:
            ciphertext = self._cipher.encrypt(iv, plaintext, header)
        except Exception as e:
            raise NotImplementedError(f"AEAD encryption failed: {e}")
        
        # Increment sequence on success
        self._seq += 1
        
        # Return optimized wire format: header || ciphertext+tag (IV omitted)
        return header + ciphertext

    def bump_epoch(self) -> None:
        """Increase epoch and reset sequence.

        Safety policy: forbid wrapping 255->0 with the same key to avoid IV reuse.
        Callers should perform a new handshake to rotate keys before wrap.
        """
        if self.epoch == 255:
            raise NotImplementedError("epoch wrap forbidden without rekey; perform handshake to rotate keys")
        self.epoch += 1
        self._seq = 0


@dataclass
class Receiver:
    version: int
    ids: AeadIds
    session_id: bytes
    epoch: int
    key_recv: bytes
    window: int
    strict_mode: bool = False  # True = raise exceptions, False = return None
    aead_token: str = "aesgcm"
    _high: int = -1
    _mask: int = 0

    def __post_init__(self):
        if not isinstance(self.version, int) or self.version != CONFIG["WIRE_VERSION"]:
            raise NotImplementedError(f"version must equal CONFIG WIRE_VERSION ({CONFIG['WIRE_VERSION']})")
        
        if not isinstance(self.ids, AeadIds):
            raise NotImplementedError("ids must be AeadIds instance")
        
        if not isinstance(self.session_id, bytes) or len(self.session_id) != 8:
            raise NotImplementedError("session_id must be exactly 8 bytes")
        
        if not isinstance(self.epoch, int) or not (0 <= self.epoch <= 255):
            raise NotImplementedError("epoch must be int in range 0-255")
        
        if not isinstance(self.key_recv, bytes):
            raise NotImplementedError("key_recv must be bytes")
        
        if not isinstance(self.window, int) or self.window < 64:
            raise NotImplementedError(f"window must be int >= 64")
        
        if not isinstance(self._high, int):
            raise NotImplementedError("_high must be int")
        
        if not isinstance(self._mask, int) or self._mask < 0:
            raise NotImplementedError("_mask must be non-negative int")

        self._aead_token = _canonicalize_aead_token(self.aead_token)
        self._cipher, self._nonce_len = _instantiate_aead(self._aead_token, self.key_recv)
        self._last_error: Optional[str] = None

    def _check_replay(self, seq: int) -> None:
        """Check if sequence number should be accepted (anti-replay)."""
        if seq > self._high:
            # Future packet - shift window forward
            shift = seq - self._high
            if shift >= self.window:
                # Window completely shifts
                self._mask = 1  # Only mark the current position
            else:
                # Partial shift
                self._mask = (self._mask << shift) | 1
                # Mask to window size to prevent overflow
                self._mask &= (1 << self.window) - 1
            self._high = seq
        elif seq > self._high - self.window:
            # Within window - check if already seen
            offset = self._high - seq
            bit_pos = offset
            if self._mask & (1 << bit_pos):
                raise ReplayError(f"duplicate packet seq={seq}")
            # Mark as seen
            self._mask |= (1 << bit_pos)
        else:
            # Too old - outside window
            raise ReplayError(f"packet too old seq={seq}, high={self._high}, window={self.window}")

    def decrypt(self, wire: bytes) -> bytes:
        """Validate header, perform anti-replay, reconstruct IV, decrypt.

        Returns plaintext bytes or None (silent mode) on failure.
        """
        if not isinstance(wire, bytes):
            raise NotImplementedError("wire must be bytes")
        
        if len(wire) < HEADER_LEN:
            raise NotImplementedError("wire too short for header")
        
        # Extract header
        header = wire[:HEADER_LEN]
        
        # Unpack and validate header
        try:
            fields = struct.unpack(HEADER_STRUCT, header)
            version, kem_id, kem_param, sig_id, sig_param, session_id, seq, epoch = fields
        except struct.error as e:
            raise NotImplementedError(f"header unpack failed: {e}")
        
        # Validate header fields
        if version != self.version:
            self._last_error = "header"
            if self.strict_mode:
                raise HeaderMismatch(f"version mismatch: expected {self.version}, got {version}")
            return None
        
        if (kem_id, kem_param, sig_id, sig_param) != (self.ids.kem_id, self.ids.kem_param, self.ids.sig_id, self.ids.sig_param):
            self._last_error = "header"
            if self.strict_mode:
                raise HeaderMismatch(f"crypto ID mismatch")
            return None
        
        if session_id != self.session_id:
            self._last_error = "session"
            return None  # Wrong session - always fail silently for security
        
        if epoch != self.epoch:
            self._last_error = "session"
            return None  # Wrong epoch - always fail silently for rekeying
        
        # Check replay protection
        try:
            self._check_replay(seq)
        except ReplayError:
            self._last_error = "replay"
            if self.strict_mode:
                raise
            return None
        
        # Reconstruct deterministic IV instead of reading from wire
        iv = _build_nonce(epoch, seq, self._nonce_len)
        ciphertext = wire[HEADER_LEN:]
        
        # Decrypt with header as AAD
        try:
            plaintext = self._cipher.decrypt(iv, ciphertext, header)
        except InvalidTag:
            self._last_error = "auth"
            if self.strict_mode:
                raise AeadAuthError("AEAD authentication failed")
            return None
        except Exception as e:
            raise NotImplementedError(f"AEAD decryption failed: {e}")
        self._last_error = None
        return plaintext

    def reset_replay(self) -> None:
        """Clear replay protection state."""
        self._high = -1
        self._mask = 0

    def bump_epoch(self) -> None:
        """Increase epoch and reset replay state.
        
        Safety policy: forbid wrapping 255->0 with the same key to avoid IV reuse.
        Callers should perform a new handshake to rotate keys before wrap.
        """
        if self.epoch == 255:
            raise NotImplementedError("epoch wrap forbidden without rekey; perform handshake to rotate keys")
        self.epoch += 1
        self.reset_replay()

    def last_error_reason(self) -> Optional[str]:
        return getattr(self, "_last_error", None)

============================================================

FILE 3/11: async_proxy.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\async_proxy.py
Size: 47,990 bytes
Modified: 2025-10-09 06:26:41
------------------------------------------------------------
"""
Selectors-based network transport proxy.

Responsibilities:
1. Perform authenticated TCP handshake (PQC KEM + signature) using `core.handshake`.
2. Bridge plaintext UDP <-> encrypted UDP (AEAD framing) both directions.
3. Enforce replay window and per-direction sequence via `core.aead`.

Note: This module uses the low-level `selectors` stdlib facility—not `asyncio`—to
remain dependency-light and fully deterministic for test harnesses. The filename
is retained for backward compatibility; a future refactor may rename it to
`selector_proxy.py` and/or introduce an asyncio variant.
"""

from __future__ import annotations

import hashlib
import json
import queue
import socket
import selectors
import struct
import sys
import threading
import time
from contextlib import contextmanager
from pathlib import Path
from typing import Callable, Dict, Optional, Tuple

from core.config import CONFIG
from core.suites import SUITES, get_suite, header_ids_for_suite, list_suites
try:
    # Optional helper (if you implemented it)
    from core.suites import header_ids_from_names  # type: ignore
except Exception:
    header_ids_from_names = None  # type: ignore

from core.handshake import HandshakeVerifyError, client_drone_handshake, server_gcs_handshake
from core.logging_utils import get_logger

from core.aead import (
    AeadAuthError,
    AeadIds,
    HeaderMismatch,
    Receiver,
    ReplayError,
    Sender,
)

from core.policy_engine import (
    ControlResult,
    ControlState,
    create_control_state,
    handle_control,
    record_rekey_result,
    request_prepare,
)

logger = get_logger("pqc")


class ProxyCounters:
    """Simple counters for proxy statistics."""

    def __init__(self) -> None:
        self.ptx_out = 0      # plaintext packets sent out to app
        self.ptx_in = 0       # plaintext packets received from app
        self.enc_out = 0      # encrypted packets sent to peer
        self.enc_in = 0       # encrypted packets received from peer
        self.drops = 0        # total drops
        # Granular drop reasons
        self.drop_replay = 0
        self.drop_auth = 0
        self.drop_header = 0
        self.drop_session_epoch = 0
        self.drop_other = 0
        self.drop_src_addr = 0
        self.rekeys_ok = 0
        self.rekeys_fail = 0
        self.last_rekey_ms = 0
        self.last_rekey_suite: Optional[str] = None

    def to_dict(self) -> Dict[str, object]:
        return {
            "ptx_out": self.ptx_out,
            "ptx_in": self.ptx_in,
            "enc_out": self.enc_out,
            "enc_in": self.enc_in,
            "drops": self.drops,
            "drop_replay": self.drop_replay,
            "drop_auth": self.drop_auth,
            "drop_header": self.drop_header,
            "drop_session_epoch": self.drop_session_epoch,
            "drop_other": self.drop_other,
            "drop_src_addr": self.drop_src_addr,
            "rekeys_ok": self.rekeys_ok,
            "rekeys_fail": self.rekeys_fail,
            "last_rekey_ms": self.last_rekey_ms,
            "last_rekey_suite": self.last_rekey_suite or "",
        }


def _dscp_to_tos(dscp: Optional[int]) -> Optional[int]:
    """Convert DSCP value to TOS byte for socket options."""
    if dscp is None:
        return None
    try:
        d = int(dscp)
        if 0 <= d <= 63:
            return d << 2  # DSCP occupies high 6 bits of TOS/Traffic Class
    except Exception:
        pass
    return None


def _parse_header_fields(
    expected_version: int,
    aead_ids: AeadIds,
    session_id: bytes,
    wire: bytes,
) -> Tuple[str, Optional[int]]:
    """
    Try to unpack the header and classify the most likely drop reason *without* AEAD work.
    Returns (reason, seq_if_available).
    """
    HEADER_STRUCT = "!BBBBB8sQB"
    HEADER_LEN = struct.calcsize(HEADER_STRUCT)
    if len(wire) < HEADER_LEN:
        return ("header_too_short", None)
    try:
        (version, kem_id, kem_param, sig_id, sig_param, sess, seq, epoch) = struct.unpack(
            HEADER_STRUCT, wire[:HEADER_LEN]
        )
    except struct.error:
        return ("header_unpack_error", None)
    if version != expected_version:
        return ("version_mismatch", seq)
    if (kem_id, kem_param, sig_id, sig_param) != (
        aead_ids.kem_id,
        aead_ids.kem_param,
        aead_ids.sig_id,
        aead_ids.sig_param,
    ):
        return ("crypto_id_mismatch", seq)
    if sess != session_id:
        return ("session_mismatch", seq)
    # If we got here, header matches; any decrypt failure that returns None is auth/tag failure.
    return ("auth_fail_or_replay", seq)


class _TokenBucket:
    """Per-IP rate limiter using token bucket algorithm."""
    def __init__(self, capacity: int, refill_per_sec: float) -> None:
        self.capacity = max(1, capacity)
        self.refill = max(0.01, float(refill_per_sec))
        self.tokens: Dict[str, float] = {}      # ip -> tokens
        self.last: Dict[str, float] = {}        # ip -> last timestamp

    def allow(self, ip: str) -> bool:
        """Check if request from IP should be allowed."""
        now = time.monotonic()
        t = self.tokens.get(ip, self.capacity)
        last = self.last.get(ip, now)
        # refill
        t = min(self.capacity, t + (now - last) * self.refill)
        self.last[ip] = now
        if t >= 1.0:
            t -= 1.0
            self.tokens[ip] = t
            return True
        self.tokens[ip] = t
        return False


def _validate_config(cfg: dict) -> None:
    """Validate required configuration keys are present."""
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX",
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX",
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW",
    ]
    for key in required_keys:
        if key not in cfg:
            raise NotImplementedError(f"CONFIG missing: {key}")


def _perform_handshake(
    role: str,
    suite: dict,
    gcs_sig_secret: Optional[object],
    gcs_sig_public: Optional[bytes],
    cfg: dict,
    stop_after_seconds: Optional[float] = None,
    ready_event: Optional[threading.Event] = None,
) -> Tuple[
    bytes,
    bytes,
    bytes,
    bytes,
    bytes,
    Optional[str],
    Optional[str],
    Tuple[str, int],
]:
    """Perform TCP handshake and return keys, session details, and authenticated peer address."""
    if role == "gcs":
        if gcs_sig_secret is None:
            raise NotImplementedError("GCS signature secret not provided")

        server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_sock.bind(("0.0.0.0", cfg["TCP_HANDSHAKE_PORT"]))
        server_sock.listen(32)

        if ready_event:
            ready_event.set()

        timeout = stop_after_seconds if stop_after_seconds is not None else 30.0
        deadline: Optional[float] = None
        if stop_after_seconds is not None:
            deadline = time.monotonic() + stop_after_seconds

        gate = _TokenBucket(
            cfg.get("HANDSHAKE_RL_BURST", 5),
            cfg.get("HANDSHAKE_RL_REFILL_PER_SEC", 1),
        )

        try:
            try:
                while True:
                    if deadline is not None:
                        remaining = deadline - time.monotonic()
                        if remaining <= 0:
                            raise socket.timeout
                        server_sock.settimeout(max(0.01, remaining))
                    else:
                        server_sock.settimeout(timeout)

                    conn, addr = server_sock.accept()
                    try:
                        ip, _port = addr
                        allowed_ips = {str(cfg["DRONE_HOST"])}
                        allowlist = cfg.get("DRONE_HOST_ALLOWLIST", []) or []
                        if isinstance(allowlist, (list, tuple, set)):
                            for entry in allowlist:
                                allowed_ips.add(str(entry))
                        else:
                            allowed_ips.add(str(allowlist))
                        if ip not in allowed_ips:
                            logger.warning(
                                "Rejected handshake from unauthorized IP",
                                extra={"role": role, "expected": sorted(allowed_ips), "received": ip},
                            )
                            conn.close()
                            continue

                        if not gate.allow(ip):
                            try:
                                conn.settimeout(0.2)
                                conn.sendall(b"\x00")
                            except Exception:
                                pass
                            finally:
                                conn.close()
                            logger.warning(
                                "Handshake rate-limit drop",
                                extra={"role": role, "ip": ip},
                            )
                            continue

                        try:
                            result = server_gcs_handshake(conn, suite, gcs_sig_secret)
                        except HandshakeVerifyError:
                            logger.warning(
                                "Rejected drone handshake with failed authentication",
                                extra={"role": role, "expected": cfg["DRONE_HOST"], "received": ip},
                            )
                            continue
                        # Support either 5-tuple or 7-tuple
                        if len(result) >= 7:
                            k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
                        else:
                            k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                            kem_name = sig_name = None
                        peer_addr = (ip, cfg["UDP_DRONE_RX"])
                        return (
                            k_d2g,
                            k_g2d,
                            nseed_d2g,
                            nseed_g2d,
                            session_id,
                            kem_name,
                            sig_name,
                            peer_addr,
                        )
                    finally:
                        try:
                            conn.close()
                        except Exception:
                            pass
            except socket.timeout:
                raise NotImplementedError("No drone connection received within timeout")
        finally:
            server_sock.close()

    elif role == "drone":
        if gcs_sig_public is None:
            raise NotImplementedError("GCS signature public key not provided")

        client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            client_sock.connect((cfg["GCS_HOST"], cfg["TCP_HANDSHAKE_PORT"]))
            peer_ip, _peer_port = client_sock.getpeername()
            result = client_drone_handshake(client_sock, suite, gcs_sig_public)
            if len(result) >= 7:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
            else:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                kem_name = sig_name = None
            peer_addr = (peer_ip, cfg["UDP_GCS_RX"])
            return (
                k_d2g,
                k_g2d,
                nseed_d2g,
                nseed_g2d,
                session_id,
                kem_name,
                sig_name,
                peer_addr,
            )
        finally:
            client_sock.close()
    else:
        raise ValueError(f"Invalid role: {role}")


@contextmanager
def _setup_sockets(role: str, cfg: dict, *, encrypted_peer: Optional[Tuple[str, int]] = None):
    """Setup and cleanup all UDP sockets for the proxy."""
    sockets = {}
    try:
        if role == "drone":
            # Encrypted socket - receive from GCS
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_DRONE_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind((cfg["DRONE_PLAINTEXT_HOST"], cfg["DRONE_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (no bind needed)
            ptx_out_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sockets["plaintext_out"] = ptx_out_sock

            # Peer addresses
            sockets["encrypted_peer"] = encrypted_peer or (cfg["GCS_HOST"], cfg["UDP_GCS_RX"])
            sockets["plaintext_peer"] = (cfg["DRONE_PLAINTEXT_HOST"], cfg["DRONE_PLAINTEXT_RX"])

        elif role == "gcs":
            # Encrypted socket - receive from Drone
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_GCS_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind((cfg["GCS_PLAINTEXT_HOST"], cfg["GCS_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (no bind needed)
            ptx_out_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sockets["plaintext_out"] = ptx_out_sock

            # Peer addresses
            sockets["encrypted_peer"] = encrypted_peer or (cfg["DRONE_HOST"], cfg["UDP_DRONE_RX"])
            sockets["plaintext_peer"] = (cfg["GCS_PLAINTEXT_HOST"], cfg["GCS_PLAINTEXT_RX"])
        else:
            raise ValueError(f"Invalid role: {role}")

        yield sockets
    finally:
        for sock in list(sockets.values()):
            if isinstance(sock, socket.socket):
                try:
                    sock.close()
                except Exception:
                    pass


def _compute_aead_ids(suite: dict, kem_name: Optional[str], sig_name: Optional[str]) -> AeadIds:
    if kem_name and sig_name and header_ids_from_names:
        ids_tuple = header_ids_from_names(kem_name, sig_name)  # type: ignore
    else:
        ids_tuple = header_ids_for_suite(suite)
    return AeadIds(*ids_tuple)


def _build_sender_receiver(
    role: str,
    ids: AeadIds,
    session_id: bytes,
    k_d2g: bytes,
    k_g2d: bytes,
    cfg: dict,
):
    aead_token = cfg.get("SUITE_AEAD_TOKEN")
    if aead_token is None:
        raise NotImplementedError("SUITE_AEAD_TOKEN missing from proxy config context")

    if role == "drone":
        sender = Sender(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_d2g, aead_token=aead_token)
        receiver = Receiver(
            CONFIG["WIRE_VERSION"],
            ids,
            session_id,
            0,
            k_g2d,
            cfg["REPLAY_WINDOW"],
            aead_token=aead_token,
        )
    else:
        sender = Sender(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_g2d, aead_token=aead_token)
        receiver = Receiver(
            CONFIG["WIRE_VERSION"],
            ids,
            session_id,
            0,
            k_d2g,
            cfg["REPLAY_WINDOW"],
            aead_token=aead_token,
        )
    return sender, receiver


def _launch_manual_console(control_state: ControlState, *, quiet: bool) -> Tuple[threading.Event, Tuple[threading.Thread, ...]]:
    suites_catalog = sorted(list_suites().keys())
    stop_event = threading.Event()

    def status_loop() -> None:
        last_line = ""
        while not stop_event.is_set():
            with control_state.lock:
                state = control_state.state
                suite_id = control_state.current_suite
            line = f"[{state}] {suite_id}"
            if line != last_line and not quiet:
                sys.stderr.write(f"\r{line:<80}")
                sys.stderr.flush()
                last_line = line
            time.sleep(0.5)
        if not quiet:
            sys.stderr.write("\r" + " " * 80 + "\r")
            sys.stderr.flush()

    def operator_loop() -> None:
        if not quiet:
            print("Manual control ready. Type a suite ID, 'list', 'status', or 'quit'.")
        while not stop_event.is_set():
            try:
                line = input("rekey> ")
            except EOFError:
                break
            if line is None:
                continue
            line = line.strip()
            if not line:
                continue
            lowered = line.lower()
            if lowered in {"quit", "exit"}:
                break
            if lowered == "list":
                if not quiet:
                    print("Available suites:")
                    for sid in suites_catalog:
                        print(f"  {sid}")
                continue
            if lowered == "status":
                with control_state.lock:
                    summary = f"state={control_state.state} suite={control_state.current_suite}"
                    if control_state.last_status:
                        summary += f" last_status={control_state.last_status}"
                if not quiet:
                    print(summary)
                continue
            try:
                target_suite = get_suite(line)
                rid = request_prepare(control_state, target_suite["suite_id"])
                if not quiet:
                    print(f"prepare queued for {target_suite['suite_id']} rid={rid}")
            except RuntimeError as exc:
                if not quiet:
                    print(f"Busy: {exc}")
            except Exception as exc:
                if not quiet:
                    print(f"Invalid suite: {exc}")

        stop_event.set()

    status_thread = threading.Thread(target=status_loop, daemon=True)
    operator_thread = threading.Thread(target=operator_loop, daemon=True)
    status_thread.start()
    operator_thread.start()
    return stop_event, (status_thread, operator_thread)


def run_proxy(
    *,
    role: str,
    suite: dict,
    cfg: dict,
    gcs_sig_secret: Optional[object] = None,
    gcs_sig_public: Optional[bytes] = None,
    stop_after_seconds: Optional[float] = None,
    manual_control: bool = False,
    quiet: bool = False,
    ready_event: Optional[threading.Event] = None,
    status_file: Optional[str] = None,
    load_gcs_secret: Optional[Callable[[Dict[str, object]], object]] = None,
    load_gcs_public: Optional[Callable[[Dict[str, object]], bytes]] = None,
) -> Dict[str, object]:
    """
    Start a blocking proxy process for `role` in {"drone","gcs"}.

    Performs the TCP handshake, bridges plaintext/encrypted UDP, and processes
    in-band control messages for rekey negotiation. Returns counters on clean exit.
    """
    if role not in {"drone", "gcs"}:
        raise ValueError(f"Invalid role: {role}")

    _validate_config(cfg)

    cfg = dict(cfg)
    cfg["SUITE_AEAD_TOKEN"] = suite.get("aead_token", "aesgcm")

    counters = ProxyCounters()
    counters_lock = threading.Lock()
    start_time = time.time()

    status_path: Optional[Path] = None
    if status_file:
        status_path = Path(status_file).expanduser()

    def write_status(payload: Dict[str, object]) -> None:
        if status_path is None:
            return
        try:
            status_path.parent.mkdir(parents=True, exist_ok=True)
            tmp_path = status_path.with_suffix(status_path.suffix + ".tmp")
            tmp_path.write_text(json.dumps(payload), encoding="utf-8")
            tmp_path.replace(status_path)
        except Exception as exc:
            logger.warning(
                "Failed to write status file",
                extra={"role": role, "error": str(exc), "path": str(status_path)},
            )

        if role == "drone" and gcs_sig_public is None:
            if load_gcs_public is None:
                raise NotImplementedError("GCS signature public key not provided (provide peer key or loader)")
        gcs_sig_public = load_gcs_public(suite)

    handshake_result = _perform_handshake(
        role, suite, gcs_sig_secret, gcs_sig_public, cfg, stop_after_seconds, ready_event
    )

    (
        k_d2g,
        k_g2d,
        _nseed_d2g,
        _nseed_g2d,
        session_id,
        kem_name,
        sig_name,
        peer_addr,
    ) = handshake_result

    suite_id = suite.get("suite_id")
    if not suite_id:
        try:
            suite_id = next((sid for sid, s in SUITES.items() if dict(s) == suite), "unknown")
        except Exception:
            suite_id = "unknown"

    write_status({
        "status": "handshake_ok",
        "suite": suite_id,
        "session_id": session_id.hex(),
    })

    sess_display = (
        session_id.hex()
        if cfg.get("LOG_SESSION_ID", False)
        else hashlib.sha256(session_id).hexdigest()[:8] + "..."
    )

    logger.info(
        "PQC handshake completed successfully",
        extra={
            "suite_id": suite_id,
            "peer_role": ("drone" if role == "gcs" else "gcs"),
            "session_id": sess_display,
        },
    )

    # Periodically persist counters to the status file while the proxy runs.
    # This allows external automation (scheduler) to observe enc_in/enc_out
    # during long-running experiments without waiting for process exit.
    stop_status_writer = threading.Event()

    def _status_writer() -> None:
        while not stop_status_writer.is_set():
            try:
                with counters_lock:
                    payload = {
                        "status": "running",
                        "suite": suite_id,
                        "counters": counters.to_dict(),
                        "ts_ns": time.time_ns(),
                    }
                write_status(payload)
            except Exception:
                logger.debug("status writer failed", extra={"role": role})
            # sleep with event to allow quick shutdown
            stop_status_writer.wait(1.0)

    status_thread: Optional[threading.Thread] = None
    try:
        status_thread = threading.Thread(target=_status_writer, daemon=True)
        status_thread.start()
    except Exception:
        status_thread = None

    aead_ids = _compute_aead_ids(suite, kem_name, sig_name)
    sender, receiver = _build_sender_receiver(role, aead_ids, session_id, k_d2g, k_g2d, cfg)

    control_state = create_control_state(role, suite_id)
    context_lock = threading.RLock()
    active_context: Dict[str, object] = {
        "suite": suite_id,
        "suite_dict": suite,
        "session_id": session_id,
        "aead_ids": aead_ids,
        "sender": sender,
        "receiver": receiver,
        "peer_addr": peer_addr,
        "peer_match_strict": bool(cfg.get("STRICT_UDP_PEER_MATCH", True)),
    }

    active_rekeys: set[str] = set()
    rekey_guard = threading.Lock()

    if manual_control and role == "gcs" and not cfg.get("ENABLE_PACKET_TYPE"):
        logger.warning("ENABLE_PACKET_TYPE is disabled; control-plane packets may not be processed correctly.")

    manual_stop: Optional[threading.Event] = None
    manual_threads: Tuple[threading.Thread, ...] = ()
    if manual_control and role == "gcs":
        manual_stop, manual_threads = _launch_manual_console(control_state, quiet=quiet)

    def _launch_rekey(target_suite_id: str, rid: str) -> None:
        with rekey_guard:
            if rid in active_rekeys:
                return
            active_rekeys.add(rid)

        logger.info(
            "Control rekey negotiation started",
            extra={"role": role, "suite_id": target_suite_id, "rid": rid},
        )

        def worker() -> None:
            nonlocal gcs_sig_public
            try:
                new_suite = get_suite(target_suite_id)
                new_secret = None
                new_public: Optional[bytes] = None
                if role == "gcs" and load_gcs_secret is not None:
                    try:
                        new_secret = load_gcs_secret(new_suite)
                    except FileNotFoundError as exc:
                        with context_lock:
                            current_suite = active_context["suite"]
                        with counters_lock:
                            counters.rekeys_fail += 1
                        record_rekey_result(control_state, rid, current_suite, success=False)
                        logger.warning(
                            "Control rekey rejected: missing signing secret",
                            extra={
                                "role": role,
                                "suite_id": target_suite_id,
                                "rid": rid,
                                "error": str(exc),
                            },
                        )
                        with rekey_guard:
                            active_rekeys.discard(rid)
                        return
                    except Exception as exc:
                        with context_lock:
                            current_suite = active_context["suite"]
                        with counters_lock:
                            counters.rekeys_fail += 1
                        record_rekey_result(control_state, rid, current_suite, success=False)
                        logger.warning(
                            "Control rekey rejected: signing secret load failed",
                            extra={
                                "role": role,
                                "suite_id": target_suite_id,
                                "rid": rid,
                                "error": str(exc),
                            },
                        )
                        with rekey_guard:
                            active_rekeys.discard(rid)
                        return
            except NotImplementedError as exc:
                with context_lock:
                    current_suite = active_context["suite"]
                with counters_lock:
                    counters.rekeys_fail += 1
                record_rekey_result(control_state, rid, current_suite, success=False)
                logger.warning(
                    "Control rekey rejected: unknown suite",
                    extra={"role": role, "suite_id": target_suite_id, "rid": rid, "error": str(exc)},
                )
                with rekey_guard:
                    active_rekeys.discard(rid)
                return

            if role == "drone" and load_gcs_public is not None:
                try:
                    new_public = load_gcs_public(new_suite)
                except FileNotFoundError as exc:
                    with context_lock:
                        current_suite = active_context["suite"]
                    with counters_lock:
                        counters.rekeys_fail += 1
                    record_rekey_result(control_state, rid, current_suite, success=False)
                    logger.warning(
                        "Control rekey rejected: missing signing public key",
                        extra={
                            "role": role,
                            "suite_id": target_suite_id,
                            "rid": rid,
                            "error": str(exc),
                        },
                    )
                    with rekey_guard:
                        active_rekeys.discard(rid)
                    return
                except Exception as exc:
                    with context_lock:
                        current_suite = active_context["suite"]
                    with counters_lock:
                        counters.rekeys_fail += 1
                    record_rekey_result(control_state, rid, current_suite, success=False)
                    logger.warning(
                        "Control rekey rejected: signing public key load failed",
                        extra={
                            "role": role,
                            "suite_id": target_suite_id,
                            "rid": rid,
                            "error": str(exc),
                        },
                    )
                    with rekey_guard:
                        active_rekeys.discard(rid)
                    return

            prev_token: Optional[str] = cfg.get("SUITE_AEAD_TOKEN")
            try:
                timeout = cfg.get("REKEY_HANDSHAKE_TIMEOUT", 20.0)
                if role == "gcs" and new_secret is not None:
                    base_secret = new_secret
                else:
                    base_secret = gcs_sig_secret
                public_key = new_public if new_public is not None else gcs_sig_public
                if role == "drone" and public_key is None:
                    raise NotImplementedError("GCS public key not available for rekey")
                rk_result = _perform_handshake(role, new_suite, base_secret, public_key, cfg, timeout)
                (
                    new_k_d2g,
                    new_k_g2d,
                    _nd1,
                    _nd2,
                    new_session_id,
                    new_kem_name,
                    new_sig_name,
                    new_peer_addr,
                ) = rk_result

                cfg["SUITE_AEAD_TOKEN"] = new_suite.get("aead_token", "aesgcm")
                new_ids = _compute_aead_ids(new_suite, new_kem_name, new_sig_name)
                new_sender, new_receiver = _build_sender_receiver(
                    role, new_ids, new_session_id, new_k_d2g, new_k_g2d, cfg
                )

                with context_lock:
                    active_context.update(
                        {
                            "sender": new_sender,
                            "receiver": new_receiver,
                            "session_id": new_session_id,
                            "aead_ids": new_ids,
                            "suite": new_suite["suite_id"],
                            "suite_dict": new_suite,
                            "peer_addr": new_peer_addr,
                        }
                    )
                    sockets["encrypted_peer"] = new_peer_addr

                with counters_lock:
                    counters.rekeys_ok += 1
                    counters.last_rekey_ms = int(time.time() * 1000)
                    counters.last_rekey_suite = new_suite["suite_id"]
                if role == "drone" and new_public is not None:
                    gcs_sig_public = new_public
                record_rekey_result(control_state, rid, new_suite["suite_id"], success=True)
                write_status(
                    {
                        "status": "rekey_ok",
                        "new_suite": new_suite["suite_id"],
                        "session_id": new_session_id.hex(),
                    }
                )
                new_sess_display = (
                    new_session_id.hex()
                    if cfg.get("LOG_SESSION_ID", False)
                    else hashlib.sha256(new_session_id).hexdigest()[:8] + "..."
                )
                logger.info(
                    "Control rekey successful",
                    extra={
                        "role": role,
                        "suite_id": new_suite["suite_id"],
                        "rid": rid,
                        "session_id": new_sess_display,
                    },
                )
            except Exception as exc:
                if prev_token is not None:
                    cfg["SUITE_AEAD_TOKEN"] = prev_token
                with context_lock:
                    current_suite = active_context["suite"]
                with counters_lock:
                    counters.rekeys_fail += 1
                record_rekey_result(control_state, rid, current_suite, success=False)
                logger.warning(
                    "Control rekey failed",
                    extra={"role": role, "suite_id": target_suite_id, "rid": rid, "error": str(exc)},
                )
            finally:
                with rekey_guard:
                    active_rekeys.discard(rid)

        threading.Thread(target=worker, daemon=True).start()

    with _setup_sockets(role, cfg, encrypted_peer=peer_addr) as sockets:
        selector = selectors.DefaultSelector()
        selector.register(sockets["encrypted"], selectors.EVENT_READ, data="encrypted")
        selector.register(sockets["plaintext_in"], selectors.EVENT_READ, data="plaintext_in")

        def send_control(payload: dict) -> None:
            body = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
            frame = b"\x02" + body
            with context_lock:
                current_sender = active_context["sender"]
            try:
                wire = current_sender.encrypt(frame)
            except Exception as exc:
                counters.drops += 1
                counters.drop_other += 1
                logger.warning("Failed to encrypt control payload", extra={"role": role, "error": str(exc)})
                return
            try:
                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                counters.enc_out += 1
            except socket.error as exc:
                counters.drops += 1
                counters.drop_other += 1
                logger.warning("Failed to send control payload", extra={"role": role, "error": str(exc)})

        try:
            while True:
                if stop_after_seconds is not None and (time.time() - start_time) >= stop_after_seconds:
                    break

                while True:
                    try:
                        control_payload = control_state.outbox.get_nowait()
                    except queue.Empty:
                        break
                    send_control(control_payload)

                events = selector.select(timeout=0.1)
                for key, _mask in events:
                    sock = key.fileobj
                    data_type = key.data

                    if data_type == "plaintext_in":
                        try:
                            payload, _addr = sock.recvfrom(16384)
                            if not payload:
                                continue
                            with counters_lock:
                                counters.ptx_in += 1

                            payload_out = (b"\x01" + payload) if cfg.get("ENABLE_PACKET_TYPE") else payload
                            with context_lock:
                                current_sender = active_context["sender"]
                            wire = current_sender.encrypt(payload_out)

                            try:
                                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                                with counters_lock:
                                    counters.enc_out += 1
                            except socket.error:
                                with counters_lock:
                                    counters.drops += 1
                        except socket.error:
                            continue

                    elif data_type == "encrypted":
                        try:
                            wire, addr = sock.recvfrom(16384)
                            if not wire:
                                continue

                            with context_lock:
                                current_receiver = active_context["receiver"]
                                expected_peer = active_context.get("peer_addr")
                                strict_match = bool(active_context.get("peer_match_strict", True))

                            src_ip, src_port = addr
                            if expected_peer is not None:
                                exp_ip, exp_port = expected_peer  # type: ignore[misc]
                                mismatch = False
                                if strict_match:
                                    mismatch = src_ip != exp_ip or src_port != exp_port
                                else:
                                    mismatch = src_ip != exp_ip
                                if mismatch:
                                    with counters_lock:
                                        counters.drops += 1
                                        counters.drop_src_addr += 1
                                    logger.debug(
                                        "Dropped encrypted packet from unauthorized source",
                                        extra={"role": role, "expected": expected_peer, "received": addr},
                                    )
                                    continue

                            with counters_lock:
                                counters.enc_in += 1

                            try:
                                plaintext = current_receiver.decrypt(wire)
                                if plaintext is None:
                                    with counters_lock:
                                        counters.drops += 1
                                        last_reason = current_receiver.last_error_reason()
                                        # Bug #7 fix: Proper error classification without redundancy
                                        if last_reason == "auth":
                                            counters.drop_auth += 1
                                        elif last_reason == "header":
                                            counters.drop_header += 1
                                        elif last_reason == "replay":
                                            counters.drop_replay += 1
                                        elif last_reason == "session":
                                            counters.drop_session_epoch += 1
                                        elif last_reason is None or last_reason == "unknown":
                                            # Only parse header if receiver didn't classify it
                                            reason, _seq = _parse_header_fields(
                                                CONFIG["WIRE_VERSION"],
                                                current_receiver.ids,
                                                current_receiver.session_id,
                                                wire,
                                            )
                                            if reason in (
                                                "version_mismatch",
                                                "crypto_id_mismatch",
                                                "header_too_short",
                                                "header_unpack_error",
                                            ):
                                                counters.drop_header += 1
                                            elif reason == "session_mismatch":
                                                counters.drop_session_epoch += 1
                                            elif reason == "auth_fail_or_replay":
                                                counters.drop_auth += 1
                                            else:
                                                counters.drop_other += 1
                                        else:
                                            # Unrecognized last_reason value
                                            counters.drop_other += 1
                                    continue
                            except ReplayError:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_replay += 1
                                continue
                            except HeaderMismatch:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_header += 1
                                continue
                            except AeadAuthError:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_auth += 1
                                continue
                            except NotImplementedError as exc:
                                with counters_lock:
                                    counters.drops += 1
                                    reason, _seq = _parse_header_fields(
                                        CONFIG["WIRE_VERSION"], current_receiver.ids, current_receiver.session_id, wire
                                    )
                                    if reason in (
                                        "version_mismatch",
                                        "crypto_id_mismatch",
                                        "header_too_short",
                                        "header_unpack_error",
                                    ):
                                        counters.drop_header += 1
                                    elif reason == "session_mismatch":
                                        counters.drop_session_epoch += 1
                                    else:
                                        counters.drop_auth += 1
                                logger.warning(
                                    "Decrypt failed (classified)",
                                    extra={
                                        "role": role,
                                        "reason": reason,
                                        "wire_len": len(wire),
                                        "error": str(exc),
                                    },
                                )
                                continue
                            except Exception as exc:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                                logger.warning(
                                    "Decrypt failed (other)",
                                    extra={"role": role, "error": str(exc), "wire_len": len(wire)},
                                )
                                continue

                            try:
                                if plaintext and plaintext[0] == 0x02:
                                    try:
                                        control_json = json.loads(plaintext[1:].decode("utf-8"))
                                    except (UnicodeDecodeError, json.JSONDecodeError):
                                        with counters_lock:
                                            counters.drops += 1
                                            counters.drop_other += 1
                                        continue
                                    result = handle_control(control_json, role, control_state)
                                    for note in result.notes:
                                        if note.startswith("prepare_fail"):
                                            with counters_lock:
                                                counters.rekeys_fail += 1
                                    for payload in result.send:
                                        control_state.outbox.put(payload)
                                    if result.start_handshake:
                                        suite_next, rid = result.start_handshake
                                        _launch_rekey(suite_next, rid)
                                    continue

                                if cfg.get("ENABLE_PACKET_TYPE") and plaintext:
                                    ptype = plaintext[0]
                                    if ptype == 0x01:
                                        out_bytes = plaintext[1:]
                                    else:
                                        with counters_lock:
                                            counters.drops += 1
                                            counters.drop_other += 1
                                        continue
                                else:
                                    out_bytes = plaintext

                                sockets["plaintext_out"].sendto(out_bytes, sockets["plaintext_peer"])
                                with counters_lock:
                                    counters.ptx_out += 1
                            except socket.error:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                        except socket.error:
                            continue
        except KeyboardInterrupt:
            pass
        finally:
            selector.close()
            if manual_stop:
                manual_stop.set()
                for thread in manual_threads:
                    thread.join(timeout=0.5)

        # Final status write and stop the status writer thread if running
        try:
            with counters_lock:
                write_status({
                    "status": "stopped",
                    "suite": suite_id,
                    "counters": counters.to_dict(),
                    "ts_ns": time.time_ns(),
                })
        except Exception:
            pass

        if 'stop_status_writer' in locals() and stop_status_writer is not None:
            try:
                stop_status_writer.set()
            except Exception:
                pass
        if 'status_thread' in locals() and status_thread is not None and status_thread.is_alive():
            try:
                status_thread.join(timeout=1.0)
            except Exception:
                pass

        return counters.to_dict()

============================================================

FILE 4/11: config.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\config.py
Size: 13,294 bytes
Modified: 2025-10-07 18:13:10
------------------------------------------------------------
"""
Core configuration constants for PQC drone-GCS secure proxy.

Single source of truth for all network ports, hosts, and runtime parameters.
"""

import os
from ipaddress import ip_address
from typing import Dict, Any


# Baseline host defaults reused throughout the configuration payload.
_DEFAULT_DRONE_HOST = "192.168.0.102"
_DEFAULT_GCS_HOST = "192.168.0.101"


# Default configuration - all required keys with correct types
CONFIG = {
    # Handshake (TCP)
    "TCP_HANDSHAKE_PORT": 46000,

    # Encrypted UDP data-plane (network)
    "UDP_DRONE_RX": 46012,   # drone binds here; GCS sends here
    "UDP_GCS_RX": 46011,     # gcs binds here; Drone sends here

    # Plaintext UDP (local loopback to apps/FC)
    "DRONE_PLAINTEXT_TX": 47003,  # app→drone-proxy (to encrypt out)
    "DRONE_PLAINTEXT_RX": 47004,  # drone-proxy→app (after decrypt)
    "GCS_PLAINTEXT_TX": 47001,    # app→gcs-proxy
    "GCS_PLAINTEXT_RX": 47002,    # gcs-proxy→app
    "DRONE_PLAINTEXT_HOST": "127.0.0.1",
    "GCS_PLAINTEXT_HOST": "127.0.0.1",

    # Hosts
    "DRONE_HOST": _DEFAULT_DRONE_HOST,
    "GCS_HOST": _DEFAULT_GCS_HOST,

    # Pre-shared key (hex) for drone authentication during handshake.
    # Default is a placeholder; override in production via environment variable.
    "DRONE_PSK": "0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef",

    # Crypto/runtime
    "REPLAY_WINDOW": 1024,
    "WIRE_VERSION": 1,      # header version byte (frozen)

    # --- Optional hardening / QoS knobs (NOT required; safe defaults) ---
    # Limit TCP handshake attempts accepted per IP at the GCS (server) side.
    # Model: token bucket; BURST tokens max, refilling at REFILL_PER_SEC tokens/sec.
    "HANDSHAKE_RL_BURST": 5,
    "HANDSHAKE_RL_REFILL_PER_SEC": 1,

    # Mark encrypted UDP with DSCP EF (46) to prioritize on WMM-enabled APs.
    # Set to None to disable. Implementation multiplies by 4 to form TOS.
    "ENCRYPTED_DSCP": 46,

    # Feature flag: if True, proxy prefixes app->proxy plaintext with 1 byte packet type.
    # 0x01 = MAVLink/data (forward to local app); 0x02 = control (route to policy engine).
    # When False (default), proxy passes bytes unchanged (backward compatible).
    "ENABLE_PACKET_TYPE": True,

    # Enforce strict matching of encrypted UDP peer IP/port with the authenticated handshake peer.
    # Disable (set to False) only when operating behind NAT where source ports may differ.
    "STRICT_UDP_PEER_MATCH": True,

    # Log real session IDs only when explicitly enabled (default False masks them to hashes).
    "LOG_SESSION_ID": False,

    # --- Simple automation defaults (tools/auto/*_simple.py) ---
    "DRONE_CONTROL_HOST": "0.0.0.0",
    "DRONE_CONTROL_PORT": 48080,
    "SIMPLE_VERIFY_TIMEOUT_S": 5.0,
    "SIMPLE_PACKETS_PER_SUITE": 1,
    "SIMPLE_PACKET_DELAY_S": 0.0,
    "SIMPLE_SUITE_DWELL_S": 0.0,
    "SIMPLE_INITIAL_SUITE": "cs-mlkem768-aesgcm-mldsa65",

    # Automation defaults for tools/auto orchestration scripts
    "AUTO_DRONE": {
        # Session IDs default to "<prefix>_<unix>" unless DRONE_SESSION_ID env overrides
        "session_prefix": "run",
        # Optional explicit initial suite override (None -> discover from secrets/config)
        "initial_suite": None,
        # Enable follower monitors (perf/pidstat/psutil) by default
        "monitors_enabled": True,
        # Apply CPU governor tweaks unless disabled
        "cpu_optimize": True,
        # Enable telemetry publisher back to the scheduler
        "telemetry_enabled": True,
        # Optional explicit telemetry host/port (None -> derive from CONFIG)
    "telemetry_host": _DEFAULT_GCS_HOST,
        "telemetry_port": 52080,
        # Override monitoring output base directory (None -> DEFAULT_MONITOR_BASE)
        "monitor_output_base": None,
        # Optional environment exports applied before creating the power monitor
        "power_env": {
            # Maintain 1 kHz sampling by default; backend remains auto unless overridden
            "DRONE_POWER_BACKEND": "ina219",
            "DRONE_POWER_SAMPLE_HZ": "1000",
            "INA219_I2C_BUS": "1",
            "INA219_ADDR": "0x40",
            "INA219_SHUNT_OHM": "0.1",
        },
    },

    "AUTO_GCS": {
        # Session IDs default to "<prefix>_<unix>" unless GCS_SESSION_ID env overrides
        "session_prefix": "run",
        # Traffic profile: "blast", "constant", "mavproxy", or "saturation"
        "traffic": "constant",
        # Duration for active traffic window per suite (seconds)
        "duration_s": 45.0,
        # Delay after rekey before starting traffic (seconds)
        "pre_gap_s": 1.0,
        # Delay between suites (seconds)
        "inter_gap_s": 15.0,
        # UDP payload size (bytes) for blaster calculations
        "payload_bytes": 256,
        # Sample every Nth send/receive event (0 disables)
        "event_sample": 100,
        # Number of full passes across suite list
        "passes": 1,
        # Explicit packets-per-second override; 0 means best-effort
        "rate_pps": 0,
        # Optional bandwidth target in Mbps (converted to PPS if > 0)
        "bandwidth_mbps": 0.0,
        # Max rate explored during saturation sweeps (Mbps)
        "max_rate_mbps": 200.0,
        # Optional ordered suite subset (None -> all suites)
        "suites": [
            "cs-mlkem512-aesgcm-mldsa44",
            "cs-mlkem512-aesgcm-mldsa65",
            "cs-mlkem512-aesgcm-mldsa87",
            "cs-mlkem512-aesgcm-falcon512",
            "cs-mlkem512-aesgcm-falcon1024",
            "cs-mlkem512-aesgcm-sphincs128fsha2",
            "cs-mlkem512-aesgcm-sphincs256fsha2",
            "cs-mlkem768-aesgcm-mldsa44",
            "cs-mlkem768-aesgcm-mldsa65",
            "cs-mlkem768-aesgcm-mldsa87",
            "cs-mlkem768-aesgcm-falcon512",
            "cs-mlkem768-aesgcm-falcon1024",
            "cs-mlkem768-aesgcm-sphincs128fsha2",
            "cs-mlkem768-aesgcm-sphincs256fsha2",
            "cs-mlkem1024-aesgcm-mldsa44",
            "cs-mlkem1024-aesgcm-mldsa65",
            "cs-mlkem1024-aesgcm-mldsa87",
            "cs-mlkem1024-aesgcm-falcon512",
            "cs-mlkem1024-aesgcm-falcon1024",
            "cs-mlkem1024-aesgcm-sphincs128fsha2",
            "cs-mlkem1024-aesgcm-sphincs256fsha2",
        ],
        # Launch local GCS proxy under scheduler control
        "launch_proxy": True,
        # Enable local proxy monitors (perf/pidstat/psutil)
        "monitors_enabled": True,
        # Start telemetry collector on the scheduler side
        "telemetry_enabled": True,
        # Bind/port for telemetry collector (defaults to CONFIG values)
        "telemetry_bind_host": "0.0.0.0",
        "telemetry_port": 52080,
        # Emit combined Excel workbook when run completes
        "export_combined_excel": True,
    },
}


# Required keys with their expected types
_REQUIRED_KEYS = {
    "TCP_HANDSHAKE_PORT": int,
    "UDP_DRONE_RX": int,
    "UDP_GCS_RX": int,
    "DRONE_PLAINTEXT_TX": int,
    "DRONE_PLAINTEXT_RX": int,
    "GCS_PLAINTEXT_TX": int,
    "GCS_PLAINTEXT_RX": int,
    "DRONE_HOST": str,
    "GCS_HOST": str,
    "DRONE_PLAINTEXT_HOST": str,
    "GCS_PLAINTEXT_HOST": str,
    "REPLAY_WINDOW": int,
    "WIRE_VERSION": int,
    "ENABLE_PACKET_TYPE": bool,
    "STRICT_UDP_PEER_MATCH": bool,
    "LOG_SESSION_ID": bool,
    "DRONE_PSK": str,
}

# Keys that can be overridden by environment variables
_ENV_OVERRIDABLE = {
    "TCP_HANDSHAKE_PORT",
    "UDP_DRONE_RX", 
    "UDP_GCS_RX",
    "DRONE_PLAINTEXT_TX",  # Added for testing/benchmarking flexibility
    "DRONE_PLAINTEXT_RX",  # Added for testing/benchmarking flexibility  
    "GCS_PLAINTEXT_TX",    # Added for testing/benchmarking flexibility
    "GCS_PLAINTEXT_RX",    # Added for testing/benchmarking flexibility
    "DRONE_PLAINTEXT_HOST",
    "GCS_PLAINTEXT_HOST",
    "DRONE_HOST",
    "GCS_HOST",
    "ENABLE_PACKET_TYPE",
    "STRICT_UDP_PEER_MATCH",
    "LOG_SESSION_ID",
    "DRONE_PSK",
}


def validate_config(cfg: Dict[str, Any]) -> None:
    """
    Ensure all required keys exist with correct types/ranges.
    Raise NotImplementedError("<reason>") on any violation.
    No return value on success.
    """
    # Check all required keys exist
    missing_keys = set(_REQUIRED_KEYS.keys()) - set(cfg.keys())
    if missing_keys:
        raise NotImplementedError(f"CONFIG missing required keys: {', '.join(sorted(missing_keys))}")
    
    # Check types for all keys
    for key, expected_type in _REQUIRED_KEYS.items():
        value = cfg[key]
        if not isinstance(value, expected_type):
            raise NotImplementedError(f"CONFIG[{key}] must be {expected_type.__name__}, got {type(value).__name__}")
    
    # Validate port ranges
    for key in _REQUIRED_KEYS:
        if key.endswith("_PORT") or key.endswith("_RX") or key.endswith("_TX"):
            port = cfg[key]
            if not (1 <= port <= 65535):
                raise NotImplementedError(f"CONFIG[{key}] must be valid port (1-65535), got {port}")
    
    # Validate specific constraints
    if cfg["WIRE_VERSION"] != 1:
        raise NotImplementedError(f"CONFIG[WIRE_VERSION] must be 1 (frozen), got {cfg['WIRE_VERSION']}")
    
    if cfg["REPLAY_WINDOW"] < 64:
        raise NotImplementedError(f"CONFIG[REPLAY_WINDOW] must be >= 64, got {cfg['REPLAY_WINDOW']}")
    if cfg["REPLAY_WINDOW"] > 8192:
        raise NotImplementedError(f"CONFIG[REPLAY_WINDOW] must be <= 8192, got {cfg['REPLAY_WINDOW']}")
    
    # Validate hosts are valid strings (basic check)
    for host_key in ["DRONE_HOST", "GCS_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise NotImplementedError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        try:
            ip_address(host)
        except ValueError as exc:
            raise NotImplementedError(f"CONFIG[{host_key}] must be a valid IP address: {exc}")

    # Loopback hosts for plaintext path may remain hostnames (e.g., 127.0.0.1).
    allow_non_loopback_plaintext = str(os.environ.get("ALLOW_NON_LOOPBACK_PLAINTEXT", "")).strip().lower() in {
        "1",
        "true",
        "yes",
        "on",
    }
    for host_key in ["DRONE_PLAINTEXT_HOST", "GCS_PLAINTEXT_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise NotImplementedError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        if allow_non_loopback_plaintext:
            continue
        try:
            parsed = ip_address(host)
            if not parsed.is_loopback:
                raise NotImplementedError(
                    f"CONFIG[{host_key}] must be a loopback address unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
        except ValueError:
            if host.lower() != "localhost":
                raise NotImplementedError(
                    f"CONFIG[{host_key}] must be loopback/localhost unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
    
    # Optional keys are intentionally not required; do light validation if present
    if "ENCRYPTED_DSCP" in cfg and cfg["ENCRYPTED_DSCP"] is not None:
        if not (0 <= int(cfg["ENCRYPTED_DSCP"]) <= 63):
            raise NotImplementedError("CONFIG[ENCRYPTED_DSCP] must be 0..63 or None")

    psk = cfg.get("DRONE_PSK", "")
    try:
        psk_bytes = bytes.fromhex(psk)
    except ValueError:
        raise NotImplementedError("CONFIG[DRONE_PSK] must be a hex string")
    if len(psk_bytes) != 32:
        raise NotImplementedError("CONFIG[DRONE_PSK] must decode to 32 bytes")


def _apply_env_overrides(cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Apply environment variable overrides to config."""
    result = cfg.copy()
    
    for key in _ENV_OVERRIDABLE:
        env_var = key
        if env_var in os.environ:
            env_value = os.environ[env_var]
            expected_type = _REQUIRED_KEYS[key]
            
            try:
                if expected_type == int:
                    result[key] = int(env_value)
                elif expected_type == str:
                    result[key] = str(env_value)
                elif expected_type == bool:
                    lowered = str(env_value).strip().lower()
                    if lowered in {"1", "true", "yes", "on"}:
                        result[key] = True
                    elif lowered in {"0", "false", "no", "off"}:
                        result[key] = False
                    else:
                        raise ValueError(f"invalid boolean literal: {env_value}")
                else:
                    raise NotImplementedError(f"Unsupported type for env override: {expected_type}")
            except ValueError:
                raise NotImplementedError(f"Invalid {expected_type.__name__} value for {env_var}: {env_value}")
    
    return result


# Apply environment overrides and validate
CONFIG = _apply_env_overrides(CONFIG)
validate_config(CONFIG)

============================================================

FILE 5/11: handshake.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\handshake.py
Size: 13,635 bytes
Modified: 2025-10-06 01:59:49
------------------------------------------------------------
from dataclasses import dataclass
import hashlib
import hmac
import os
import struct
from core.config import CONFIG
from core.suites import get_suite
from core.logging_utils import get_logger
from oqs.oqs import KeyEncapsulation, Signature

logger = get_logger("pqc")

class HandshakeFormatError(Exception):
    pass

class HandshakeVerifyError(Exception):
    pass

@dataclass(frozen=True)
class ServerHello:
    version: int
    kem_name: bytes
    sig_name: bytes
    session_id: bytes
    kem_pub: bytes
    signature: bytes
    challenge: bytes

@dataclass
class ServerEphemeral:
    kem_name: str
    sig_name: str
    session_id: bytes
    kem_obj: object  # oqs.KeyEncapsulation instance
    challenge: bytes

def build_server_hello(suite_id: str, server_sig_obj):
    suite = get_suite(suite_id)
    if not suite:
        raise NotImplementedError("suite_id not found")
    version = CONFIG["WIRE_VERSION"]
    kem_name = suite["kem_name"].encode("utf-8")
    sig_name = suite["sig_name"].encode("utf-8")
    if not kem_name or not sig_name:
        raise NotImplementedError("kem_name/sig_name empty")
    if not isinstance(server_sig_obj, Signature):
        raise NotImplementedError("server_sig_obj must be oqs.Signature")
    session_id = os.urandom(8)
    challenge = os.urandom(8)
    kem_obj = KeyEncapsulation(kem_name.decode("utf-8"))
    kem_pub = kem_obj.generate_keypair()
    # Include negotiated wire version as first byte of transcript to prevent downgrade
    transcript = (
        struct.pack("!B", version)
        + b"|pq-drone-gcs:v1|"
        + session_id
        + b"|"
        + kem_name
        + b"|"
        + sig_name
        + b"|"
        + kem_pub
        + b"|"
        + challenge
    )
    signature = server_sig_obj.sign(transcript)
    wire = struct.pack("!B", version)
    wire += struct.pack("!H", len(kem_name)) + kem_name
    wire += struct.pack("!H", len(sig_name)) + sig_name
    wire += session_id
    wire += challenge
    wire += struct.pack("!I", len(kem_pub)) + kem_pub
    wire += struct.pack("!H", len(signature)) + signature
    ephemeral = ServerEphemeral(
        kem_name=kem_name.decode("utf-8"),
        sig_name=sig_name.decode("utf-8"),
        session_id=session_id,
        kem_obj=kem_obj,
        challenge=challenge,
    )
    return wire, ephemeral

def parse_and_verify_server_hello(wire: bytes, expected_version: int, server_sig_pub: bytes) -> ServerHello:
    try:
        offset = 0
        version = wire[offset]
        offset += 1
        if version != expected_version:
            raise HandshakeFormatError("bad wire version")
        kem_name_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        kem_name = wire[offset:offset+kem_name_len]
        offset += kem_name_len
        sig_name_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        sig_name = wire[offset:offset+sig_name_len]
        offset += sig_name_len
        session_id = wire[offset:offset+8]
        offset += 8
        challenge = wire[offset:offset+8]
        offset += 8
        kem_pub_len = struct.unpack_from("!I", wire, offset)[0]
        offset += 4
        kem_pub = wire[offset:offset+kem_pub_len]
        offset += kem_pub_len
        sig_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        signature = wire[offset:offset+sig_len]
        offset += sig_len
    except Exception:
        raise HandshakeFormatError("malformed server hello")
    transcript = (
        struct.pack("!B", version)
        + b"|pq-drone-gcs:v1|"
        + session_id
        + b"|"
        + kem_name
        + b"|"
        + sig_name
        + b"|"
        + kem_pub
        + b"|"
        + challenge
    )
    sig = None
    try:
        sig = Signature(sig_name.decode("utf-8"))
        if not sig.verify(transcript, signature, server_sig_pub):
            raise HandshakeVerifyError("bad signature")
    except HandshakeVerifyError:
        raise
    except Exception:
        raise HandshakeVerifyError("signature verification failed")
    finally:
        if sig is not None and hasattr(sig, "free"):
            try:
                sig.free()
            except Exception:
                pass
    return ServerHello(
        version=version,
        kem_name=kem_name,
        sig_name=sig_name,
        session_id=session_id,
        kem_pub=kem_pub,
        signature=signature,
        challenge=challenge,
    )

def _drone_psk_bytes() -> bytes:
    psk_hex = CONFIG.get("DRONE_PSK", "")
    try:
        psk = bytes.fromhex(psk_hex)
    except ValueError as exc:
        raise NotImplementedError(f"Invalid DRONE_PSK hex: {exc}")
    if len(psk) != 32:
        raise NotImplementedError("DRONE_PSK must decode to 32 bytes")
    return psk


def client_encapsulate(server_hello: ServerHello):
    kem = None
    try:
        kem = KeyEncapsulation(server_hello.kem_name.decode("utf-8"))
        kem_ct, shared_secret = kem.encap_secret(server_hello.kem_pub)
        return kem_ct, shared_secret
    except Exception:
        raise NotImplementedError("client_encapsulate failed")
    finally:
        if kem is not None and hasattr(kem, "free"):
            try:
                kem.free()
            except Exception:
                pass


def server_decapsulate(ephemeral: ServerEphemeral, kem_ct: bytes):
    kem_obj = getattr(ephemeral, "kem_obj", None)
    try:
        if kem_obj is None:
            raise NotImplementedError("server_decapsulate missing kem_obj")
        shared_secret = kem_obj.decap_secret(kem_ct)
        return shared_secret
    except Exception:
        raise NotImplementedError("server_decapsulate failed")
    finally:
        if kem_obj is not None and hasattr(kem_obj, "free"):
            try:
                kem_obj.free()
            except Exception:
                pass
        if hasattr(ephemeral, "kem_obj"):
            ephemeral.kem_obj = None


def derive_transport_keys(role: str, session_id: bytes, kem_name: bytes, sig_name: bytes, shared_secret: bytes):
    if role not in {"client", "server"}:
        raise NotImplementedError("invalid role")
    if not (isinstance(session_id, bytes) and len(session_id) == 8):
        raise NotImplementedError("session_id must be 8 bytes")
    if not kem_name or not sig_name:
        raise NotImplementedError("kem_name/sig_name empty")
    try:
        from cryptography.hazmat.primitives.kdf.hkdf import HKDF
        from cryptography.hazmat.primitives import hashes
    except ImportError:
        raise NotImplementedError("cryptography not available")
    info = b"pq-drone-gcs:kdf:v1|" + session_id + b"|" + kem_name + b"|" + sig_name
    hkdf = HKDF(
        algorithm=hashes.SHA256(),
        length=64,
        salt=b"pq-drone-gcs|hkdf|v1",
        info=info,
    )
    okm = hkdf.derive(shared_secret)
    key_d2g = okm[:32]
    key_g2d = okm[32:64]

    if role == "client":
        # Drone acts as client; return (send_to_gcs, receive_from_gcs).
        return key_d2g, key_g2d
    else:  # server == GCS
        # GCS perspective: send_to_drone first, receive_from_drone second.
        return key_g2d, key_d2g
def server_gcs_handshake(conn, suite, gcs_sig_secret):
    """Authenticated GCS side handshake.

    Requires a ready oqs.Signature object (with generated key pair). Fails fast if not.
    """
    from oqs.oqs import Signature
    import struct

    conn.settimeout(10.0)

    if not isinstance(gcs_sig_secret, Signature):
        raise ValueError("gcs_sig_secret must be an oqs.Signature object with a loaded keypair")

    # Resolve suite_id by matching suite dict
    suite_id = None
    from core.suites import SUITES
    for sid, s in SUITES.items():
        if dict(s) == suite:
            suite_id = sid
            break
    if suite_id is None:
        raise ValueError("suite not found in registry")

    hello_wire, ephemeral = build_server_hello(suite_id, gcs_sig_secret)
    conn.sendall(struct.pack("!I", len(hello_wire)) + hello_wire)

    # Receive KEM ciphertext
    ct_len_bytes = b""
    while len(ct_len_bytes) < 4:
        chunk = conn.recv(4 - len(ct_len_bytes))
        if not chunk:
            raise ConnectionError("Connection closed reading ciphertext length")
        ct_len_bytes += chunk
    ct_len = struct.unpack("!I", ct_len_bytes)[0]
    kem_ct = b""
    while len(kem_ct) < ct_len:
        chunk = conn.recv(ct_len - len(kem_ct))
        if not chunk:
            raise ConnectionError("Connection closed reading ciphertext")
        kem_ct += chunk

    tag_len = hashlib.sha256().digest_size
    tag = b""
    while len(tag) < tag_len:
        chunk = conn.recv(tag_len - len(tag))
        if not chunk:
            raise ConnectionError("Connection closed reading drone authentication tag")
        tag += chunk

    expected_tag = hmac.new(_drone_psk_bytes(), hello_wire, hashlib.sha256).digest()
    if not hmac.compare_digest(tag, expected_tag):
        peer_ip = "unknown"
        try:
            peer_info = conn.getpeername()
            if isinstance(peer_info, tuple) and peer_info:
                peer_ip = str(peer_info[0])
            elif isinstance(peer_info, str) and peer_info:
                peer_ip = peer_info
        except (OSError, ValueError):
            peer_ip = "unknown"
        logger.warning(
            "Rejected drone handshake with bad authentication tag",
            extra={"role": "gcs", "expected_peer": CONFIG["DRONE_HOST"], "received": peer_ip},
        )
        raise HandshakeVerifyError("drone authentication failed")

    shared_secret = server_decapsulate(ephemeral, kem_ct)
    key_send, key_recv = derive_transport_keys(
        "server",
        ephemeral.session_id,
        ephemeral.kem_name.encode("utf-8"),
        ephemeral.sig_name.encode("utf-8"),
        shared_secret,
    )
    # Return (drone→gcs key, gcs→drone key, ...)
    return key_recv, key_send, b"", b"", ephemeral.session_id, ephemeral.kem_name, ephemeral.sig_name

def client_drone_handshake(client_sock, suite, gcs_sig_public):
    # Real handshake implementation with MANDATORY signature verification
    import struct
    
    # Add socket timeout to prevent hanging
    client_sock.settimeout(10.0)
    
    # Receive server hello with length prefix
    hello_len_bytes = b""
    while len(hello_len_bytes) < 4:
        chunk = client_sock.recv(4 - len(hello_len_bytes))
        if not chunk:
            raise NotImplementedError("Connection closed reading hello length")
        hello_len_bytes += chunk
        
    hello_len = struct.unpack("!I", hello_len_bytes)[0]
    hello_wire = b""
    while len(hello_wire) < hello_len:
        chunk = client_sock.recv(hello_len - len(hello_wire))
        if not chunk:
            raise NotImplementedError("Connection closed reading hello")
        hello_wire += chunk
    
    # Parse and VERIFY server hello - NO BYPASS ALLOWED
    # This is critical for security - verification failure must abort
    hello = parse_and_verify_server_hello(hello_wire, CONFIG["WIRE_VERSION"], gcs_sig_public)

    expected_kem = suite.get("kem_name") if isinstance(suite, dict) else None
    expected_sig = suite.get("sig_name") if isinstance(suite, dict) else None
    negotiated_kem = hello.kem_name.decode("utf-8") if isinstance(hello.kem_name, bytes) else hello.kem_name
    negotiated_sig = hello.sig_name.decode("utf-8") if isinstance(hello.sig_name, bytes) else hello.sig_name
    if expected_kem and negotiated_kem != expected_kem:
        logger.error(
            "Suite mismatch",
            extra={
                "expected_kem": expected_kem,
                "expected_sig": expected_sig,
                "negotiated_kem": negotiated_kem,
                "negotiated_sig": negotiated_sig,
            },
        )
        raise HandshakeVerifyError(
            f"Downgrade attempt detected: expected {expected_kem}, got {negotiated_kem}"
        )
    if expected_sig and negotiated_sig != expected_sig:
        logger.error(
            "Suite mismatch",
            extra={
                "expected_kem": expected_kem,
                "expected_sig": expected_sig,
                "negotiated_kem": negotiated_kem,
                "negotiated_sig": negotiated_sig,
            },
        )
        raise HandshakeVerifyError(
            f"Downgrade attempt detected: expected {expected_sig}, got {negotiated_sig}"
        )
    
    # Encapsulate and send KEM ciphertext + authentication tag
    kem_ct, shared_secret = client_encapsulate(hello)
    tag = hmac.new(_drone_psk_bytes(), hello_wire, hashlib.sha256).digest()
    client_sock.sendall(struct.pack("!I", len(kem_ct)) + kem_ct + tag)
    
    # Derive transport keys
    key_send, key_recv = derive_transport_keys("client", hello.session_id, 
                                              hello.kem_name, hello.sig_name, 
                                              shared_secret)
    
    # Return in expected format (nonce seeds are unused)
    return (
        key_send,
        key_recv,
        b"",
        b"",
        hello.session_id,
        hello.kem_name.decode() if isinstance(hello.kem_name, bytes) else hello.kem_name,
        hello.sig_name.decode() if isinstance(hello.sig_name, bytes) else hello.sig_name,
    )


============================================================

FILE 6/11: logging_utils.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\logging_utils.py
Size: 2,957 bytes
Modified: 2025-09-25 23:55:52
------------------------------------------------------------
import json, logging, sys, time
from pathlib import Path

class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        payload = {
            "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(record.created)),
            "level": record.levelname,
            "name": record.name,
            "msg": record.getMessage(),
        }
        if record.exc_info:
            payload["exc_info"] = self.formatException(record.exc_info)
        # Allow extra fields via record.__dict__ (filtered)
        for k, v in record.__dict__.items():
            if k not in ("msg", "args", "exc_info", "exc_text", "stack_info", "stack_level", "created",
                         "msecs", "relativeCreated", "levelno", "levelname", "pathname", "filename",
                         "module", "lineno", "funcName", "thread", "threadName", "processName", "process"):
                try:
                    json.dumps({k: v})
                    payload[k] = v
                except Exception:
                    payload[k] = str(v)
        return json.dumps(payload)

def get_logger(name: str = "pqc") -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger
    logger.setLevel(logging.INFO)
    h = logging.StreamHandler(sys.stdout)
    h.setFormatter(JsonFormatter())
    logger.addHandler(h)
    logger.propagate = False
    return logger


def configure_file_logger(role: str, logger: logging.Logger | None = None) -> Path:
    """Attach a JSON file handler and return log path."""

    active_logger = logger or get_logger()

    # Drop any previous file handlers we attached to avoid duplicate writes during tests.
    for handler in list(active_logger.handlers):
        if getattr(handler, "_pqc_file_handler", False):
            active_logger.removeHandler(handler)
            try:
                handler.close()
            except Exception:
                pass

    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    path = logs_dir / f"{role}-{timestamp}.log"

    file_handler = logging.FileHandler(path, encoding="utf-8")
    file_handler.setFormatter(JsonFormatter())
    file_handler._pqc_file_handler = True  # type: ignore[attr-defined]
    active_logger.addHandler(file_handler)

    return path

# Very small metrics hook (no deps)
class Counter:
    def __init__(self): self.value = 0
    def inc(self, n: int = 1): self.value += n

class Gauge:
    def __init__(self): self.value = 0
    def set(self, v: float): self.value = v

class Metrics:
    def __init__(self):
        self.counters = {}
        self.gauges = {}
    def counter(self, name: str) -> Counter:
        self.counters.setdefault(name, Counter()); return self.counters[name]
    def gauge(self, name: str) -> Gauge:
        self.gauges.setdefault(name, Gauge()); return self.gauges[name]

METRICS = Metrics()

============================================================

FILE 7/11: policy_engine.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\policy_engine.py
Size: 7,034 bytes
Modified: 2025-09-27 01:00:21
------------------------------------------------------------
"""
In-band control-plane state machine for interactive rekey negotiation.

Implements a two-phase commit protocol carried over packet type 0x02 payloads.
"""

from __future__ import annotations

import queue
import secrets
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from typing import Callable, Dict, List, Optional, Tuple


def _now_ms() -> int:
    """Return monotonic milliseconds for control timestamps."""

    return time.monotonic_ns() // 1_000_000


def _default_safe() -> bool:
    return True


@dataclass
class ControlState:
    """Mutable control-plane state shared between proxy threads."""

    role: str
    current_suite: str
    safe_guard: Callable[[], bool] = field(default_factory=_default_safe)
    lock: threading.Lock = field(default_factory=threading.Lock)
    outbox: "queue.Queue[dict]" = field(default_factory=queue.Queue)
    pending: Dict[str, str] = field(default_factory=dict)
    state: str = "RUNNING"
    active_rid: Optional[str] = None
    last_rekey_ms: Optional[int] = None
    last_rekey_suite: Optional[str] = None
    last_status: Optional[Dict[str, object]] = None
    stats: Dict[str, int] = field(default_factory=lambda: {
        "prepare_sent": 0,
        "prepare_received": 0,
        "rekeys_ok": 0,
        "rekeys_fail": 0,
    })
    seen_rids: deque[str] = field(default_factory=lambda: deque(maxlen=256))


@dataclass
class ControlResult:
    """Outcome of processing a control message."""

    send: List[dict] = field(default_factory=list)
    start_handshake: Optional[Tuple[str, str]] = None  # (suite_id, rid)
    notes: List[str] = field(default_factory=list)


def create_control_state(role: str, suite_id: str, *, safe_guard: Callable[[], bool] | None = None) -> ControlState:
    """Initialise ControlState with the provided role and suite."""

    guard = safe_guard or _default_safe
    return ControlState(role=role, current_suite=suite_id, safe_guard=guard)


def generate_rid() -> str:
    """Generate a random 64-bit hex request identifier."""

    return secrets.token_hex(8)


def enqueue_json(state: ControlState, payload: dict) -> None:
    """Place an outbound JSON payload onto the control outbox."""

    state.outbox.put(payload)


def request_prepare(state: ControlState, suite_id: str) -> str:
    """Queue a prepare_rekey message and transition to NEGOTIATING."""

    rid = generate_rid()
    now = _now_ms()
    with state.lock:
        if state.state != "RUNNING":
            raise RuntimeError("control-plane already negotiating")
        state.pending[rid] = suite_id
        state.active_rid = rid
        state.state = "NEGOTIATING"
        state.stats["prepare_sent"] += 1
    enqueue_json(
        state,
        {
            "type": "prepare_rekey",
            "suite": suite_id,
            "rid": rid,
            "t_ms": now,
        },
    )
    return rid


def record_rekey_result(state: ControlState, rid: str, suite_id: str, *, success: bool) -> None:
    """Record outcome of a rekey attempt and enqueue status update."""

    now = _now_ms()
    status_payload = {
        "type": "status",
        "state": "RUNNING",
        "suite": suite_id if success else state.current_suite,
        "rid": rid,
        "result": "ok" if success else "fail",
        "t_ms": now,
    }
    with state.lock:
        if success:
            state.current_suite = suite_id
            state.last_rekey_suite = suite_id
            state.last_rekey_ms = now
            state.stats["rekeys_ok"] += 1
        else:
            state.stats["rekeys_fail"] += 1
        state.pending.pop(rid, None)
        state.active_rid = None
        state.state = "RUNNING"
    enqueue_json(state, status_payload)


def handle_control(msg: dict, role: str, state: ControlState) -> ControlResult:
    """Process inbound control JSON and return actions for the proxy."""

    result = ControlResult()
    msg_type = msg.get("type")
    if not isinstance(msg_type, str):
        result.notes.append("missing_type")
        return result

    rid = msg.get("rid")
    now = _now_ms()

    if role == "gcs":
        if msg_type == "prepare_ok" and isinstance(rid, str):
            with state.lock:
                suite = state.pending.get(rid)
                if not suite:
                    result.notes.append("unknown_rid")
                    return result
                state.state = "SWAPPING"
                state.seen_rids.append(rid)
            result.send.append({
                "type": "commit_rekey",
                "suite": suite,
                "rid": rid,
                "t_ms": now,
            })
            result.start_handshake = (suite, rid)
        elif msg_type == "prepare_fail" and isinstance(rid, str):
            reason = msg.get("reason", "unknown")
            with state.lock:
                state.pending.pop(rid, None)
                state.active_rid = None
                state.state = "RUNNING"
                state.stats["rekeys_fail"] += 1
                state.seen_rids.append(rid)
            result.notes.append(f"prepare_fail:{reason}")
        elif msg_type == "status":
            with state.lock:
                state.last_status = msg
        else:
            result.notes.append(f"ignored:{msg_type}")
        return result

    if msg_type == "prepare_rekey":
        suite = msg.get("suite")
        if not isinstance(rid, str) or not isinstance(suite, str):
            result.notes.append("invalid_prepare")
            return result

        with state.lock:
            if rid in state.seen_rids:
                allow = False
            else:
                allow = state.state == "RUNNING" and state.safe_guard()
            if allow:
                state.pending[rid] = suite
                state.active_rid = rid
                state.state = "NEGOTIATING"
                state.stats["prepare_received"] += 1
                state.seen_rids.append(rid)
        if allow:
            result.send.append({
                "type": "prepare_ok",
                "rid": rid,
                "t_ms": now,
            })
        else:
            result.send.append({
                "type": "prepare_fail",
                "rid": rid,
                "reason": "unsafe",
                "t_ms": now,
            })
    elif msg_type == "commit_rekey" and isinstance(rid, str):
        with state.lock:
            suite = state.pending.get(rid)
            if not suite:
                result.notes.append("unknown_commit_rid")
                return result
            state.state = "SWAPPING"
        result.start_handshake = (suite, rid)
    elif msg_type == "status":
        with state.lock:
            state.last_status = msg
    else:
        result.notes.append(f"ignored:{msg_type}")

    return result

============================================================

FILE 8/11: power_monitor.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\power_monitor.py
Size: 42,867 bytes
Modified: 2025-10-07 20:33:26
------------------------------------------------------------
"""High-frequency power monitoring helpers for drone follower."""

from __future__ import annotations

import csv
import math
import os
import random
import re
import shutil
import subprocess
import threading
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Optional, Protocol

try:  # Best-effort hardware import; unavailable on dev hosts.
    import smbus2 as smbus  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - exercised on non-Pi hosts
    try:
        import smbus2 as smbus  # type: ignore
    except ModuleNotFoundError:  # pragma: no cover - exercised on hosts without I2C libs
        smbus = None  # type: ignore[assignment]

try:
    import psutil  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - psutil optional on host
    psutil = None  # type: ignore[assignment]


_DEFAULT_SAMPLE_HZ = int(os.getenv("INA219_SAMPLE_HZ", "1000"))
_DEFAULT_SHUNT_OHM = float(os.getenv("INA219_SHUNT_OHM", "0.1"))
_DEFAULT_I2C_BUS = int(os.getenv("INA219_I2C_BUS", "1"))
_DEFAULT_ADDR = int(os.getenv("INA219_ADDR", "0x40"), 16)
_DEFAULT_SIGN_MODE = os.getenv("INA219_SIGN_MODE", "auto").lower()

_RPI5_HWMON_PATH_ENV = "RPI5_HWMON_PATH"
_RPI5_HWMON_NAME_ENV = "RPI5_HWMON_NAME"
_RPI5_VOLTAGE_FILE_ENV = "RPI5_VOLTAGE_FILE"
_RPI5_CURRENT_FILE_ENV = "RPI5_CURRENT_FILE"
_RPI5_POWER_FILE_ENV = "RPI5_POWER_FILE"
_RPI5_VOLTAGE_SCALE_ENV = "RPI5_VOLTAGE_SCALE"
_RPI5_CURRENT_SCALE_ENV = "RPI5_CURRENT_SCALE"
_RPI5_POWER_SCALE_ENV = "RPI5_POWER_SCALE"

_RPI5_VOLTAGE_CANDIDATES = (
    "in0_input",
    "in1_input",
    "voltage0_input",
    "voltage1_input",
    "voltage_input",
    "vbus_input",
)

_RPI5_CURRENT_CANDIDATES = (
    "curr0_input",
    "curr1_input",
    "current0_input",
    "current1_input",
    "current_input",
    "ibus_input",
)

_RPI5_POWER_CANDIDATES = (
    "power0_input",
    "power1_input",
    "power_input",
)


# Registers and config masks from INA219 datasheet.
_CFG_BUS_RANGE_32V = 0x2000
_CFG_GAIN_8_320MV = 0x1800
_CFG_MODE_SANDBUS_CONT = 0x0007

_ADC_PROFILES = {
    "highspeed": {"badc": 0x0080, "sadc": 0x0000, "settle": 0.0004, "hz": 1100},
    "balanced": {"badc": 0x0400, "sadc": 0x0018, "settle": 0.0010, "hz": 900},
    "precision": {"badc": 0x0400, "sadc": 0x0048, "settle": 0.0020, "hz": 450},
}


@dataclass
class PowerSummary:
    """Aggregate statistics for a capture window."""

    label: str
    duration_s: float
    samples: int
    avg_current_a: float
    avg_voltage_v: float
    avg_power_w: float
    energy_j: float
    sample_rate_hz: float
    csv_path: str
    start_ns: int
    end_ns: int


@dataclass
class PowerSample:
    """Single instantaneous power sample."""

    timestamp_ns: int
    current_a: float
    voltage_v: float
    power_w: float


class PowerMonitorUnavailable(RuntimeError):
    """Raised when a power monitor backend cannot be initialised."""


class PowerMonitor(Protocol):
    sample_hz: int

    @property
    def sign_factor(self) -> int:  # pragma: no cover - protocol definition only
        ...

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:  # pragma: no cover - protocol definition only
        ...

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:  # pragma: no cover - protocol definition only
        ...


def _pick_profile(sample_hz: float) -> tuple[str, dict]:
    profile_key = os.getenv("INA219_ADC_PROFILE", "auto").lower()
    if profile_key == "auto":
        if sample_hz >= 900:
            profile_key = "highspeed"
        elif sample_hz >= 500:
            profile_key = "balanced"
        else:
            profile_key = "precision"
    return profile_key if profile_key in _ADC_PROFILES else "balanced", _ADC_PROFILES.get(profile_key, _ADC_PROFILES["balanced"])


def _sanitize_label(label: str) -> str:
    return "".join(ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in label)[:64] or "capture"


class Ina219PowerMonitor:
    """Wraps basic INA219 sampling with CSV logging and summary stats."""

    def __init__(
        self,
        output_dir: Path,
        *,
        i2c_bus: int = _DEFAULT_I2C_BUS,
        address: int = _DEFAULT_ADDR,
        shunt_ohm: float = _DEFAULT_SHUNT_OHM,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        sign_mode: str = _DEFAULT_SIGN_MODE,
    ) -> None:
        if smbus is None:
            raise PowerMonitorUnavailable("smbus module not available on host")
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.address = address
        self.shunt_ohm = shunt_ohm
        self.sample_hz = sample_hz
        self._bus = None
        self._bus_lock = threading.Lock()
        self._sign_factor = 1
        self._sign_mode = sign_mode

        try:
            self._bus = smbus.SMBus(i2c_bus)
        except Exception as exc:  # pragma: no cover - requires hardware
            raise PowerMonitorUnavailable(f"failed to open I2C bus {i2c_bus}: {exc}") from exc

        try:
            self._configure(sample_hz)
            self._sign_factor = self._resolve_sign()
        except Exception as exc:  # pragma: no cover - requires hardware
            raise PowerMonitorUnavailable(f"INA219 init failed: {exc}") from exc

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")
        if self._bus is None:
            raise PowerMonitorUnavailable("power monitor not initialised")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while True:
                elapsed = time.perf_counter() - start_perf
                if elapsed >= duration_s:
                    break
                try:
                    current_a, voltage_v = self._read_current_voltage()
                except Exception as exc:  # pragma: no cover - hardware failure path
                    raise PowerMonitorUnavailable(f"INA219 read failed: {exc}") from exc

                power_w = current_a * voltage_v
                writer.writerow([time.time_ns(), f"{current_a:.6f}", f"{voltage_v:.6f}", f"{power_w:.6f}", self._sign_factor])
                if samples % 250 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = avg_power * elapsed_s
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        if self._bus is None:
            raise PowerMonitorUnavailable("power monitor not initialised")
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            timestamp_ns = time.time_ns()
            current_a, voltage_v = self._read_current_voltage()
            power_w = current_a * voltage_v
            yield PowerSample(
                timestamp_ns=timestamp_ns,
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    def _configure(self, sample_hz: float) -> None:
        profile_key, profile = _pick_profile(sample_hz)
        cfg = (
            _CFG_BUS_RANGE_32V
            | _CFG_GAIN_8_320MV
            | profile["badc"]
            | profile["sadc"]
            | _CFG_MODE_SANDBUS_CONT
        )
        payload = [(cfg >> 8) & 0xFF, cfg & 0xFF]
        with self._bus_lock:
            self._bus.write_i2c_block_data(self.address, 0x00, payload)  # type: ignore[union-attr]
        time.sleep(profile["settle"])

    def _resolve_sign(self) -> int:
        mode = self._sign_mode
        if mode.startswith("pos"):
            return 1
        if mode.startswith("neg"):
            return -1
        probe_deadline = time.time() + float(os.getenv("INA219_SIGN_PROBE_SEC", "2"))
        readings = []
        while time.time() < probe_deadline:
            vsh = self._read_shunt_voltage()
            readings.append(vsh)
            time.sleep(0.005)
        if not readings:
            return 1
        readings.sort()
        median = readings[len(readings) // 2]
        return -1 if median < -20e-6 else 1

    def _read_current_voltage(self) -> tuple[float, float]:
        vsh = self._read_shunt_voltage()
        current = (vsh / self.shunt_ohm) * self._sign_factor
        voltage = self._read_bus_voltage()
        return current, voltage

    def _read_shunt_voltage(self) -> float:
        raw = self._read_s16(0x01)
        return raw * 10e-6

    def _read_bus_voltage(self) -> float:
        raw = self._read_u16(0x02)
        return ((raw >> 3) & 0x1FFF) * 0.004

    def _read_u16(self, register: int) -> int:
        with self._bus_lock:
            hi, lo = self._bus.read_i2c_block_data(self.address, register, 2)  # type: ignore[union-attr]
        return (hi << 8) | lo

    def _read_s16(self, register: int) -> int:
        val = self._read_u16(register)
        if val & 0x8000:
            val -= 1 << 16
        return val


class Rpi5PowerMonitor:
    """Power monitor backend using Raspberry Pi 5 onboard telemetry via hwmon."""

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        sign_mode: str = _DEFAULT_SIGN_MODE,
        hwmon_path: Optional[str] = None,
        hwmon_name_hint: Optional[str] = None,
        voltage_file: Optional[str] = None,
        current_file: Optional[str] = None,
        power_file: Optional[str] = None,
        voltage_scale: Optional[float] = None,
        current_scale: Optional[float] = None,
        power_scale: Optional[float] = None,
    ) -> None:
        del sign_mode  # Pi 5 telemetry reports already-correct sign
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self._sign_factor = 1
        self._hwmon_dir = self._find_hwmon_dir(hwmon_path, hwmon_name_hint, strict=True)
        self._voltage_path, self._current_path, self._power_path = self._resolve_channels(
            voltage_file,
            current_file,
            power_file,
        )
        self._voltage_scale = self._resolve_scale(voltage_scale, _RPI5_VOLTAGE_SCALE_ENV, 1e-6)
        self._current_scale = self._resolve_scale(current_scale, _RPI5_CURRENT_SCALE_ENV, 1e-6)
        self._power_scale = self._resolve_scale(power_scale, _RPI5_POWER_SCALE_ENV, 1e-6)

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while True:
                elapsed = time.perf_counter() - start_perf
                if elapsed >= duration_s:
                    break
                current_a, voltage_v, power_w = self._read_measurements()
                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}",
                    f"{voltage_v:.6f}",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 250 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = avg_power * elapsed_s
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            timestamp_ns = time.time_ns()
            current_a, voltage_v, power_w = self._read_measurements()
            yield PowerSample(
                timestamp_ns=timestamp_ns,
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    @staticmethod
    def is_supported(
        hwmon_path: Optional[str] = None,
        hwmon_name_hint: Optional[str] = None,
    ) -> bool:
        try:
            return Rpi5PowerMonitor._find_hwmon_dir(hwmon_path, hwmon_name_hint, strict=False) is not None
        except PowerMonitorUnavailable:
            return False

    @staticmethod
    def _find_hwmon_dir(
        hwmon_path: Optional[str],
        hwmon_name_hint: Optional[str],
        *,
        strict: bool,
    ) -> Optional[Path]:
        candidates = []
        if hwmon_path:
            candidates.append(hwmon_path)
        env_path = os.getenv(_RPI5_HWMON_PATH_ENV)
        if env_path:
            candidates.append(env_path)

        for candidate in candidates:
            path = Path(candidate).expanduser()
            if path.is_dir():
                return path
            if strict:
                raise PowerMonitorUnavailable(f"hwmon path not found: {path}")

        hwmon_root = Path("/sys/class/hwmon")
        if not hwmon_root.exists():
            if strict:
                raise PowerMonitorUnavailable("/sys/class/hwmon not present on host")
            return None

        hint_source = hwmon_name_hint or os.getenv(_RPI5_HWMON_NAME_ENV) or ""
        hints = [part.strip().lower() for part in hint_source.split(",") if part.strip()]

        for entry in sorted(hwmon_root.iterdir()):
            name_file = entry / "name"
            try:
                name_value = name_file.read_text().strip().lower()
            except Exception:
                continue
            if not name_value:
                continue
            if hints:
                if any(hint in name_value for hint in hints):
                    return entry
            else:
                if "rpi" in name_value and (
                    "power" in name_value
                    or "pmic" in name_value
                    or "monitor" in name_value
                    or "volt" in name_value
                ):
                    return entry

        if strict:
            raise PowerMonitorUnavailable("unable to locate Raspberry Pi power hwmon device")
        return None

    def _resolve_channels(
        self,
        voltage_file: Optional[str],
        current_file: Optional[str],
        power_file: Optional[str],
    ) -> tuple[Path, Path, Optional[Path]]:
        search_dirs = [self._hwmon_dir]
        device_dir = self._hwmon_dir / "device"
        if device_dir.is_dir():
            search_dirs.append(device_dir)

        def pick(
            defaults: tuple[str, ...],
            override: Optional[str],
            env_var: str,
            *,
            required: bool,
        ) -> Optional[Path]:
            # Prefer explicit override paths first.
            if override:
                override_path = Path(override)
                if override_path.is_absolute() or override_path.exists():
                    if override_path.exists():
                        return override_path
                    if required:
                        raise PowerMonitorUnavailable(f"override channel path not found: {override_path}")
                else:
                    for base in search_dirs:
                        candidate = base / override
                        if candidate.exists():
                            return candidate
                    if required:
                        raise PowerMonitorUnavailable(f"override channel name not found: {override}")

            env_override = os.getenv(env_var)
            if env_override:
                for token in env_override.split(","):
                    name = token.strip()
                    if not name:
                        continue
                    env_path = Path(name)
                    if env_path.is_absolute() or env_path.exists():
                        if env_path.exists():
                            return env_path
                        continue
                    for base in search_dirs:
                        candidate = base / name
                        if candidate.exists():
                            return candidate

            for name in defaults:
                for base in search_dirs:
                    candidate = base / name
                    if candidate.exists():
                        return candidate

            if required:
                raise PowerMonitorUnavailable(f"missing required hwmon channel {defaults[0] if defaults else 'unknown'}")
            return None

        voltage_path = pick(_RPI5_VOLTAGE_CANDIDATES, voltage_file, _RPI5_VOLTAGE_FILE_ENV, required=True)
        current_path = pick(_RPI5_CURRENT_CANDIDATES, current_file, _RPI5_CURRENT_FILE_ENV, required=True)
        power_path = pick(_RPI5_POWER_CANDIDATES, power_file, _RPI5_POWER_FILE_ENV, required=False)
        if voltage_path is None or current_path is None:
            raise PowerMonitorUnavailable("incomplete hwmon channel mapping")
        return voltage_path, current_path, power_path

    def _read_measurements(self) -> tuple[float, float, float]:
        voltage_v = self._read_channel(self._voltage_path, self._voltage_scale)
        current_a = self._read_channel(self._current_path, self._current_scale)
        if self._power_path is not None:
            power_w = self._read_channel(self._power_path, self._power_scale)
        else:
            power_w = voltage_v * current_a
        return current_a, voltage_v, power_w

    def _read_channel(self, path: Path, scale: float) -> float:
        try:
            raw = path.read_text().strip()
        except FileNotFoundError as exc:
            raise PowerMonitorUnavailable(f"hwmon channel missing: {path}") from exc
        except PermissionError as exc:  # pragma: no cover - depends on host permissions
            raise PowerMonitorUnavailable(f"insufficient permissions for {path}") from exc
        if not raw:
            raise PowerMonitorUnavailable(f"empty hwmon reading from {path}")
        try:
            value = float(raw)
        except ValueError as exc:
            raise PowerMonitorUnavailable(f"invalid hwmon reading from {path}: {raw!r}") from exc
        return value * scale

    def _resolve_scale(self, explicit: Optional[float], env_name: str, default: float) -> float:
        if explicit is not None:
            return explicit
        raw = os.getenv(env_name)
        if raw is None or raw == "":
            return default
        try:
            return float(raw)
        except ValueError as exc:
            raise PowerMonitorUnavailable(f"invalid {env_name} value: {raw!r}") from exc


class Rpi5PmicPowerMonitor:
    """Power monitor backend using Raspberry Pi 5 PMIC telemetry via `vcgencmd`."""

    _RAIL_PATTERN = re.compile(
        r"^\s*(?P<name>[A-Z0-9_]+)\s+(?P<kind>current|volt)\(\d+\)=(?P<value>[0-9.]+)(?P<unit>A|V)\s*$"
    )

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = 10,
        sign_mode: str = "auto",
    ) -> None:
        del sign_mode  # PMIC telemetry is unsigned
        if sample_hz <= 0 or sample_hz > 20:
            raise PowerMonitorUnavailable("rpi5-pmic sample_hz must be between 1 and 20")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self._sign_factor = 1

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")
        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while (time.perf_counter() - start_perf) < duration_s:
                rails = self._read_once()
                voltage_v = self._choose_voltage(rails)
                power_w = self._sum_power(rails)
                current_a = self._derive_current(power_w, voltage_v)

                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}" if not math.isnan(current_a) else "nan",
                    f"{voltage_v:.6f}" if not math.isnan(voltage_v) else "nan",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 10 == 0:
                    handle.flush()

                if not math.isnan(current_a):
                    sum_current += current_a
                if not math.isnan(voltage_v):
                    sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick = start_perf + samples * dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed = max(end_perf - start_perf, 1e-9)
        avg_current = (sum_current / samples) if samples else 0.0
        avg_voltage = (sum_voltage / samples) if samples else 0.0
        avg_power = (sum_power / samples) if samples else 0.0
        energy_j = avg_power * elapsed
        sample_rate = samples / elapsed if elapsed > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        start_perf = time.perf_counter()
        samples = 0
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            rails = self._read_once()
            voltage_v = self._choose_voltage(rails)
            power_w = self._sum_power(rails)
            current_a = self._derive_current(power_w, voltage_v)

            yield PowerSample(
                timestamp_ns=time.time_ns(),
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )

            samples += 1
            next_tick = start_perf + samples * dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    def _read_once(self) -> dict[str, dict[str, Optional[float]]]:
        try:
            output = subprocess.check_output(["vcgencmd", "pmic_read_adc"], text=True, timeout=1.0)
        except FileNotFoundError as exc:
            raise PowerMonitorUnavailable("vcgencmd not found; install raspberrypi-userland") from exc
        except subprocess.SubprocessError as exc:
            raise PowerMonitorUnavailable(f"vcgencmd pmic_read_adc failed: {exc}") from exc

        rails: dict[str, dict[str, Optional[float]]] = {}
        for line in output.splitlines():
            match = self._RAIL_PATTERN.match(line)
            if not match:
                continue
            name = match.group("name")
            kind = match.group("kind")
            value = float(match.group("value"))
            rail = rails.setdefault(name, {"current_a": None, "voltage_v": None})
            if kind == "current":
                rail["current_a"] = value
            else:
                rail["voltage_v"] = value
        if not rails:
            raise PowerMonitorUnavailable("pmic_read_adc returned no rail telemetry")
        return rails

    def _sum_power(self, rails: dict[str, dict[str, Optional[float]]]) -> float:
        total = 0.0
        for rail in rails.values():
            current_a = rail.get("current_a")
            voltage_v = rail.get("voltage_v")
            if current_a is None or voltage_v is None:
                continue
            total += current_a * voltage_v
        return total

    def _choose_voltage(self, rails: dict[str, dict[str, Optional[float]]]) -> float:
        ext5 = rails.get("EXT5V_V", {}).get("voltage_v") if "EXT5V_V" in rails else None
        if ext5 is not None and ext5 > 0:
            return ext5
        return max((rail.get("voltage_v") or float("nan") for rail in rails.values()), default=float("nan"))

    def _derive_current(self, power_w: float, voltage_v: float) -> float:
        if math.isnan(voltage_v) or voltage_v <= 0:
            return float("nan")
        return power_w / voltage_v


class SyntheticPowerMonitor:
    """Synthetic fallback monitor that approximates power via host telemetry."""

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        base_power_w: float = 18.0,
        dynamic_power_w: float = 12.0,
        voltage_v: float = 11.1,
        noise_w: float = 1.5,
    ) -> None:
        if psutil is None:
            raise PowerMonitorUnavailable("psutil module not available for synthetic backend")
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self.base_power_w = max(0.0, float(base_power_w))
        self.dynamic_power_w = max(0.0, float(dynamic_power_w))
        self.noise_w = max(0.0, float(noise_w))
        self.voltage_v = max(1e-3, float(voltage_v))
        self._sign_factor = 1
        self.backend_name = "synthetic"

    @staticmethod
    def is_supported() -> bool:
        return psutil is not None

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def _compute_power(self, cpu_percent: float, net_bytes_per_s: float) -> float:
        cpu_term = (cpu_percent / 100.0) * self.dynamic_power_w
        net_term = min(self.dynamic_power_w * 0.5, (net_bytes_per_s / 1_000_000.0) * 4.0)
        jitter = random.uniform(-self.noise_w, self.noise_w)
        return max(0.0, self.base_power_w + cpu_term + net_term + jitter)

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        refresh_cpu_every = max(1, int(self.sample_hz * 0.05))  # ~20 Hz refresh
        refresh_net_every = max(refresh_cpu_every * 2, int(self.sample_hz * 0.1))
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        start_wall_ns = time.time_ns()

        samples = 0
        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0

        cpu_percent = psutil.cpu_percent(interval=None)
        net = psutil.net_io_counters() if hasattr(psutil, "net_io_counters") else None
        last_net_total = (net.bytes_sent + net.bytes_recv) if net else 0
        last_net_ts = time.perf_counter()
        net_bytes_per_s = 0.0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            target_samples = int(round(duration_s * self.sample_hz))
            while samples < target_samples:
                if samples % refresh_cpu_every == 0:
                    cpu_percent = psutil.cpu_percent(interval=None)

                if net and samples % refresh_net_every == 0:
                    now = time.perf_counter()
                    elapsed = max(now - last_net_ts, 1e-6)
                    net_curr = psutil.net_io_counters()
                    total = net_curr.bytes_sent + net_curr.bytes_recv
                    delta = max(0, total - last_net_total)
                    net_bytes_per_s = delta / elapsed
                    last_net_total = total
                    last_net_ts = now

                power_w = self._compute_power(cpu_percent, net_bytes_per_s)
                voltage_v = self.voltage_v
                current_a = power_w / voltage_v

                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}",
                    f"{voltage_v:.6f}",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 500 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = sum_power * dt
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        refresh_cpu_every = max(1, int(self.sample_hz * 0.05))
        refresh_net_every = max(refresh_cpu_every * 2, int(self.sample_hz * 0.1))
        start_perf = time.perf_counter()
        next_tick = time.perf_counter()
        samples = 0

        cpu_percent = psutil.cpu_percent(interval=None) if psutil else 0.0
        net = psutil.net_io_counters() if hasattr(psutil, "net_io_counters") else None
        last_net_total = (net.bytes_sent + net.bytes_recv) if net else 0
        last_net_ts = time.perf_counter()
        net_bytes_per_s = 0.0

        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break

            if psutil and samples % refresh_cpu_every == 0:
                cpu_percent = psutil.cpu_percent(interval=None)

            if psutil and net and samples % refresh_net_every == 0:
                now = time.perf_counter()
                elapsed = max(now - last_net_ts, 1e-6)
                net_curr = psutil.net_io_counters()
                total = net_curr.bytes_sent + net_curr.bytes_recv
                delta = max(0, total - last_net_total)
                net_bytes_per_s = delta / elapsed
                last_net_total = total
                last_net_ts = now

            power_w = self._compute_power(cpu_percent, net_bytes_per_s)
            voltage_v = self.voltage_v
            current_a = power_w / voltage_v
            yield PowerSample(
                timestamp_ns=time.time_ns(),
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )

            samples += 1
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

def create_power_monitor(
    output_dir: Path,
    *,
    backend: str = "auto",
    sample_hz: Optional[int] = None,
    sign_mode: Optional[str] = None,
    shunt_ohm: Optional[float] = None,
    i2c_bus: Optional[int] = None,
    address: Optional[int] = None,
    hwmon_path: Optional[str] = None,
    hwmon_name_hint: Optional[str] = None,
    voltage_file: Optional[str] = None,
    current_file: Optional[str] = None,
    power_file: Optional[str] = None,
    voltage_scale: Optional[float] = None,
    current_scale: Optional[float] = None,
    power_scale: Optional[float] = None,
) -> PowerMonitor:
    resolved_backend = (backend or "auto").lower()
    env_backend = os.getenv("POWER_MONITOR_BACKEND")
    if resolved_backend == "auto" and env_backend:
        resolved_backend = env_backend.lower()

    resolved_sample_hz = int(sample_hz if sample_hz is not None else _DEFAULT_SAMPLE_HZ)
    resolved_sign_mode = (sign_mode or _DEFAULT_SIGN_MODE).lower()
    resolved_shunt = float(shunt_ohm if shunt_ohm is not None else _DEFAULT_SHUNT_OHM)
    resolved_i2c_bus = int(i2c_bus if i2c_bus is not None else _DEFAULT_I2C_BUS)
    resolved_address = address if address is not None else _DEFAULT_ADDR
    if isinstance(resolved_address, str):
        resolved_address = int(resolved_address, 0)

    ina_kwargs = {
        "i2c_bus": resolved_i2c_bus,
        "address": resolved_address,
        "shunt_ohm": resolved_shunt,
        "sample_hz": resolved_sample_hz,
        "sign_mode": resolved_sign_mode,
    }
    rpi_kwargs = {
        "sample_hz": resolved_sample_hz,
        "sign_mode": resolved_sign_mode,
        "hwmon_path": hwmon_path,
        "hwmon_name_hint": hwmon_name_hint,
        "voltage_file": voltage_file,
        "current_file": current_file,
        "power_file": power_file,
        "voltage_scale": voltage_scale,
        "current_scale": current_scale,
        "power_scale": power_scale,
    }

    if resolved_backend == "ina219":
        return Ina219PowerMonitor(output_dir, **ina_kwargs)
    if resolved_backend == "rpi5":
        return Rpi5PowerMonitor(output_dir, **rpi_kwargs)
    if resolved_backend == "rpi5-pmic":
        return Rpi5PmicPowerMonitor(output_dir, sample_hz=resolved_sample_hz, sign_mode=resolved_sign_mode)
    if resolved_backend == "synthetic":
        return SyntheticPowerMonitor(output_dir, sample_hz=resolved_sample_hz)
    if resolved_backend != "auto":
        raise ValueError(f"unknown power monitor backend: {backend}")

    rpi_error: Optional[PowerMonitorUnavailable] = None
    pmic_error: Optional[PowerMonitorUnavailable] = None
    synthetic_error: Optional[PowerMonitorUnavailable] = None
    if Rpi5PowerMonitor.is_supported(hwmon_path=hwmon_path, hwmon_name_hint=hwmon_name_hint):
        try:
            return Rpi5PowerMonitor(output_dir, **rpi_kwargs)
        except PowerMonitorUnavailable as exc:
            rpi_error = exc

    if shutil.which("vcgencmd"):
        try:
            return Rpi5PmicPowerMonitor(output_dir, sample_hz=resolved_sample_hz, sign_mode=resolved_sign_mode)
        except PowerMonitorUnavailable as exc:
            pmic_error = exc

    try:
        return Ina219PowerMonitor(output_dir, **ina_kwargs)
    except PowerMonitorUnavailable as exc:
        ina_error = exc
        if SyntheticPowerMonitor.is_supported():
            try:
                return SyntheticPowerMonitor(output_dir, sample_hz=resolved_sample_hz)
            except PowerMonitorUnavailable as syn_exc:
                synthetic_error = syn_exc
        if pmic_error is not None:
            raise pmic_error
        if rpi_error is not None:
            raise rpi_error
        if synthetic_error is not None:
            raise synthetic_error
        raise ina_error


__all__ = [
    "Ina219PowerMonitor",
    "Rpi5PowerMonitor",
    "Rpi5PmicPowerMonitor",
    "SyntheticPowerMonitor",
    "PowerMonitor",
    "PowerSummary",
    "PowerSample",
    "PowerMonitorUnavailable",
    "create_power_monitor",
]

============================================================

FILE 9/11: project_config.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\project_config.py
Size: 168 bytes
Modified: 2025-09-24 15:32:25
------------------------------------------------------------
# Thin shim so planned path 'project_config.py' exists without breaking tests.
# Source of truth remains core/config.py
from .config import CONFIG
__all__ = ["CONFIG"]

============================================================

FILE 10/11: run_proxy.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\run_proxy.py
Size: 27,151 bytes
Modified: 2025-10-06 14:30:57
------------------------------------------------------------
"""
Unified CLI entrypoint for the PQC drone-GCS proxy.

Supports subcommands:
- init-identity: Create persistent GCS signing identity
- gcs: Start GCS proxy (requires secret key by default)  
- drone: Start drone proxy (requires GCS public key)

Uses persistent file-based keys by default for production security.
"""

import sys
import argparse
import signal
import os
import json
import time
import logging
import threading
from pathlib import Path
from typing import Callable, Dict, Optional

from core.config import CONFIG
from core.suites import get_suite, build_suite_id
from core.logging_utils import get_logger, configure_file_logger

logger = get_logger("pqc")


def _require_signature_class():
    """Lazily import oqs Signature and provide a friendly error if missing."""

    try:
        from oqs.oqs import Signature  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - exercised via CLI
        if exc.name in {"oqs", "oqs.oqs"}:
            print(
                "Error: oqs-python is required for cryptographic operations. "
                "Install it with 'pip install oqs-python' or activate the project environment."
            )
            sys.exit(1)
        raise

    return Signature


def _require_run_proxy():
    """Import run_proxy only when needed, surfacing helpful guidance on failure."""

    try:
        from core.async_proxy import run_proxy as _run_proxy  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - exercised via CLI
        if exc.name in {"oqs", "oqs.oqs"}:
            print(
                "Error: oqs-python is required to start the proxy. "
                "Install it with 'pip install oqs-python' or activate the project environment."
            )
            sys.exit(1)
        raise

    return _run_proxy


def _build_matrix_secret_loader(
    *,
    suite_id: Optional[str],
    default_secret_path: Optional[Path],
    initial_secret: Optional[object],
    signature_cls,
    matrix_dir: Optional[Path] = None,
) -> Callable[[Dict[str, object]], object]:
    """Return loader that fetches per-suite signing secrets from disk.

    The loader prefers a suite-specific directory under `secrets/matrix/` and falls
    back to the primary secret path when targeting the initial suite. Results are
    cached per suite and guarded with a lock because rekeys may run in background
    threads.
    """

    lock = threading.Lock()
    cache: Dict[str, object] = {}
    if suite_id and initial_secret is not None and isinstance(initial_secret, signature_cls):
        cache[suite_id] = initial_secret

    matrix_secrets_dir = matrix_dir or Path("secrets/matrix")

    def instantiate(secret_bytes: bytes, sig_name: str):
        errors = []
        sig_obj = None
        try:
            sig_obj = signature_cls(sig_name)
        except Exception as exc:  # pragma: no cover - depends on oqs build
            errors.append(f"Signature ctor failed: {exc}")
            sig_obj = None

        if sig_obj is not None and hasattr(sig_obj, "import_secret_key"):
            try:
                sig_obj.import_secret_key(secret_bytes)
                return sig_obj
            except Exception as exc:
                errors.append(f"import_secret_key failed: {exc}")

        try:
            return signature_cls(sig_name, secret_key=secret_bytes)
        except TypeError as exc:
            errors.append(f"ctor secret_key unsupported: {exc}")
        except Exception as exc:  # pragma: no cover - defensive logging only
            errors.append(f"ctor secret_key failed: {exc}")

        detail = "; ".join(errors) if errors else "unknown error"
        raise RuntimeError(f"Unable to load signature secret: {detail}")

    def load_secret_for_suite(target_suite: Dict[str, object]):
        target_suite_id = target_suite.get("suite_id") if isinstance(target_suite, dict) else None
        if not target_suite_id:
            raise RuntimeError("Suite dictionary missing suite_id")

        with lock:
            cached = cache.get(target_suite_id)
            if cached is not None:
                return cached

        candidates = []
        if default_secret_path and suite_id and target_suite_id == suite_id:
            candidates.append(default_secret_path)
        candidates.append(matrix_secrets_dir / target_suite_id / "gcs_signing.key")

        seen: Dict[str, None] = {}
        for candidate in candidates:
            candidate_path = candidate.expanduser()
            key = str(candidate_path.resolve()) if candidate_path.exists() else str(candidate_path)
            if key in seen:
                continue
            seen[key] = None
            if not candidate_path.exists():
                continue
            try:
                secret_bytes = candidate_path.read_bytes()
            except Exception as exc:
                raise RuntimeError(f"Failed to read GCS secret key {candidate_path}: {exc}") from exc
            try:
                sig_obj = instantiate(secret_bytes, target_suite["sig_name"])  # type: ignore[index]
            except Exception as exc:
                raise RuntimeError(
                    f"Failed to load GCS secret key {candidate_path} for suite {target_suite_id}: {exc}"
                ) from exc
            with lock:
                cache[target_suite_id] = sig_obj
            return sig_obj

        raise FileNotFoundError(f"No GCS signing secret key found for suite {target_suite_id}")

    return load_secret_for_suite


def _build_matrix_public_loader(
    *,
    suite_id: Optional[str],
    default_public_path: Optional[Path],
    initial_public: Optional[bytes],
    matrix_dir: Optional[Path] = None,
) -> Callable[[Dict[str, object]], bytes]:
    """Return loader that fetches per-suite GCS signing public keys from disk."""

    lock = threading.Lock()
    cache: Dict[str, bytes] = {}
    if suite_id and initial_public is not None:
        cache[suite_id] = initial_public

    matrix_public_dir = matrix_dir or Path("secrets/matrix")

    def load_public_for_suite(target_suite: Dict[str, object]) -> bytes:
        target_suite_id = target_suite.get("suite_id") if isinstance(target_suite, dict) else None
        if not target_suite_id:
            raise RuntimeError("Suite dictionary missing suite_id")

        with lock:
            cached = cache.get(target_suite_id)
            if cached is not None:
                return cached

        candidates = []
        if default_public_path and suite_id and target_suite_id == suite_id:
            candidates.append(default_public_path)
        candidates.append(matrix_public_dir / target_suite_id / "gcs_signing.pub")

        seen: Dict[str, None] = {}
        for candidate in candidates:
            candidate_path = candidate.expanduser()
            key = str(candidate_path.resolve()) if candidate_path.exists() else str(candidate_path)
            if key in seen:
                continue
            seen[key] = None
            if not candidate_path.exists():
                continue
            try:
                public_bytes = candidate_path.read_bytes()
            except Exception as exc:
                raise RuntimeError(f"Failed to read GCS public key {candidate_path}: {exc}") from exc
            with lock:
                cache[target_suite_id] = public_bytes
            return public_bytes

        raise FileNotFoundError(f"No GCS signing public key found for suite {target_suite_id}")

    return load_public_for_suite


def signal_handler(signum, frame):
    """Handle interrupt signals gracefully."""
    print("\nReceived interrupt signal. Shutting down...")
    sys.exit(0)


def create_secrets_dir():
    """Create secrets directory if it doesn't exist."""
    secrets_dir = Path("secrets")
    secrets_dir.mkdir(exist_ok=True)
    return secrets_dir


def write_json_report(json_path: Optional[str], payload: dict, *, quiet: bool = False) -> None:
    """Persist counters payload to JSON if a path is provided."""

    if not json_path:
        return

    try:
        path = Path(json_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        if not quiet:
            print(f"Wrote JSON report to {path}")
    except Exception as exc:
        print(f"Warning: Failed to write JSON output to {json_path}: {exc}")


def _resolve_suite(args, role_label: str) -> dict:
    """Resolve suite via legacy --suite or new --kem/--aead/--sig components."""

    suite_arg = getattr(args, "suite", None)
    kem = getattr(args, "kem", None)
    sig = getattr(args, "sig", None)
    aead = getattr(args, "aead", None)

    if suite_arg and any(v is not None for v in (kem, sig, aead)):
        print("Error: --suite cannot be combined with --kem/--sig/--aead")
        sys.exit(1)

    try:
        if suite_arg:
            suite = get_suite(suite_arg)
        elif any(v is not None for v in (kem, sig, aead)):
            if not all(v is not None for v in (kem, sig, aead)):
                print("Error: --kem, --sig, and --aead must be provided together")
                sys.exit(1)
            suite_id = build_suite_id(kem, aead, sig)
            suite = get_suite(suite_id)
        else:
            print(f"Error: {role_label} requires --suite or --kem/--sig/--aead")
            sys.exit(1)
    except NotImplementedError as exc:
        print(f"Error: {exc}")
        sys.exit(1)

    # Normalize suite argument for downstream logging
    setattr(args, "suite", suite.get("suite_id", getattr(args, "suite", None)))
    return suite


def init_identity_command(args):
    """Create GCS signing identity and save to persistent files."""
    # Use custom output_dir if provided, otherwise default secrets directory
    if hasattr(args, 'output_dir') and args.output_dir:
        secrets_dir = Path(args.output_dir)
        secrets_dir.mkdir(parents=True, exist_ok=True)
    else:
        secrets_dir = create_secrets_dir()
    
    try:
        suite = get_suite(args.suite) if hasattr(args, 'suite') and args.suite else get_suite("cs-kyber768-aesgcm-dilithium3")
    except KeyError as e:
        print(f"Error: Unknown suite: {args.suite if hasattr(args, 'suite') else 'default'}")
        sys.exit(1)
    
    secret_path = secrets_dir / "gcs_signing.key"
    public_path = secrets_dir / "gcs_signing.pub"
    
    if secret_path.exists() or public_path.exists():
        print("Warning: Identity files already exist. Overwriting with a new keypair.")
    
    Signature = _require_signature_class()

    try:
        sig = Signature(suite["sig_name"])
        if hasattr(sig, 'export_secret_key'):
            gcs_sig_public = sig.generate_keypair()
            gcs_sig_secret = sig.export_secret_key()
            
            # Write files with appropriate permissions
            secret_path.write_bytes(gcs_sig_secret)
            public_path.write_bytes(gcs_sig_public)
            
            # Secure the secret file
            try:
                os.chmod(secret_path, 0o600)
            except Exception:
                pass  # Best effort on Windows
                
            print(f"Created GCS signing identity:")
            print(f"  Secret: {secret_path}")
            print(f"  Public: {public_path}")
            print(f"  Public key (hex): {gcs_sig_public.hex()}")
            return 0  # Success
            
        else:
            print("Error: oqs build lacks key import/export; use --ephemeral or upgrade oqs-python.")
            sys.exit(1)
            
    except Exception as e:
        print(f"Error creating identity: {e}")
        sys.exit(1)


def gcs_command(args):
    """Start GCS proxy."""
    suite = _resolve_suite(args, "GCS proxy")
    suite_id = suite["suite_id"]
    
    Signature = _require_signature_class()
    proxy_runner = _require_run_proxy()

    gcs_sig_secret = None
    gcs_sig_public = None
    json_out_path = getattr(args, "json_out", None)
    quiet = getattr(args, "quiet", False)
    status_file = getattr(args, "status_file", None)
    primary_secret_path: Optional[Path] = None

    def info(msg: str) -> None:
        if not quiet:
            print(msg)
    
    if args.ephemeral:
        info("⚠️  WARNING: Using EPHEMERAL keys - not suitable for production!")
        info("⚠️  Key will be lost when process exits.")
        if not quiet:
            print()
        
        # Generate ephemeral keypair
        sig = Signature(suite["sig_name"])
        gcs_sig_public = sig.generate_keypair()
        gcs_sig_secret = sig
        info("Generated ephemeral GCS signing keypair:")
        if not quiet:
            print(f"Public key (hex): {gcs_sig_public.hex()}")
            print("Provide this to the drone via --gcs-pub-hex or --peer-pubkey-file")
            print()
        primary_secret_path = None
        
    else:
        # Load persistent key
        if args.gcs_secret_file:
            secret_path = Path(args.gcs_secret_file)
        else:
            secret_path = Path("secrets/gcs_signing.key")
            
        if not secret_path.exists():
            print(f"Error: Secret key file not found: {secret_path}")
            print("Run 'python -m core.run_proxy init-identity' to create one,")
            print("or use --ephemeral for development only.")
            sys.exit(1)
            
        secret_bytes = None
        try:
            secret_bytes = secret_path.read_bytes()
        except Exception as exc:
            print(f"Error reading secret key file: {exc}")
            sys.exit(1)

        load_errors = []
        imported_public: Optional[bytes] = None
        load_method: Optional[str] = None

        try:
            primary_sig = Signature(suite["sig_name"])
        except Exception as exc:
            load_errors.append(f"Signature ctor failed: {exc}")
            primary_sig = None  # type: ignore

        if primary_sig is not None and hasattr(primary_sig, "import_secret_key"):
            try:
                imported_public = primary_sig.import_secret_key(secret_bytes)
                gcs_sig_secret = primary_sig
                load_method = "import_secret_key"
            except Exception as exc:
                load_errors.append(f"import_secret_key failed: {exc}")

        if gcs_sig_secret is None:
            try:
                fallback_sig = Signature(suite["sig_name"], secret_key=secret_bytes)
                gcs_sig_secret = fallback_sig
                load_method = "ctor_secret_key"
            except TypeError as exc:
                load_errors.append(f"ctor secret_key unsupported: {exc}")
            except Exception as exc:
                load_errors.append(f"ctor secret_key failed: {exc}")

        if gcs_sig_secret is None:
            print("Error: oqs build lacks usable key import. Tried import_secret_key and constructor fallback without success.")
            if load_errors:
                print("Details:")
                for err in load_errors:
                    print(f"  - {err}")
            print("Consider running with --ephemeral or upgrading oqs-python/liboqs with key import support.")
            sys.exit(1)

        info("Loaded GCS signing key from file.")
        if load_method == "ctor_secret_key":
            info("Using constructor-based fallback because import/export APIs are unavailable.")

        gcs_sig_public = imported_public
        if gcs_sig_public is None:
            public_candidates = []
            if secret_path.suffix:
                public_candidates.append(secret_path.with_suffix(".pub"))
            public_candidates.append(secret_path.parent / "gcs_signing.pub")
            seen = set()
            for candidate in public_candidates:
                key = str(candidate.resolve()) if candidate.exists() else str(candidate)
                if key in seen:
                    continue
                seen.add(key)
                if candidate.exists():
                    try:
                        gcs_sig_public = candidate.read_bytes()
                        info(f"Loaded public key from {candidate}.")
                    except Exception as exc:
                        load_errors.append(f"public key read failed ({candidate}): {exc}")
                    break

        if gcs_sig_public is not None and not quiet:
            print(f"Public key (hex): {gcs_sig_public.hex()}")
        elif gcs_sig_public is None and not quiet:
            print("Warning: Could not locate public key file for display. Ensure the drone has the matching public key.")
        if not quiet:
            print()
        primary_secret_path = secret_path
    load_secret_for_suite = _build_matrix_secret_loader(
        suite_id=suite_id,
        default_secret_path=primary_secret_path,
        initial_secret=gcs_sig_secret,
        signature_cls=Signature,
    )

    try:
        log_path = configure_file_logger("gcs", logger)
        if not quiet:
            print(f"Log file: {log_path}")

        info(f"Starting GCS proxy with suite {suite_id}")
        if args.stop_seconds:
            info(f"Will auto-stop after {args.stop_seconds} seconds")
        if not quiet:
            print()
        
        counters = proxy_runner(
            role="gcs",
            suite=suite,
            cfg=CONFIG,
            gcs_sig_secret=gcs_sig_secret,
            gcs_sig_public=None,
            stop_after_seconds=args.stop_seconds,
            manual_control=getattr(args, "control_manual", False),
            quiet=quiet,
            status_file=status_file,
            load_gcs_secret=load_secret_for_suite,
        )
        
        # Log final counters as JSON
        logger.info("GCS proxy shutdown", extra={"counters": counters})
        
        if not quiet:
            print("GCS proxy stopped. Final counters:")
            for key, value in counters.items():
                print(f"  {key}: {value}")

        payload = {
            "role": "gcs",
            "suite": suite_id,
            "counters": counters,
            "ts_stop_ns": time.time_ns(),
        }
        write_json_report(json_out_path, payload, quiet=quiet)
            
    except KeyboardInterrupt:
        if not quiet:
            print("\nGCS proxy stopped by user.")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


def drone_command(args):
    """Start drone proxy."""
    suite = _resolve_suite(args, "Drone proxy")
    suite_id = suite["suite_id"]
    
    proxy_runner = _require_run_proxy()

    # Get GCS public key
    gcs_sig_public = None
    json_out_path = getattr(args, "json_out", None)
    quiet = getattr(args, "quiet", False)
    status_file = getattr(args, "status_file", None)
    primary_public_path: Optional[Path] = None

    def info(msg: str) -> None:
        if not quiet:
            print(msg)
    
    try:
        if args.peer_pubkey_file:
            pub_path = Path(args.peer_pubkey_file)
            if not pub_path.exists():
                raise FileNotFoundError(f"Public key file not found: {pub_path}")
            gcs_sig_public = pub_path.read_bytes()
            primary_public_path = pub_path
        elif args.gcs_pub_hex:
            gcs_sig_public = bytes.fromhex(args.gcs_pub_hex)
        else:
            # Try default location
            default_pub = Path("secrets/gcs_signing.pub")
            if default_pub.exists():
                gcs_sig_public = default_pub.read_bytes()
                info(f"Using GCS public key from: {default_pub}")
                primary_public_path = default_pub
            else:
                raise ValueError("No GCS public key provided. Use --peer-pubkey-file, --gcs-pub-hex, or ensure secrets/gcs_signing.pub exists.")
                
    except Exception as e:
        print(f"Error loading GCS public key: {e}")
        sys.exit(1)
    
    try:
        log_path = configure_file_logger("drone", logger)
        if not quiet:
            print(f"Log file: {log_path}")

        info(f"Starting drone proxy with suite {suite_id}")
        if args.stop_seconds:
            info(f"Will auto-stop after {args.stop_seconds} seconds")
        if not quiet:
            print()

        load_public_for_suite = _build_matrix_public_loader(
            suite_id=suite_id,
            default_public_path=primary_public_path,
            initial_public=gcs_sig_public,
        )
        
        counters = proxy_runner(
            role="drone",
            suite=suite,
            cfg=CONFIG,
            gcs_sig_secret=None,
            gcs_sig_public=gcs_sig_public,
            stop_after_seconds=args.stop_seconds,
            manual_control=False,
            quiet=quiet,
            status_file=status_file,
            load_gcs_public=load_public_for_suite,
        )
        
        # Log final counters as JSON
        logger.info("Drone proxy shutdown", extra={"counters": counters})
        
        if not quiet:
            print("Drone proxy stopped. Final counters:")
            for key, value in counters.items():
                print(f"  {key}: {value}")

        payload = {
            "role": "drone",
            "suite": suite_id,
            "counters": counters,
            "ts_stop_ns": time.time_ns(),
        }
        write_json_report(json_out_path, payload, quiet=quiet)
            
    except KeyboardInterrupt:
        if not quiet:
            print("\nDrone proxy stopped by user.")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


def main():
    """Main CLI entrypoint with subcommands."""
    # Set up signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)
    
    parser = argparse.ArgumentParser(description="PQC Drone-GCS Secure Proxy")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # init-identity subcommand
    init_parser = subparsers.add_parser('init-identity', 
                                       help='Create persistent GCS signing identity')
    init_parser.add_argument("--suite", default="cs-kyber768-aesgcm-dilithium3",
                            help="Cryptographic suite ID (default: cs-kyber768-aesgcm-dilithium3)")
    init_parser.add_argument("--output-dir", 
                            help="Directory for key files (default: secrets/)")
    
    # gcs subcommand
    gcs_parser = subparsers.add_parser('gcs', help='Start GCS proxy')
    gcs_parser.add_argument("--suite",
                           help="Cryptographic suite ID (e.g., cs-kyber768-aesgcm-dilithium3)")
    gcs_parser.add_argument("--kem",
                           help="KEM alias (e.g., ML-KEM-768, kyber768)")
    gcs_parser.add_argument("--aead",
                           help="AEAD alias (e.g., AES-GCM)")
    gcs_parser.add_argument("--sig",
                           help="Signature alias (e.g., ML-DSA-65, dilithium3)")
    gcs_parser.add_argument("--gcs-secret-file",
                           help="Path to GCS secret key file (default: secrets/gcs_signing.key)")
    gcs_parser.add_argument("--ephemeral", action='store_true',
                           help="Use ephemeral keys (development only - prints warning)")
    gcs_parser.add_argument("--stop-seconds", type=float,
                           help="Auto-stop after N seconds (for testing)")
    gcs_parser.add_argument("--quiet", action="store_true",
                           help="Suppress informational prints (warnings/errors still shown)")
    gcs_parser.add_argument("--json-out",
                           help="Optional path to write counters JSON on shutdown")
    gcs_parser.add_argument("--control-manual", action="store_true",
                           help="Enable interactive manual in-band rekey control thread")
    gcs_parser.add_argument("--status-file",
                           help="Path to write proxy status JSON updates (handshake/rekey)")
    
    # drone subcommand
    drone_parser = subparsers.add_parser('drone', help='Start drone proxy')
    drone_parser.add_argument("--suite",
                             help="Cryptographic suite ID (e.g., cs-kyber768-aesgcm-dilithium3)")
    drone_parser.add_argument("--kem",
                             help="KEM alias (e.g., ML-KEM-768, kyber768)")
    drone_parser.add_argument("--aead",
                             help="AEAD alias (e.g., AES-GCM)")
    drone_parser.add_argument("--sig",
                             help="Signature alias (e.g., ML-DSA-65, dilithium3)")
    drone_parser.add_argument("--peer-pubkey-file",
                             help="Path to GCS public key file (default: secrets/gcs_signing.pub)")
    drone_parser.add_argument("--gcs-pub-hex",
                             help="GCS public key as hex string")
    drone_parser.add_argument("--stop-seconds", type=float,
                             help="Auto-stop after N seconds (for testing)")
    drone_parser.add_argument("--quiet", action="store_true",
                              help="Suppress informational prints (warnings/errors still shown)")
    drone_parser.add_argument("--json-out",
                              help="Optional path to write counters JSON on shutdown")
    drone_parser.add_argument("--status-file",
                              help="Path to write proxy status JSON updates (handshake/rekey)")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Validate required CONFIG keys
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX", 
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX", 
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW"
    ]
    
    missing_keys = [key for key in required_keys if key not in CONFIG]
    if missing_keys:
        print(f"Error: CONFIG missing required keys: {', '.join(missing_keys)}")
        sys.exit(1)
    
    # Route to appropriate command handler
    if args.command == 'init-identity':
        init_identity_command(args)
    elif args.command == 'gcs':
        if getattr(args, "quiet", False):
            logger.setLevel(logging.WARNING)
        gcs_command(args)
    elif args.command == 'drone':
        if getattr(args, "quiet", False):
            logger.setLevel(logging.WARNING)
        drone_command(args)


if __name__ == "__main__":
    main()

============================================================

FILE 11/11: suites.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\suites.py
Size: 15,926 bytes
Modified: 2025-10-09 06:26:41
------------------------------------------------------------
"""PQC cryptographic suite registry and algorithm ID mapping.

Provides a composable {KEM × AEAD × SIG} registry with synonym resolution and
helpers for querying oqs availability.
"""

from __future__ import annotations

from types import MappingProxyType
from typing import Dict, Iterable, Tuple


def _normalize_alias(value: str) -> str:
    """Normalize alias strings for case- and punctuation-insensitive matching."""

    return "".join(ch for ch in value.lower() if ch.isalnum())


_KEM_REGISTRY = {
    "mlkem512": {
        "oqs_name": "ML-KEM-512",
        "token": "mlkem512",
        "nist_level": "L1",
        "kem_id": 1,
        "kem_param_id": 1,
        "aliases": (
            "ML-KEM-512",
            "ml-kem-512",
            "mlkem512",
            "kyber512",
            "kyber-512",
            "kyber_512",
        ),
    },
    "mlkem768": {
        "oqs_name": "ML-KEM-768",
        "token": "mlkem768",
        "nist_level": "L3",
        "kem_id": 1,
        "kem_param_id": 2,
        "aliases": (
            "ML-KEM-768",
            "ml-kem-768",
            "mlkem768",
            "kyber768",
            "kyber-768",
            "kyber_768",
        ),
    },
    "mlkem1024": {
        "oqs_name": "ML-KEM-1024",
        "token": "mlkem1024",
        "nist_level": "L5",
        "kem_id": 1,
        "kem_param_id": 3,
        "aliases": (
            "ML-KEM-1024",
            "ml-kem-1024",
            "mlkem1024",
            "kyber1024",
            "kyber-1024",
            "kyber_1024",
        ),
    },
    "frodokem640aes": {
        "oqs_name": "FrodoKEM-640-AES",
        "token": "frodokem640aes",
        "nist_level": "L1",
        "kem_id": 2,
        "kem_param_id": 1,
        "aliases": (
            "FrodoKEM-640-AES",
            "frodokem-640-aes",
            "frodokem640aes",
            "frodokem640",
        ),
    },
    "frodokem976aes": {
        "oqs_name": "FrodoKEM-976-AES",
        "token": "frodokem976aes",
        "nist_level": "L3",
        "kem_id": 2,
        "kem_param_id": 2,
        "aliases": (
            "FrodoKEM-976-AES",
            "frodokem-976-aes",
            "frodokem976aes",
            "frodokem976",
        ),
    },
    "classicmceliece348864": {
        "oqs_name": "Classic-McEliece-348864",
        "token": "classicmceliece348864",
        "nist_level": "L1",
        "kem_id": 3,
        "kem_param_id": 1,
        "aliases": (
            "Classic-McEliece-348864",
            "classicmceliece-348864",
            "classicmceliece348864",
        ),
    },
    "classicmceliece460896": {
        "oqs_name": "Classic-McEliece-460896",
        "token": "classicmceliece460896",
        "nist_level": "L3",
        "kem_id": 3,
        "kem_param_id": 2,
        "aliases": (
            "Classic-McEliece-460896",
            "classicmceliece-460896",
            "classicmceliece460896",
        ),
    },
    "classicmceliece8192128": {
        "oqs_name": "Classic-McEliece-8192128",
        "token": "classicmceliece8192128",
        "nist_level": "L5",
        "kem_id": 3,
        "kem_param_id": 3,
        "aliases": (
            "Classic-McEliece-8192128",
            "classicmceliece-8192128",
            "classicmceliece8192128",
        ),
    },
    "hqc128": {
        "oqs_name": "HQC-128",
        "token": "hqc128",
        "nist_level": "L1",
        "kem_id": 5,
        "kem_param_id": 1,
        "aliases": (
            "HQC-128",
            "hqc-128",
            "hqc128",
        ),
    },
    "hqc192": {
        "oqs_name": "HQC-192",
        "token": "hqc192",
        "nist_level": "L3",
        "kem_id": 5,
        "kem_param_id": 2,
        "aliases": (
            "HQC-192",
            "hqc-192",
            "hqc192",
        ),
    },
    "hqc256": {
        "oqs_name": "HQC-256",
        "token": "hqc256",
        "nist_level": "L5",
        "kem_id": 5,
        "kem_param_id": 3,
        "aliases": (
            "HQC-256",
            "hqc-256",
            "hqc256",
        ),
    },
    "sntrup761": {
        "oqs_name": "sntrup761",
        "token": "sntrup761",
        "nist_level": "L1",
        "kem_id": 4,
        "kem_param_id": 1,
        "aliases": (
            "sntrup761",
            "sntrup-761",
        ),
    },
}


_SIG_REGISTRY = {
    "mldsa44": {
        "oqs_name": "ML-DSA-44",
        "token": "mldsa44",
        "nist_level": "L1",
        "sig_id": 1,
        "sig_param_id": 1,
        "aliases": (
            "ML-DSA-44",
            "ml-dsa-44",
            "mldsa44",
            "dilithium2",
            "dilithium-2",
        ),
    },
    "mldsa65": {
        "oqs_name": "ML-DSA-65",
        "token": "mldsa65",
        "nist_level": "L3",
        "sig_id": 1,
        "sig_param_id": 2,
        "aliases": (
            "ML-DSA-65",
            "ml-dsa-65",
            "mldsa65",
            "dilithium3",
            "dilithium-3",
        ),
    },
    "mldsa87": {
        "oqs_name": "ML-DSA-87",
        "token": "mldsa87",
        "nist_level": "L5",
        "sig_id": 1,
        "sig_param_id": 3,
        "aliases": (
            "ML-DSA-87",
            "ml-dsa-87",
            "mldsa87",
            "dilithium5",
            "dilithium-5",
        ),
    },
    "falcon512": {
        "oqs_name": "Falcon-512",
        "token": "falcon512",
        "nist_level": "L1",
        "sig_id": 2,
        "sig_param_id": 1,
        "aliases": (
            "Falcon-512",
            "falcon512",
            "falcon-512",
        ),
    },
    "falcon1024": {
        "oqs_name": "Falcon-1024",
        "token": "falcon1024",
        "nist_level": "L5",
        "sig_id": 2,
        "sig_param_id": 2,
        "aliases": (
            "Falcon-1024",
            "falcon1024",
            "falcon-1024",
        ),
    },
    "sphincs128fsha2": {
        "oqs_name": "SPHINCS+-SHA2-128f-simple",
        "token": "sphincs128fsha2",
        "nist_level": "L1",
        "sig_id": 3,
        "sig_param_id": 1,
        "aliases": (
            "SLH-DSA-SHA2-128f",
            "sphincs+-sha2-128f-simple",
            "sphincs128fsha2",
            "sphincs128f_sha2",
        ),
    },
    "sphincs256fsha2": {
        "oqs_name": "SPHINCS+-SHA2-256f-simple",
        "token": "sphincs256fsha2",
        "nist_level": "L5",
        "sig_id": 3,
        "sig_param_id": 2,
        "aliases": (
            "SLH-DSA-SHA2-256f",
            "sphincs+-sha2-256f-simple",
            "sphincs256fsha2",
            "sphincs256f_sha2",
        ),
    },
}


_AEAD_REGISTRY = {
    "aesgcm": {
        "display_name": "AES-256-GCM",
        "token": "aesgcm",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "AES-256-GCM",
            "aes-256-gcm",
            "aesgcm",
            "aes256gcm",
            "aes-gcm",
        ),
    },
    "chacha20poly1305": {
        "display_name": "ChaCha20-Poly1305",
        "token": "chacha20poly1305",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "ChaCha20-Poly1305",
            "chacha20poly1305",
            "chacha20-poly1305",
            "chacha20",
        ),
    },
    "ascon128": {
        "display_name": "ASCON-128",
        "token": "ascon128",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "ASCON-128",
            "ascon-128",
            "ascon128",
        ),
    },
}


def _build_alias_map(registry: Dict[str, Dict]) -> Dict[str, str]:
    alias_map: Dict[str, str] = {}
    for key, entry in registry.items():
        for alias in entry["aliases"]:
            normalized = _normalize_alias(alias)
            alias_map[normalized] = key
        alias_map[_normalize_alias(entry["oqs_name"]) if "oqs_name" in entry else _normalize_alias(entry["display_name"])] = key
        alias_map[_normalize_alias(entry["token"])] = key
    return alias_map


_KEM_ALIASES = _build_alias_map(_KEM_REGISTRY)
_SIG_ALIASES = _build_alias_map(_SIG_REGISTRY)
_AEAD_ALIASES = _build_alias_map(_AEAD_REGISTRY)


def _resolve_kem_key(name: str) -> str:
    lookup = _KEM_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise NotImplementedError(f"unknown KEM: {name}")
    return lookup


def _resolve_sig_key(name: str) -> str:
    lookup = _SIG_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise NotImplementedError(f"unknown signature: {name}")
    return lookup


def _resolve_aead_key(name: str) -> str:
    lookup = _AEAD_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise NotImplementedError(f"unknown AEAD: {name}")
    return lookup


def build_suite_id(kem: str, aead: str, sig: str) -> str:
    """Build canonical suite identifier from component aliases."""

    kem_key = _resolve_kem_key(kem)
    aead_key = _resolve_aead_key(aead)
    sig_key = _resolve_sig_key(sig)

    kem_entry = _KEM_REGISTRY[kem_key]
    aead_entry = _AEAD_REGISTRY[aead_key]
    sig_entry = _SIG_REGISTRY[sig_key]

    return f"cs-{kem_entry['token']}-{aead_entry['token']}-{sig_entry['token']}"


_SUITE_ALIASES = {
    "cs-kyber512-aesgcm-dilithium2": "cs-mlkem512-aesgcm-mldsa44",
    "cs-kyber768-aesgcm-dilithium3": "cs-mlkem768-aesgcm-mldsa65",
    "cs-kyber1024-aesgcm-dilithium5": "cs-mlkem1024-aesgcm-mldsa87",
    "cs-kyber512-aesgcm-falcon512": "cs-mlkem512-aesgcm-falcon512",
    "cs-kyber768-aesgcm-falcon512": "cs-mlkem768-aesgcm-falcon512",
    "cs-kyber1024-aesgcm-falcon1024": "cs-mlkem1024-aesgcm-falcon1024",
    "cs-kyber512-aesgcm-sphincs128f_sha2": "cs-mlkem512-aesgcm-sphincs128fsha2",
    "cs-kyber1024-aesgcm-sphincs256f_sha2": "cs-mlkem1024-aesgcm-sphincs256fsha2",
}


def _compose_suite(kem_key: str, aead_key: str, sig_key: str) -> Dict[str, object]:
    kem_entry = _KEM_REGISTRY[kem_key]
    aead_entry = _AEAD_REGISTRY[aead_key]
    sig_entry = _SIG_REGISTRY[sig_key]

    if kem_entry["nist_level"] != sig_entry["nist_level"]:
        raise NotImplementedError(
            f"NIST level mismatch for {kem_entry['oqs_name']} / {sig_entry['oqs_name']}"
        )

    suite_id = f"cs-{kem_entry['token']}-{aead_entry['token']}-{sig_entry['token']}"

    return {
        "suite_id": suite_id,
        "kem_name": kem_entry["oqs_name"],
        "kem_id": kem_entry["kem_id"],
        "kem_param_id": kem_entry["kem_param_id"],
        "sig_name": sig_entry["oqs_name"],
        "sig_id": sig_entry["sig_id"],
        "sig_param_id": sig_entry["sig_param_id"],
        "nist_level": kem_entry["nist_level"],
        "aead": aead_entry["display_name"],
        "kdf": aead_entry["kdf"],
        "aead_token": aead_entry["token"],
    }

_SUITE_MATRIX: Tuple[Tuple[str, str], ...] = (
    ("mlkem512", "mldsa44"),
    ("mlkem512", "falcon512"),
    ("mlkem512", "sphincs128fsha2"),
    ("frodokem640aes", "mldsa44"),
    ("classicmceliece348864", "sphincs128fsha2"),
    ("hqc128", "falcon512"),
    ("mlkem768", "mldsa65"),
    ("frodokem976aes", "mldsa65"),
    ("classicmceliece460896", "mldsa65"),
    ("hqc192", "mldsa65"),
    ("mlkem1024", "mldsa87"),
    ("mlkem1024", "falcon1024"),
    ("mlkem1024", "sphincs256fsha2"),
    ("classicmceliece8192128", "sphincs256fsha2"),
    ("hqc256", "mldsa87"),
)

_AEAD_ORDER: Tuple[str, ...] = ("aesgcm", "chacha20poly1305", "ascon128")


def _canonicalize_suite_id(suite_id: str) -> str:
    if not suite_id:
        raise NotImplementedError("suite_id cannot be empty")

    candidate = suite_id.strip()
    if candidate in _SUITE_ALIASES:
        return _SUITE_ALIASES[candidate]

    if not candidate.startswith("cs-"):
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    parts = candidate[3:].split("-")
    if len(parts) < 3:
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    kem_part = parts[0]
    aead_part = parts[1]
    sig_part = "-".join(parts[2:])

    try:
        return build_suite_id(kem_part, aead_part, sig_part)
    except NotImplementedError as exc:
        raise NotImplementedError(f"unknown suite_id: {suite_id}") from exc


def _generate_suite_registry() -> MappingProxyType:
    suites: Dict[str, MappingProxyType] = {}
    for kem_key, sig_key in _SUITE_MATRIX:
        if kem_key not in _KEM_REGISTRY:
            raise NotImplementedError(f"unknown KEM in suite matrix: {kem_key}")
        if sig_key not in _SIG_REGISTRY:
            raise NotImplementedError(f"unknown signature in suite matrix: {sig_key}")
        for aead_key in _AEAD_ORDER:
            suite_dict = _compose_suite(kem_key, aead_key, sig_key)
            suites[suite_dict["suite_id"]] = MappingProxyType(suite_dict)
    return MappingProxyType(suites)


SUITES = _generate_suite_registry()


def list_suites() -> Dict[str, Dict]:
    """Return all available suites as immutable mapping."""

    return {suite_id: dict(config) for suite_id, config in SUITES.items()}


def get_suite(suite_id: str) -> Dict:
    """Get suite configuration by ID, resolving legacy aliases and synonyms."""

    canonical_id = _canonicalize_suite_id(suite_id)

    if canonical_id not in SUITES:
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    suite = SUITES[canonical_id]

    required_fields = {"kem_name", "sig_name", "aead", "kdf", "nist_level"}
    missing_fields = required_fields - set(suite.keys())
    if missing_fields:
        raise NotImplementedError(f"malformed suite {suite_id}: missing fields {missing_fields}")

    return dict(suite)


def _safe_get_enabled_kem_mechanisms() -> Iterable[str]:
    from oqs.oqs import get_enabled_KEM_mechanisms

    return get_enabled_KEM_mechanisms()


def _safe_get_enabled_sig_mechanisms() -> Iterable[str]:
    from oqs.oqs import get_enabled_sig_mechanisms

    return get_enabled_sig_mechanisms()


def enabled_kems() -> Tuple[str, ...]:
    """Return tuple of oqs KEM mechanism names supported by the runtime."""

    mechanisms = {_normalize_alias(name) for name in _safe_get_enabled_kem_mechanisms()}
    result = [
        entry["oqs_name"]
        for entry in _KEM_REGISTRY.values()
        if _normalize_alias(entry["oqs_name"]) in mechanisms
    ]
    return tuple(result)


def enabled_sigs() -> Tuple[str, ...]:
    """Return tuple of oqs signature mechanism names supported by the runtime."""

    mechanisms = {_normalize_alias(name) for name in _safe_get_enabled_sig_mechanisms()}
    result = [
        entry["oqs_name"]
        for entry in _SIG_REGISTRY.values()
        if _normalize_alias(entry["oqs_name"]) in mechanisms
    ]
    return tuple(result)


def header_ids_for_suite(suite: Dict) -> Tuple[int, int, int, int]:
    """Return embedded header ID bytes for provided suite dict copy."""

    try:
        return (
            suite["kem_id"],
            suite["kem_param_id"],
            suite["sig_id"],
            suite["sig_param_id"],
        )
    except KeyError as e:
        raise NotImplementedError(f"suite missing embedded id field: {e}")


def suite_bytes_for_hkdf(suite: Dict) -> bytes:
    """Generate deterministic bytes from suite for HKDF info parameter."""

    if "suite_id" in suite:
        return suite["suite_id"].encode("utf-8")

    try:
        suite_id = build_suite_id(suite["kem_name"], suite["aead"], suite["sig_name"])
    except (KeyError, NotImplementedError) as exc:
        raise NotImplementedError("Suite configuration not found in registry") from exc

    return suite_id.encode("utf-8")

============================================================
#!/usr/bin/env python3
"""Drone follower/loopback agent driven entirely by core configuration.

This script launches the drone proxy, exposes the TCP control channel for the
GCS scheduler, and runs the plaintext UDP echo used to validate the encrypted
path. All network endpoints originate from :mod:`core.config`. Test behaviour
can be tuned via optional CLI flags (e.g. to disable perf monitors), but no
network parameters are duplicated here.
"""

from __future__ import annotations

import sys
from pathlib import Path


def _ensure_core_importable() -> Path:
    """Guarantee the repository root is on sys.path before importing core."""

    root = Path(__file__).resolve().parents[2]
    root_str = str(root)
    if root_str not in sys.path:
        sys.path.insert(0, root_str)
    try:
        __import__("core")
    except ModuleNotFoundError as exc:
        raise RuntimeError(
            f"Unable to import 'core'; repo root {root} missing from sys.path."
        ) from exc
    return root


ROOT = _ensure_core_importable()

import argparse
import csv
import json
import math
import os
import shlex
import signal
import socket
import struct
import subprocess
import threading
import time
import queue
from datetime import datetime, timezone
from copy import deepcopy
from typing import IO, Optional, Tuple


def optimize_cpu_performance(target_khz: int = 1800000) -> None:
    governors = list(Path("/sys/devices/system/cpu").glob("cpu[0-9]*/cpufreq"))
    for governor_dir in governors:
        gov = governor_dir / "scaling_governor"
        min_freq = governor_dir / "scaling_min_freq"
        max_freq = governor_dir / "scaling_max_freq"
        try:
            if gov.exists():
                gov.write_text("performance\n", encoding="utf-8")
            if min_freq.exists():
                min_freq.write_text(f"{target_khz}\n", encoding="utf-8")
            if max_freq.exists():
                current_max = int(max_freq.read_text().strip())
                if current_max < target_khz:
                    max_freq.write_text(f"{target_khz}\n", encoding="utf-8")
        except PermissionError:
            print("[follower] insufficient permissions to adjust CPU governor")
        except Exception as exc:
            print(f"[follower] governor tuning failed: {exc}")


import psutil

from core.config import CONFIG
from core import suites as suites_mod
from core.power_monitor import (
    PowerMonitor,
    PowerMonitorUnavailable,
    PowerSummary,
    create_power_monitor,
)

from bench_models import calculate_predicted_flight_constraint


CONTROL_HOST = CONFIG.get("DRONE_CONTROL_HOST", "0.0.0.0")
CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_BIND_HOST = CONFIG.get("DRONE_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
APP_SEND_HOST = CONFIG.get("DRONE_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))

DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

TELEMETRY_DEFAULT_HOST = (
    CONFIG.get("DRONE_TELEMETRY_HOST")
    or CONFIG.get("GCS_HOST")
    or "127.0.0.1"
)
TELEMETRY_DEFAULT_PORT = int(
    CONFIG.get("DRONE_TELEMETRY_PORT")
    or CONFIG.get("GCS_TELEMETRY_PORT")
    or 52080
)

OUTDIR = ROOT / "logs/auto/drone"
MARK_DIR = OUTDIR / "marks"
SECRETS_DIR = ROOT / "secrets/matrix"

PI4_TARGET_KHZ = 1_800_000
PI5_TARGET_KHZ = 2_400_000

DEFAULT_MONITOR_BASE = Path(
    CONFIG.get("DRONE_MONITOR_OUTPUT_BASE")
    or os.getenv("DRONE_MONITOR_OUTPUT_BASE", "/home/dev/research/output/drone")
)
LOG_INTERVAL_MS = 100

GRAVITY = 9.80665  # m/s^2, standard gravity for synthetic flight modeling

PERF_EVENTS = "task-clock,cycles,instructions,cache-misses,branch-misses,context-switches,branches"

_VCGENCMD_WARNING_EMITTED = False


def _warn_vcgencmd_unavailable() -> None:
    global _VCGENCMD_WARNING_EMITTED
    if not _VCGENCMD_WARNING_EMITTED:
        print("[monitor] vcgencmd not available; thermal metrics disabled")
        _VCGENCMD_WARNING_EMITTED = True


def _merge_defaults(defaults: dict, override: Optional[dict]) -> dict:
    result = deepcopy(defaults)
    if isinstance(override, dict):
        for key, value in override.items():
            if isinstance(value, dict) and isinstance(result.get(key), dict):
                merged = result[key].copy()
                merged.update(value)
                result[key] = merged
            else:
                result[key] = value
    return result

AUTO_DRONE_DEFAULTS = {
    "session_prefix": "session",
    "monitors_enabled": True,
    "cpu_optimize": True,
    "telemetry_enabled": True,
    "telemetry_host": None,
    "telemetry_port": TELEMETRY_DEFAULT_PORT,
    "monitor_output_base": None,
    "power_env": {},
    "initial_suite": None,
    "mock_mass_kg": 6.5,
    "kinematics_horizontal_mps": 13.0,
    "kinematics_vertical_mps": 3.5,
    "kinematics_cycle_s": 18.0,
    "kinematics_yaw_rate_dps": 45.0,
}

AUTO_DRONE_CONFIG = _merge_defaults(AUTO_DRONE_DEFAULTS, CONFIG.get("AUTO_DRONE"))


def _parse_args(argv: Optional[list[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Drone follower controller")
    parser.add_argument(
        "--5",
        "--pi5",
        dest="pi5",
        action="store_true",
        help="Treat hardware as Raspberry Pi 5 (defaults to Pi 4 governor settings)",
    )
    parser.add_argument(
        "--pi4",
        dest="pi5",
        action="store_false",
        help=argparse.SUPPRESS,
    )
    parser.set_defaults(pi5=False)
    return parser.parse_args(argv)


def ts() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def log_runtime_environment(component: str) -> None:
    """Emit interpreter context to help debug sudo/venv mismatches."""

    preview = ";".join(sys.path[:5])
    print(f"[{ts()}] {component} python_exe={sys.executable}")
    print(f"[{ts()}] {component} cwd={Path.cwd()}")
    print(f"[{ts()}] {component} sys.path_prefix={preview}")


class TelemetryPublisher:
    """Best-effort telemetry pipe from the drone follower to the GCS scheduler."""

    def __init__(self, host: str, port: int, session_id: str) -> None:
        self.host = host
        self.port = port
        self.session_id = session_id
        self.queue: "queue.Queue[dict]" = queue.Queue(maxsize=5000)
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.sock: Optional[socket.socket] = None
        self.writer = None
        self._connect_attempts = 0
        self._max_connect_attempts = 60
        self._initial_backoff = 1.0
        self._connect_deadline_s = 60.0
        self._connect_start_monotonic = time.monotonic()
        self._failure_first_monotonic: Optional[float] = None
        self._last_failure_log = 0.0
        self._throttle_after_s = 60.0
        self._throttle_interval_s = 60.0

    def start(self) -> None:
        self.thread.start()

    def publish(self, kind: str, payload: dict) -> None:
        if self.stop_event.is_set():
            return
        message = {
            "session_id": self.session_id,
            "kind": kind,
            **payload,
        }
        message["component"] = "drone_follower"
        try:
            self.queue.put_nowait(message)
        except queue.Full:
            # Drop oldest by removing one item to make space, then enqueue.
            try:
                self.queue.get_nowait()
            except queue.Empty:
                pass
            try:
                self.queue.put_nowait(message)
            except queue.Full:
                pass

    def stop(self) -> None:
        self.stop_event.set()
        if self.thread.is_alive():
            self.thread.join(timeout=2.0)
        self._close_socket()

    def _close_socket(self) -> None:
        if self.writer is not None:
            try:
                self.writer.close()
            except Exception:
                pass
            self.writer = None
        if self.sock is not None:
            try:
                self.sock.close()
            except Exception:
                pass
            self.sock = None

    def _ensure_connection(self) -> bool:
        if self.sock is not None and self.writer is not None:
            return True
        return self._attempt_connection()

    def _attempt_connection(self) -> bool:
        self._connect_attempts += 1
        attempt = self._connect_attempts
        try:
            sock = socket.create_connection((self.host, self.port), timeout=3.0)
        except OSError as exc:
            elapsed = time.monotonic() - self._connect_start_monotonic
            self._log_connect_failure(attempt, exc, elapsed)
            self._close_socket()
            return False
        else:
            self.sock = sock
            self.writer = sock.makefile("w", encoding="utf-8", buffering=1)
            hello = {
                "session_id": self.session_id,
                "kind": "telemetry_hello",
                "timestamp_ns": time.time_ns(),
            }
            self.writer.write(json.dumps(hello) + "\n")
            self.writer.flush()
            print(
                f"[follower] telemetry connected to {self.host}:{self.port} on attempt {attempt}",
                flush=True,
            )
            self._connect_attempts = 0
            self._connect_start_monotonic = time.monotonic()
            self._failure_first_monotonic = None
            self._last_failure_log = 0.0
            return True

    def _log_connect_failure(self, attempt: int, exc: Exception, elapsed_since_start: float) -> None:
        now = time.monotonic()
        if self._failure_first_monotonic is None:
            self._failure_first_monotonic = now
        elapsed_total = now - self._failure_first_monotonic
        should_log = True
        if elapsed_total >= self._throttle_after_s:
            if now - self._last_failure_log < self._throttle_interval_s:
                should_log = False
        if should_log:
            print(
                f"[follower] telemetry connect attempt {attempt}/{self._max_connect_attempts} to {self.host}:{self.port} failed after {elapsed_since_start:.1f}s: {exc}",
                flush=True,
            )
            self._last_failure_log = now
            if elapsed_total >= self._throttle_after_s and attempt >= self._max_connect_attempts:
                print(
                    f"[follower] telemetry collector still unavailable at {self.host}:{self.port}; throttling failure logs but continuing retries",
                    flush=True,
                )

    def _run(self) -> None:
        backoff = self._initial_backoff
        while not self.stop_event.is_set():
            if not self._ensure_connection():
                time.sleep(min(backoff, 5.0))
                backoff = min(backoff * 1.5, 5.0)
                continue
            backoff = self._initial_backoff
            try:
                item = self.queue.get(timeout=0.5)
            except queue.Empty:
                continue
            try:
                if self.writer is None:
                    continue
                if "timestamp_ns" not in item:
                    item["timestamp_ns"] = time.time_ns()
                self.writer.write(json.dumps(item) + "\n")
                self.writer.flush()
            except Exception as exc:
                print(f"[follower] telemetry send failed: {exc}")
                self._close_socket()


class SyntheticKinematicsModel:
    """Deterministic mock flight profile used for telemetry and PFC estimation."""

    def __init__(
        self,
        *,
        weight_n: float,
        horizontal_peak_mps: float,
        vertical_peak_mps: float,
        yaw_rate_dps: float,
        cycle_s: float,
    ) -> None:
        self.weight_n = max(0.0, weight_n)
        self.horizontal_peak_mps = max(0.0, horizontal_peak_mps)
        self.vertical_peak_mps = float(vertical_peak_mps)
        self.yaw_rate_dps = float(yaw_rate_dps)
        self.cycle_s = max(4.0, float(cycle_s))
        self._start_monotonic = time.monotonic()
        self._last_monotonic = self._start_monotonic
        self._altitude_m = 30.0
        self._heading_rad = 0.0
        self._prev_horizontal_mps = 0.0
        self._prev_vertical_mps = 0.0
        self._sequence = 0

    def _phase(self, now: float) -> float:
        elapsed = now - self._start_monotonic
        return (elapsed % self.cycle_s) / self.cycle_s

    def step(self, timestamp_ns: int) -> dict:
        now = time.monotonic()
        dt = max(0.0, now - self._last_monotonic)
        self._last_monotonic = now
        phase = self._phase(now)
        phase_rad = 2.0 * math.pi * phase

        horiz_mps = self.horizontal_peak_mps * math.sin(phase_rad)
        vert_mps = self.vertical_peak_mps * math.sin(phase_rad + math.pi / 3.0)
        speed_mps = math.hypot(horiz_mps, vert_mps)

        yaw_rate_rps = math.radians(self.yaw_rate_dps) * math.cos(phase_rad + math.pi / 6.0)
        self._heading_rad = (self._heading_rad + yaw_rate_rps * dt) % (2.0 * math.pi)
        self._altitude_m = max(0.0, self._altitude_m + vert_mps * dt)

        horiz_accel = 0.0 if dt == 0.0 else (horiz_mps - self._prev_horizontal_mps) / dt
        vert_accel = 0.0 if dt == 0.0 else (vert_mps - self._prev_vertical_mps) / dt
        self._prev_horizontal_mps = horiz_mps
        self._prev_vertical_mps = vert_mps

        pfc_w = calculate_predicted_flight_constraint(abs(horiz_mps), vert_mps, self.weight_n)
        tilt_deg = math.degrees(math.atan2(abs(vert_mps), max(0.1, abs(horiz_mps))))

        self._sequence += 1
        return {
            "timestamp_ns": timestamp_ns,
            "sequence": self._sequence,
            "velocity_horizontal_mps": horiz_mps,
            "velocity_vertical_mps": vert_mps,
            "speed_mps": speed_mps,
            "horizontal_accel_mps2": horiz_accel,
            "vertical_accel_mps2": vert_accel,
            "yaw_rate_dps": math.degrees(yaw_rate_rps),
            "heading_deg": math.degrees(self._heading_rad),
            "altitude_m": self._altitude_m,
            "tilt_deg": tilt_deg,
            "predicted_flight_constraint_w": pfc_w,
        }


def _summary_to_dict(summary: PowerSummary, *, suite: str, session_id: str) -> dict:
    return {
        "timestamp_ns": summary.end_ns,
        "suite": suite,
        "label": summary.label,
        "session_id": session_id,
        "duration_s": summary.duration_s,
        "samples": summary.samples,
        "avg_current_a": summary.avg_current_a,
        "avg_voltage_v": summary.avg_voltage_v,
        "avg_power_w": summary.avg_power_w,
        "energy_j": summary.energy_j,
        "sample_rate_hz": summary.sample_rate_hz,
        "csv_path": summary.csv_path,
        "start_ns": summary.start_ns,
        "end_ns": summary.end_ns,
    }


class PowerCaptureManager:
    """Coordinates power captures for control commands."""

    def __init__(
        self,
        output_dir: Path,
        session_id: str,
        telemetry: Optional[TelemetryPublisher],
    ) -> None:
        self.telemetry = telemetry
        self.session_id = session_id
        self.lock = threading.Lock()
        self._thread: Optional[threading.Thread] = None
        self._last_summary: Optional[dict] = None
        self._last_error: Optional[str] = None
        self._pending_suite: Optional[str] = None
        self.monitor: Optional[PowerMonitor] = None
        self.monitor_backend: Optional[str] = None

        def _parse_int_env(name: str, default: int) -> int:
            raw = os.getenv(name)
            if not raw:
                return default
            try:
                return int(raw)
            except ValueError:
                print(f"[follower] invalid {name}={raw!r}, using {default}")
                return default

        def _parse_float_env(name: str, default: float) -> float:
            raw = os.getenv(name)
            if not raw:
                return default
            try:
                return float(raw)
            except ValueError:
                print(f"[follower] invalid {name}={raw!r}, using {default}")
                return default

        def _parse_float_optional(name: str) -> Optional[float]:
            raw = os.getenv(name)
            if raw is None or raw == "":
                return None
            try:
                return float(raw)
            except ValueError:
                print(f"[follower] invalid {name}={raw!r}, ignoring")
                return None

        backend = os.getenv("DRONE_POWER_BACKEND", "auto")
        sample_hz = _parse_int_env("DRONE_POWER_SAMPLE_HZ", 1000)
        shunt_ohm = _parse_float_env("DRONE_POWER_SHUNT_OHM", 0.1)
        sign_mode = os.getenv("DRONE_POWER_SIGN_MODE", "auto")
        hwmon_path = os.getenv("DRONE_POWER_HWMON_PATH")
        hwmon_name_hint = os.getenv("DRONE_POWER_HWMON_NAME")
        voltage_file = os.getenv("DRONE_POWER_VOLTAGE_FILE")
        current_file = os.getenv("DRONE_POWER_CURRENT_FILE")
        power_file = os.getenv("DRONE_POWER_POWER_FILE")
        voltage_scale = _parse_float_optional("DRONE_POWER_VOLTAGE_SCALE")
        current_scale = _parse_float_optional("DRONE_POWER_CURRENT_SCALE")
        power_scale = _parse_float_optional("DRONE_POWER_POWER_SCALE")

        try:
            self.monitor = create_power_monitor(
                output_dir,
                backend=backend,
                sample_hz=sample_hz,
                shunt_ohm=shunt_ohm,
                sign_mode=sign_mode,
                hwmon_path=hwmon_path,
                hwmon_name_hint=hwmon_name_hint,
                voltage_file=voltage_file,
                current_file=current_file,
                power_file=power_file,
                voltage_scale=voltage_scale,
                current_scale=current_scale,
                power_scale=power_scale,
            )
            self.available = True
            self.monitor_backend = getattr(self.monitor, "backend_name", self.monitor.__class__.__name__)
            print(f"[follower] power monitor backend: {self.monitor_backend}")
        except PowerMonitorUnavailable as exc:
            self.monitor = None
            self.available = False
            self._last_error = str(exc)
            print(f"[follower] power monitor disabled: {exc}")
        except ValueError as exc:
            self.monitor = None
            self.available = False
            self._last_error = str(exc)
            print(f"[follower] power monitor configuration invalid: {exc}")

    def start_capture(self, suite: str, duration_s: float, start_ns: Optional[int]) -> tuple[bool, Optional[str]]:
        if not self.available or self.monitor is None:
            return False, self._last_error or "power_monitor_unavailable"
        if duration_s <= 0:
            return False, "invalid_duration"
        with self.lock:
            if self._thread and self._thread.is_alive():
                return False, "busy"
            self._last_error = None
            self._pending_suite = suite

            def worker() -> None:
                try:
                    summary = self.monitor.capture(label=suite, duration_s=duration_s, start_ns=start_ns)
                    summary_dict = _summary_to_dict(summary, suite=suite, session_id=self.session_id)
                    summary_json_path = Path(summary.csv_path).with_suffix(".json")
                    try:
                        summary_json_path.parent.mkdir(parents=True, exist_ok=True)
                        summary_json_path.write_text(json.dumps(summary_dict, indent=2), encoding="utf-8")
                        summary_dict["summary_json_path"] = str(summary_json_path)
                    except Exception as exc_json:
                        print(f"[follower] power summary write failed: {exc_json}")
                    print(
                        f"[follower] power summary suite={suite} avg={summary.avg_power_w:.3f} W "
                        f"energy={summary.energy_j:.3f} J duration={summary.duration_s:.3f}s"
                    )
                    with self.lock:
                        self._last_summary = summary_dict
                        self._pending_suite = None
                    if self.telemetry:
                        self.telemetry.publish("power_summary", dict(summary_dict))
                except Exception as exc:  # pragma: no cover - depends on hardware
                    with self.lock:
                        self._last_error = str(exc)
                        self._pending_suite = None
                    print(f"[follower] power capture failed: {exc}")
                    if self.telemetry:
                        self.telemetry.publish(
                            "power_summary_error",
                            {
                                "timestamp_ns": time.time_ns(),
                                "suite": suite,
                                "error": str(exc),
                            },
                        )
                finally:
                    with self.lock:
                        self._thread = None

            self._thread = threading.Thread(target=worker, daemon=True)
            self._thread.start()
        return True, None

    def status(self) -> dict:
        with self.lock:
            busy = bool(self._thread and self._thread.is_alive())
            summary = dict(self._last_summary) if self._last_summary else None
            error = self._last_error
            pending_suite = self._pending_suite
        return {
            "available": self.available,
            "busy": busy,
            "last_summary": summary,
            "error": error,
            "pending_suite": pending_suite,
        }



def popen(cmd, **kw) -> subprocess.Popen:
    if isinstance(cmd, (list, tuple)):
        display = " ".join(shlex.quote(str(part)) for part in cmd)
    else:
        display = str(cmd)
    print(f"[{ts()}] exec: {display}", flush=True)
    return subprocess.Popen(cmd, **kw)


def killtree(proc: Optional[subprocess.Popen]) -> None:
    if not proc or proc.poll() is not None:
        return
    try:
        proc.terminate()
        proc.wait(timeout=3)
    except Exception:
        try:
            proc.kill()
        except Exception:
            pass


def discover_initial_suite() -> str:
    configured = CONFIG.get("SIMPLE_INITIAL_SUITE")
    if configured:
        return configured

    suite_map = suites_mod.list_suites()
    if suite_map:
        return sorted(suite_map.keys())[0]

    if SECRETS_DIR.exists():
        for path in sorted(SECRETS_DIR.iterdir()):
            if (path / "gcs_signing.pub").exists():
                return path.name

    return "cs-mlkem768-aesgcm-mldsa65"


def suite_outdir(suite: str) -> Path:
    path = OUTDIR / suite
    path.mkdir(parents=True, exist_ok=True)
    return path


def suite_secrets_dir(suite: str) -> Path:
    return SECRETS_DIR / suite


def write_marker(suite: str) -> None:
    MARK_DIR.mkdir(parents=True, exist_ok=True)
    marker = MARK_DIR / f"{int(time.time())}_{suite}.json"
    with open(marker, "w", encoding="utf-8") as handle:
        json.dump({"ts": ts(), "suite": suite}, handle)


def start_drone_proxy(suite: str) -> tuple[subprocess.Popen, IO[str]]:
    suite_dir = suite_secrets_dir(suite)
    if not suite_dir.exists():
        raise FileNotFoundError(f"Suite directory missing: {suite_dir}")
    pub = suite_dir / "gcs_signing.pub"
    if not pub.exists() or not os.access(pub, os.R_OK):
        print(f"[follower] ERROR: missing {pub}", file=sys.stderr)
        sys.exit(2)

    os.environ["DRONE_HOST"] = DRONE_HOST
    os.environ["GCS_HOST"] = GCS_HOST
    os.environ["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    os.environ["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"

    suite_path = suite_outdir(suite)
    status = suite_path / "drone_status.json"
    summary = suite_path / "drone_summary.json"
    status.parent.mkdir(parents=True, exist_ok=True)
    summary.parent.mkdir(parents=True, exist_ok=True)
    OUTDIR.mkdir(parents=True, exist_ok=True)
    log_path = OUTDIR / f"drone_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_path.parent.mkdir(parents=True, exist_ok=True)
    log_handle: IO[str] = open(log_path, "w", encoding="utf-8")

    env = os.environ.copy()
    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str

    print(f"[follower] launching drone proxy on suite {suite}", flush=True)
    proc = popen([
        sys.executable,
        "-m",
        "core.run_proxy",
        "drone",
        "--suite",
        suite,
        "--peer-pubkey-file",
        str(pub),
        "--status-file",
        str(status),
        "--json-out",
        str(summary),
    ], stdout=log_handle, stderr=subprocess.STDOUT, text=True, env=env, cwd=str(ROOT))
    return proc, log_handle


class HighSpeedMonitor(threading.Thread):
    def __init__(
        self,
        output_dir: Path,
        session_id: str,
        publisher: Optional[TelemetryPublisher],
    ):
        super().__init__(daemon=True)
        self.output_dir = output_dir
        self.session_id = session_id
        self.stop_event = threading.Event()
        self.current_suite = "unknown"
        self.pending_suite: Optional[str] = None
        self.proxy_pid: Optional[int] = None
        self.rekey_start_ns: Optional[int] = None
        self.csv_handle: Optional[object] = None
        self.csv_writer: Optional[csv.writer] = None
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.csv_path = self.output_dir / f"system_monitoring_{session_id}.csv"
        self.publisher = publisher
        self._vcgencmd_available = True
        self.rekey_marks_path = self.output_dir / f"rekey_marks_{session_id}.csv"
        self._rekey_marks_lock = threading.Lock()
        auto_cfg = AUTO_DRONE_CONFIG
        mass_kg = auto_cfg.get("mock_mass_kg", 6.5)
        horiz_mps = auto_cfg.get("kinematics_horizontal_mps", 13.0)
        vert_mps = auto_cfg.get("kinematics_vertical_mps", 3.5)
        yaw_rate_dps = auto_cfg.get("kinematics_yaw_rate_dps", 45.0)
        cycle_s = auto_cfg.get("kinematics_cycle_s", 18.0)
        try:
            weight_n = max(0.0, float(mass_kg) * GRAVITY)
        except (TypeError, ValueError):
            weight_n = 0.0
        try:
            horiz_peak = float(horiz_mps)
        except (TypeError, ValueError):
            horiz_peak = 0.0
        try:
            vert_peak = float(vert_mps)
        except (TypeError, ValueError):
            vert_peak = 0.0
        try:
            yaw_peak = float(yaw_rate_dps)
        except (TypeError, ValueError):
            yaw_peak = 0.0
        try:
            cycle = float(cycle_s)
        except (TypeError, ValueError):
            cycle = 18.0
        self._kinematics_model = SyntheticKinematicsModel(
            weight_n=weight_n,
            horizontal_peak_mps=max(0.0, horiz_peak),
            vertical_peak_mps=vert_peak,
            yaw_rate_dps=yaw_peak,
            cycle_s=cycle,
        ) if weight_n > 0.0 else None

    def attach_proxy(self, pid: int) -> None:
        self.proxy_pid = pid

    def start_rekey(self, old_suite: str, new_suite: str) -> None:
        self.pending_suite = new_suite
        self.rekey_start_ns = time.time_ns()
        print(f"[monitor] rekey transition {old_suite} -> {new_suite}")
        if self.publisher:
            self.publisher.publish(
                "rekey_transition_start",
                {
                    "timestamp_ns": self.rekey_start_ns,
                    "old_suite": old_suite,
                    "new_suite": new_suite,
                    "pending_suite": new_suite,
                },
            )
        self._append_rekey_mark([
            "start",
            str(self.rekey_start_ns),
            old_suite or "",
            new_suite or "",
            self.pending_suite or "",
        ])

    def end_rekey(self, *, success: bool, new_suite: Optional[str]) -> None:
        if self.rekey_start_ns is None:
            self.pending_suite = None
            return
        duration_ms = (time.time_ns() - self.rekey_start_ns) / 1_000_000
        target_suite = new_suite or self.pending_suite or self.current_suite
        if success and new_suite:
            self.current_suite = new_suite
        status_text = "completed" if success else "failed"
        print(f"[monitor] rekey {status_text} in {duration_ms:.2f} ms (target={target_suite})")
        if self.publisher:
            payload = {
                "timestamp_ns": time.time_ns(),
                "suite": self.current_suite,
                "duration_ms": duration_ms,
                "success": success,
            }
            if target_suite:
                payload["requested_suite"] = target_suite
            if self.pending_suite:
                payload["pending_suite"] = self.pending_suite
            self.publisher.publish("rekey_transition_end", payload)
        end_timestamp = time.time_ns()
        self._append_rekey_mark([
            "end",
            str(end_timestamp),
            "ok" if success else "fail",
            target_suite or "",
            f"{duration_ms:.3f}",
        ])
        self.rekey_start_ns = None
        self.pending_suite = None

    def _append_rekey_mark(self, row: list[str]) -> None:
        try:
            self.rekey_marks_path.parent.mkdir(parents=True, exist_ok=True)
            with self._rekey_marks_lock:
                new_file = not self.rekey_marks_path.exists()
                with self.rekey_marks_path.open("a", newline="", encoding="utf-8") as handle:
                    writer = csv.writer(handle)
                    if new_file:
                        writer.writerow(["kind", "timestamp_ns", "field1", "field2", "field3"])
                    writer.writerow(row)
        except Exception as exc:
            print(f"[monitor] rekey mark append failed: {exc}")

    def run(self) -> None:
        self.csv_handle = open(self.csv_path, "w", newline="", encoding="utf-8")
        self.csv_writer = csv.writer(self.csv_handle)
        self.csv_writer.writerow(
            [
                "timestamp_iso",
                "timestamp_ns",
                "suite",
                "proxy_pid",
                "cpu_percent",
                "cpu_freq_mhz",
                "cpu_temp_c",
                "mem_used_mb",
                "mem_percent",
                "rekey_duration_ms",
            ]
        )
        interval = LOG_INTERVAL_MS / 1000.0
        while not self.stop_event.is_set():
            start = time.time()
            self._sample()
            elapsed = time.time() - start
            sleep_for = max(0.0, interval - elapsed)
            if sleep_for:
                time.sleep(sleep_for)

    def _sample(self) -> None:
        timestamp_ns = time.time_ns()
        timestamp_iso = datetime.fromtimestamp(
            timestamp_ns / 1e9,
            tz=timezone.utc,
        ).strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        cpu_percent = psutil.cpu_percent(interval=None)
        try:
            with open("/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq", "r", encoding="utf-8") as handle:
                cpu_freq_mhz = int(handle.read().strip()) / 1000.0
        except Exception:
            cpu_freq_mhz = 0.0
        cpu_temp_c = 0.0
        try:
            if self._vcgencmd_available:
                result = subprocess.run(["vcgencmd", "measure_temp"], capture_output=True, text=True)
                if result.returncode == 0 and "=" in result.stdout:
                    cpu_temp_c = float(result.stdout.split("=")[1].split("'")[0])
                else:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()
        except Exception:
            if self._vcgencmd_available:
                self._vcgencmd_available = False
                _warn_vcgencmd_unavailable()
        mem = psutil.virtual_memory()
        rekey_ms = ""
        if self.rekey_start_ns is not None:
            rekey_ms = f"{(timestamp_ns - self.rekey_start_ns) / 1_000_000:.2f}"
        if self.csv_writer is None:
            return
        self.csv_writer.writerow(
            [
                timestamp_iso,
                str(timestamp_ns),
                self.current_suite,
                self.proxy_pid or "",
                f"{cpu_percent:.1f}",
                f"{cpu_freq_mhz:.1f}",
                f"{cpu_temp_c:.1f}",
                f"{mem.used / (1024 * 1024):.1f}",
                f"{mem.percent:.1f}",
                rekey_ms,
            ]
        )
        self.csv_handle.flush()
        if self.publisher:
            sample = {
                "timestamp_ns": timestamp_ns,
                "timestamp_iso": timestamp_iso,
                "suite": self.current_suite,
                "proxy_pid": self.proxy_pid,
                "cpu_percent": cpu_percent,
                "cpu_freq_mhz": cpu_freq_mhz,
                "cpu_temp_c": cpu_temp_c,
                "mem_used_mb": mem.used / (1024 * 1024),
                "mem_percent": mem.percent,
            }
            if self.rekey_start_ns is not None:
                sample["rekey_elapsed_ms"] = (timestamp_ns - self.rekey_start_ns) / 1_000_000
            self.publisher.publish("system_sample", sample)
            if self._kinematics_model is not None:
                kin = self._kinematics_model.step(timestamp_ns)
                kin_payload = dict(kin)
                kin_payload.setdefault("suite", self.current_suite)
                kin_payload.setdefault("weight_n", self._kinematics_model.weight_n)
                kin_payload.setdefault("mass_kg", self._kinematics_model.weight_n / GRAVITY if GRAVITY else 0.0)
                self.publisher.publish("kinematics", kin_payload)

    def stop(self) -> None:
        self.stop_event.set()
        if self.is_alive():
            self.join(timeout=2.0)
        if self.csv_handle:
            self.csv_handle.close()


class UdpEcho(threading.Thread):
    def __init__(
        self,
        bind_host: str,
        recv_port: int,
        send_host: str,
        send_port: int,
        stop_event: threading.Event,
        monitor: Optional[HighSpeedMonitor],
        session_dir: Path,
        publisher: Optional[TelemetryPublisher],
    ):
        super().__init__(daemon=True)
        self.bind_host = bind_host
        self.recv_port = recv_port
        self.send_host = send_host
        self.send_port = send_port
        self.stop_event = stop_event
        self.monitor = monitor
        self.session_dir = session_dir
        self.publisher = publisher
        def _bind_socket(host: str, port: int) -> socket.socket:
            flags = socket.AI_PASSIVE if not host else 0
            try:
                addrinfo = socket.getaddrinfo(host, port, 0, socket.SOCK_DGRAM, 0, flags)
            except socket.gaierror as exc:
                raise OSError(f"UDP echo bind failed for {host}:{port}: {exc}") from exc

            last_exc: Optional[Exception] = None
            for family, socktype, proto, _canon, sockaddr in addrinfo:
                sock: Optional[socket.socket] = None
                try:
                    sock = socket.socket(family, socktype, proto)
                    try:
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    except OSError:
                        pass
                    if family == socket.AF_INET6:
                        try:
                            sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                        except OSError:
                            pass
                    sock.bind(sockaddr)
                    return sock
                except Exception as exc:
                    last_exc = exc
                    if sock is not None:
                        try:
                            sock.close()
                        except Exception:
                            pass
                    continue

            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"UDP echo bind failed for {host}:{port}: {message}")

        def _connect_tuple(host: str, port: int, preferred_family: int) -> tuple[socket.socket, tuple]:
            addrinfo: list[tuple] = []
            try:
                addrinfo = socket.getaddrinfo(host, port, preferred_family, socket.SOCK_DGRAM)
            except socket.gaierror:
                pass
            if not addrinfo:
                try:
                    addrinfo = socket.getaddrinfo(host, port, 0, socket.SOCK_DGRAM)
                except socket.gaierror as exc:
                    raise OSError(f"UDP echo resolve failed for {host}:{port}: {exc}") from exc

            last_exc: Optional[Exception] = None
            for family, socktype, proto, _canon, sockaddr in addrinfo:
                sock: Optional[socket.socket] = None
                try:
                    sock = socket.socket(family, socktype, proto)
                    try:
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    except OSError:
                        pass
                    return sock, sockaddr
                except Exception as exc:
                    last_exc = exc
                    if sock is not None:
                        try:
                            sock.close()
                        except Exception:
                            pass
                    continue

            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"UDP echo socket creation failed for {host}:{port}: {message}")

        self.rx_sock = _bind_socket(self.bind_host, self.recv_port)
        self.tx_sock, self.send_addr = _connect_tuple(self.send_host, self.send_port, self.rx_sock.family)
        try:
            sndbuf = int(os.getenv("DRONE_SOCK_SNDBUF", str(16 << 20)))
            rcvbuf = int(os.getenv("DRONE_SOCK_RCVBUF", str(16 << 20)))
            self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, rcvbuf)
            self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, sndbuf)
            actual_snd = self.tx_sock.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF)
            actual_rcv = self.rx_sock.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)
            print(
                f"[{ts()}] follower UDP socket buffers: snd={actual_snd} rcv={actual_rcv}",
                flush=True,
            )
        except Exception:
            pass
        self.packet_log_path = self.session_dir / "packet_timing.csv"
        self.packet_log_handle: Optional[object] = None
        self.packet_writer: Optional[csv.writer] = None
        self.samples = 0
        self.log_every_packet = False

    def run(self) -> None:
        print(
            f"[follower] UDP echo up: recv:{self.bind_host}:{self.recv_port} -> send:{self.send_host}:{self.send_port}",
            flush=True,
        )
        self.packet_log_handle = open(self.packet_log_path, "w", newline="", encoding="utf-8")
        self.packet_writer = csv.writer(self.packet_log_handle)
        self.packet_writer.writerow([
            "recv_timestamp_ns",
            "send_timestamp_ns",
            "processing_ns",
            "processing_ms",
            "sequence",
            "payload_len",
        ])
        self.rx_sock.settimeout(0.001)
        while not self.stop_event.is_set():
            try:
                data, _ = self.rx_sock.recvfrom(65535)
                recv_ns = time.time_ns()
                enhanced = self._annotate_packet(data, recv_ns)
                send_ns = time.time_ns()
                self.tx_sock.sendto(enhanced, self.send_addr)
                self._record_packet(data, recv_ns, send_ns)
            except socket.timeout:
                continue
            except Exception as exc:
                print(f"[follower] UDP echo error: {exc}", flush=True)
        self.rx_sock.close()
        self.tx_sock.close()
        if self.packet_log_handle:
            self.packet_log_handle.close()

    def _annotate_packet(self, data: bytes, recv_ns: int) -> bytes:
        # Last 8 bytes carry drone receive timestamp for upstream OWD inference.
        if len(data) >= 20:
            return data[:-8] + recv_ns.to_bytes(8, "big")
        return data + recv_ns.to_bytes(8, "big")

    def _record_packet(self, data: bytes, recv_ns: int, send_ns: int) -> None:
        if self.packet_writer is None or len(data) < 4:
            return
        try:
            seq, = struct.unpack("!I", data[:4])
        except struct.error:
            return
        processing_ns = send_ns - recv_ns
        monitor_active = bool(self.monitor and self.monitor.rekey_start_ns is not None)
        if monitor_active and not self.log_every_packet:
            self.log_every_packet = True
        elif not monitor_active and self.log_every_packet:
            self.log_every_packet = False

        should_log = self.log_every_packet or (seq % 100 == 0)
        if should_log:
            self.packet_writer.writerow([
                recv_ns,
                send_ns,
                processing_ns,
                f"{processing_ns / 1_000_000:.6f}",
                seq,
                len(data),
            ])
            # Always flush to prevent data loss on crashes
            if self.packet_log_handle:
                self.packet_log_handle.flush()
            if self.publisher:
                suite = self.monitor.current_suite if self.monitor else "unknown"
                self.publisher.publish(
                    "udp_echo_sample",
                    {
                        "recv_timestamp_ns": recv_ns,
                        "send_timestamp_ns": send_ns,
                        "processing_ns": processing_ns,
                        "sequence": seq,
                        "suite": suite,
                    },
                )



class Monitors:
    """Structured performance/telemetry collectors for the drone proxy."""

    PERF_FIELDS = [
        "ts_unix_ns",
        "t_offset_ms",
        "instructions",
        "cycles",
        "cache-misses",
        "branch-misses",
        "task-clock",
        "context-switches",
        "branches",
    ]

    def __init__(self, enabled: bool, telemetry: Optional[TelemetryPublisher]):
        self.enabled = enabled
        self.telemetry = telemetry
        self.perf: Optional[subprocess.Popen] = None
        self.pidstat: Optional[subprocess.Popen] = None
        self.perf_thread: Optional[threading.Thread] = None
        self.perf_stop = threading.Event()
        self.perf_csv_handle: Optional[object] = None
        self.perf_writer: Optional[csv.DictWriter] = None
        self.perf_start_ns = 0
        self.current_suite = "unknown"

        self.psutil_thread: Optional[threading.Thread] = None
        self.psutil_stop = threading.Event()
        self.psutil_csv_handle: Optional[object] = None
        self.psutil_writer: Optional[csv.DictWriter] = None
        self.psutil_proc: Optional[psutil.Process] = None

        self.temp_thread: Optional[threading.Thread] = None
        self.temp_stop = threading.Event()
        self.temp_csv_handle: Optional[object] = None
        self.temp_writer: Optional[csv.DictWriter] = None
        self.pidstat_out: Optional[IO[str]] = None
        self._vcgencmd_available = True

    def start(self, pid: int, outdir: Path, suite: str) -> None:
        if not self.enabled:
            return
        outdir.mkdir(parents=True, exist_ok=True)
        self.current_suite = suite
        self._vcgencmd_available = True

        # Structured perf samples
        perf_cmd = [
            "perf",
            "stat",
            "-I",
            "1000",
            "-x",
            ",",
            "-e",
            PERF_EVENTS,
            "-p",
            str(pid),
            "--log-fd",
            "1",
        ]
        perf_path = outdir / f"perf_samples_{suite}.csv"
        self.perf_csv_handle = open(perf_path, "w", newline="", encoding="utf-8")
        self.perf_writer = csv.DictWriter(self.perf_csv_handle, fieldnames=self.PERF_FIELDS)
        self.perf_writer.writeheader()
        self.perf_start_ns = time.time_ns()

        self.perf = popen(
            perf_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
        )
        self.perf_stop.clear()
        self.perf_thread = threading.Thread(
            target=self._consume_perf,
            args=(self.perf.stdout,),
            daemon=True,
        )
        self.perf_thread.start()

        # pidstat baseline dump for parity with legacy tooling
        self.pidstat_out = open(outdir / f"pidstat_{suite}.txt", "w", encoding="utf-8")
        self.pidstat = popen(
            ["pidstat", "-hlur", "-p", str(pid), "1"],
            stdout=self.pidstat_out,
            stderr=subprocess.STDOUT,
        )

        # psutil metrics (CPU%, RSS, threads)
        self.psutil_proc = psutil.Process(pid)
        self.psutil_proc.cpu_percent(interval=None)
        psutil_path = outdir / f"psutil_proc_{suite}.csv"
        self.psutil_csv_handle = open(psutil_path, "w", newline="", encoding="utf-8")
        self.psutil_writer = csv.DictWriter(
            self.psutil_csv_handle,
            fieldnames=["ts_unix_ns", "cpu_percent", "rss_bytes", "num_threads"],
        )
        self.psutil_writer.writeheader()
        self.psutil_stop.clear()
        self.psutil_thread = threading.Thread(target=self._psutil_loop, daemon=True)
        self.psutil_thread.start()

        # Temperature / frequency / throttled flags
        temp_path = outdir / f"sys_telemetry_{suite}.csv"
        self.temp_csv_handle = open(temp_path, "w", newline="", encoding="utf-8")
        self.temp_writer = csv.DictWriter(
            self.temp_csv_handle,
            fieldnames=["ts_unix_ns", "temp_c", "freq_hz", "throttled_hex"],
        )
        self.temp_writer.writeheader()
        self.temp_stop.clear()
        self.temp_thread = threading.Thread(target=self._telemetry_loop, daemon=True)
        self.temp_thread.start()

        if self.telemetry:
            self.telemetry.publish(
                "monitors_started",
                {
                    "timestamp_ns": time.time_ns(),
                    "suite": suite,
                    "proxy_pid": pid,
                },
            )

    def _consume_perf(self, stream) -> None:
        if not self.perf_writer:
            return
        current_ms = None
        row = None
        try:
            for line in iter(stream.readline, ""):
                if self.perf_stop.is_set():
                    break
                parts = [part.strip() for part in line.strip().split(",")]
                if len(parts) < 4:
                    continue
                try:
                    offset_ms = float(parts[0])
                except ValueError:
                    continue
                event = parts[3]
                if event.startswith("#"):
                    continue
                raw_value = parts[1].replace(",", "")
                if event == "task-clock":
                    try:
                        value = float(raw_value)
                    except Exception:
                        value = ""
                else:
                    try:
                        value = int(raw_value)
                    except Exception:
                        value = ""

                if current_ms is None or abs(offset_ms - current_ms) >= 0.5:
                    if row:
                        self.perf_writer.writerow(row)
                        self.perf_csv_handle.flush()
                    current_ms = offset_ms
                    row = {field: "" for field in self.PERF_FIELDS}
                    row["t_offset_ms"] = f"{offset_ms:.0f}"
                    row["ts_unix_ns"] = str(self.perf_start_ns + int(offset_ms * 1_000_000))

                key_map = {
                    "instructions": "instructions",
                    "cycles": "cycles",
                    "cache-misses": "cache-misses",
                    "branch-misses": "branch-misses",
                    "task-clock": "task-clock",
                    "context-switches": "context-switches",
                    "branches": "branches",
                }
                column = key_map.get(event)
                if row is not None and column:
                    row[column] = value

            if row:
                self.perf_writer.writerow(row)
                self.perf_csv_handle.flush()
                if self.telemetry:
                    sample = {k: row.get(k, "") for k in self.PERF_FIELDS}
                    sample["suite"] = self.current_suite
                    self.telemetry.publish("perf_sample", sample)
        finally:
            try:
                stream.close()
            except Exception:
                pass

    def _psutil_loop(self) -> None:
        while not self.psutil_stop.is_set():
            try:
                assert self.psutil_writer is not None
                ts_now = time.time_ns()
                cpu_percent = self.psutil_proc.cpu_percent(interval=None)  # type: ignore[arg-type]
                rss_bytes = self.psutil_proc.memory_info().rss  # type: ignore[union-attr]
                num_threads = self.psutil_proc.num_threads()  # type: ignore[union-attr]
                self.psutil_writer.writerow({
                    "ts_unix_ns": ts_now,
                    "cpu_percent": cpu_percent,
                    "rss_bytes": rss_bytes,
                    "num_threads": num_threads,
                })
                self.psutil_csv_handle.flush()
                if self.telemetry:
                    self.telemetry.publish(
                        "psutil_sample",
                        {
                            "timestamp_ns": ts_now,
                            "suite": self.current_suite,
                            "cpu_percent": cpu_percent,
                            "rss_bytes": rss_bytes,
                            "num_threads": num_threads,
                        },
                    )
            except Exception:
                pass
            time.sleep(1.0)
            try:
                self.psutil_proc.cpu_percent(interval=None)  # type: ignore[arg-type]
            except Exception:
                pass

    def _telemetry_loop(self) -> None:
        while not self.temp_stop.is_set():
            payload = {
                "ts_unix_ns": time.time_ns(),
                "temp_c": None,
                "freq_hz": None,
                "throttled_hex": "",
            }
            if self._vcgencmd_available:
                try:
                    out = subprocess.check_output(["vcgencmd", "measure_temp"]).decode(errors="ignore")
                    payload["temp_c"] = float(out.split("=")[1].split("'")[0])
                except Exception:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()

            freq_path = Path("/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq")
            if freq_path.exists():
                try:
                    payload["freq_hz"] = int(freq_path.read_text().strip()) * 1000
                except Exception:
                    pass
            elif self._vcgencmd_available:
                try:
                    out = subprocess.check_output(["vcgencmd", "measure_clock", "arm"]).decode(errors="ignore")
                    payload["freq_hz"] = int(out.split("=")[1].strip())
                except Exception:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()

            if self._vcgencmd_available:
                try:
                    out = subprocess.check_output(["vcgencmd", "get_throttled"]).decode(errors="ignore")
                    payload["throttled_hex"] = out.strip().split("=")[1]
                except Exception:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()
            try:
                assert self.temp_writer is not None
                self.temp_writer.writerow(payload)
                self.temp_csv_handle.flush()
                if self.telemetry:
                    payload = dict(payload)
                    payload["suite"] = self.current_suite
                    self.telemetry.publish("thermal_sample", payload)
            except Exception:
                pass
            time.sleep(1.0)

    def rotate(self, pid: int, outdir: Path, suite: str) -> None:
        if not self.enabled:
            write_marker(suite)
            return
        self.stop()
        self.start(pid, outdir, suite)
        write_marker(suite)

    def stop(self) -> None:
        if not self.enabled:
            return

        self.perf_stop.set()
        if self.perf_thread:
            self.perf_thread.join(timeout=1.0)
        if self.perf:
            killtree(self.perf)
            self.perf = None
        if self.perf_csv_handle:
            try:
                self.perf_csv_handle.close()
            except Exception:
                pass
            self.perf_csv_handle = None

        killtree(self.pidstat)
        self.pidstat = None
        if self.pidstat_out:
            try:
                self.pidstat_out.close()
            except Exception:
                pass
            self.pidstat_out = None

        self.psutil_stop.set()
        if self.psutil_thread:
            self.psutil_thread.join(timeout=1.0)
            self.psutil_thread = None
        if self.psutil_csv_handle:
            try:
                self.psutil_csv_handle.close()
            except Exception:
                pass
            self.psutil_csv_handle = None

        self.temp_stop.set()
        if self.temp_thread:
            self.temp_thread.join(timeout=1.0)
            self.temp_thread = None
        if self.temp_csv_handle:
            try:
                self.temp_csv_handle.close()
            except Exception:
                pass
            self.temp_csv_handle = None

        if self.telemetry:
            self.telemetry.publish(
                "monitors_stopped",
                {
                    "timestamp_ns": time.time_ns(),
                    "suite": self.current_suite,
                },
            )


class ControlServer(threading.Thread):
    """Line-delimited JSON control server for the scheduler."""

    def __init__(self, host: str, port: int, state: dict):
        super().__init__(daemon=True)
        self.host = host
        self.port = port
        self.state = state
        try:
            addrinfo = socket.getaddrinfo(
                self.host,
                self.port,
                0,
                socket.SOCK_STREAM,
                proto=0,
                flags=socket.AI_PASSIVE if not self.host else 0,
            )
        except socket.gaierror as exc:
            raise OSError(f"control server bind failed for {self.host}:{self.port}: {exc}") from exc

        last_exc: Optional[Exception] = None
        bound_sock: Optional[socket.socket] = None
        for family, socktype, proto, _canon, sockaddr in addrinfo:
            try:
                candidate = socket.socket(family, socktype, proto)
                try:
                    candidate.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    if family == socket.AF_INET6:
                        try:
                            candidate.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                        except OSError:
                            pass
                    candidate.bind(sockaddr)
                    candidate.listen(5)
                except Exception:
                    candidate.close()
                    raise
            except Exception as exc:
                last_exc = exc
                continue
            bound_sock = candidate
            break

        if bound_sock is None:
            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"control server bind failed for {self.host}:{self.port}: {message}")

        self.sock = bound_sock

    def run(self) -> None:
        print(f"[follower] control listening on {self.host}:{self.port}", flush=True)
        while not self.state["stop_event"].is_set():
            try:
                self.sock.settimeout(0.5)
                conn, _addr = self.sock.accept()
            except socket.timeout:
                continue
            threading.Thread(target=self.handle, args=(conn,), daemon=True).start()
        self.sock.close()

    def handle(self, conn: socket.socket) -> None:
        try:
            line = conn.makefile().readline()
            request = json.loads(line.strip()) if line else {}
        except Exception:
            request = {}

        try:
            cmd = request.get("cmd")
            if cmd == "ping":
                self._send(conn, {"ok": True, "ts": ts()})
                return
            if cmd == "timesync":
                t1 = int(request.get("t1_ns", 0))
                t2 = time.time_ns()
                response = {"ok": True, "t1_ns": t1, "t2_ns": t2}
                t3 = time.time_ns()
                response["t3_ns"] = t3
                self._send(conn, response)
                return
            state_lock = self.state.get("lock")
            if state_lock is None:
                state_lock = threading.Lock()
                self.state["lock"] = state_lock
            if cmd == "status":
                with state_lock:
                    proxy = self.state["proxy"]
                    suite = self.state["suite"]
                    monitors_enabled = self.state["monitors"].enabled
                    running = bool(proxy and proxy.poll() is None)
                    proxy_pid = proxy.pid if proxy else None
                    telemetry: Optional[TelemetryPublisher] = self.state.get("telemetry")
                    pending_suite = self.state.get("pending_suite")
                    last_requested = self.state.get("last_requested_suite")
                    session_id = self.state.get("session_id")
                    status_payload = {
                        "suite": suite,
                        "pending_suite": pending_suite,
                        "last_requested_suite": last_requested,
                        "proxy_pid": proxy_pid,
                        "running": running,
                        "control_host": self.host,
                        "control_port": self.port,
                        "udp_recv_port": APP_RECV_PORT,
                        "udp_send_port": APP_SEND_PORT,
                        "session_id": session_id,
                        "monitors_enabled": monitors_enabled,
                    }
                self._send(conn, {"ok": True, **status_payload})
                if telemetry:
                    telemetry.publish(
                        "status_reply",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": status_payload["suite"],
                            "running": status_payload["running"],
                            "pending_suite": status_payload["pending_suite"],
                            "last_requested_suite": status_payload["last_requested_suite"],
                        },
                    )
                return
            if cmd == "session_info":
                with state_lock:
                    session_id = self.state.get("session_id")
                session_value = str(session_id) if session_id is not None else ""
                self._send(
                    conn,
                    {
                        "ok": True,
                        "session_id": session_value,
                    },
                )
                return
            if cmd == "mark":
                suite = request.get("suite")
                kind = str(request.get("kind") or "rekey")
                telemetry: Optional[TelemetryPublisher] = None
                monitor: Optional[HighSpeedMonitor] = None
                monitors = None
                monitor_prev_suite: Optional[str] = None
                proxy = None
                rotate_args: Optional[Tuple[int, Path, str]] = None
                with state_lock:
                    if not suite:
                        self._send(conn, {"ok": False, "error": "missing suite"})
                        return
                    proxy = self.state["proxy"]
                    proxy_running = bool(proxy and proxy.poll() is None)
                    if not proxy_running:
                        self._send(conn, {"ok": False, "error": "proxy not running"})
                        return
                    old_suite = self.state.get("suite")
                    self.state["prev_suite"] = old_suite
                    self.state["pending_suite"] = suite
                    self.state["last_requested_suite"] = suite
                    suite_outdir = self.state["suite_outdir"]
                    outdir = suite_outdir(suite)
                    monitors = self.state["monitors"]
                    monitor = self.state.get("high_speed_monitor")
                    telemetry = self.state.get("telemetry")
                    monitor_prev_suite = old_suite
                    if proxy:
                        rotate_args = (proxy.pid, outdir, suite)
                if monitor and monitor_prev_suite != suite:
                    monitor.start_rekey(monitor_prev_suite or "unknown", suite)
                if monitors and rotate_args:
                    pid, outdir, new_suite = rotate_args
                    monitors.rotate(pid, outdir, new_suite)
                self._send(conn, {"ok": True, "marked": suite})
                if telemetry:
                    telemetry.publish(
                        "mark",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "prev_suite": monitor_prev_suite,
                            "requested_suite": suite,
                            "kind": kind,
                        },
                    )
                self._append_mark_entry([
                    "mark",
                    str(time.time_ns()),
                    kind,
                    suite or "",
                    monitor_prev_suite or "",
                ])
                return
            if cmd == "rekey_complete":
                status_value = str(request.get("status", "ok"))
                success = status_value.lower() == "ok"
                requested_suite = str(request.get("suite") or "")
                monitor: Optional[HighSpeedMonitor] = None
                telemetry: Optional[TelemetryPublisher] = None
                monitors = None
                proxy = None
                rotate_args: Optional[Tuple[int, Path, str]] = None
                monitor_update_suite: Optional[str] = None
                with state_lock:
                    monitor = self.state.get("high_speed_monitor")
                    telemetry = self.state.get("telemetry")
                    monitors = self.state["monitors"]
                    proxy = self.state.get("proxy")
                    suite_outdir = self.state["suite_outdir"]
                    if requested_suite:
                        self.state["last_requested_suite"] = requested_suite
                    previous_suite = self.state.get("prev_suite")
                    pending_suite = self.state.get("pending_suite")
                    if success:
                        if requested_suite and pending_suite and requested_suite != pending_suite:
                            print(
                                f"[follower] pending suite {pending_suite} does not match requested {requested_suite}; updating to requested",
                                flush=True,
                            )
                            pending_suite = requested_suite
                        if pending_suite:
                            self.state["suite"] = pending_suite
                            monitor_update_suite = pending_suite
                        elif requested_suite:
                            self.state["suite"] = requested_suite
                            monitor_update_suite = requested_suite
                    else:
                        if previous_suite is not None:
                            self.state["suite"] = previous_suite
                            monitor_update_suite = previous_suite
                            if proxy and proxy.poll() is None:
                                outdir = suite_outdir(previous_suite)
                                rotate_args = (proxy.pid, outdir, previous_suite)
                        elif pending_suite:
                            monitor_update_suite = pending_suite
                    self.state.pop("pending_suite", None)
                    self.state.pop("prev_suite", None)
                    current_suite = self.state.get("suite")
                    if success and requested_suite and current_suite != requested_suite:
                        print(
                            f"[follower] active suite {current_suite} disagrees with requested {requested_suite}; forcing to requested",
                            flush=True,
                        )
                        self.state["suite"] = requested_suite
                        current_suite = requested_suite
                        monitor_update_suite = requested_suite
                if rotate_args and monitors and proxy and proxy.poll() is None:
                    pid, outdir, suite_name = rotate_args
                    monitors.rotate(pid, outdir, suite_name)
                if monitor and monitor_update_suite:
                    monitor.current_suite = monitor_update_suite
                    monitor.end_rekey(success=success, new_suite=monitor_update_suite)
                elif monitor:
                    monitor.end_rekey(success=success, new_suite=current_suite)
                self._send(conn, {"ok": True})
                if telemetry:
                    telemetry.publish(
                        "rekey_complete",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": current_suite,
                            "requested_suite": requested_suite or current_suite,
                            "status": status_value,
                        },
                    )
                return
            if cmd == "schedule_mark":
                suite = request.get("suite")
                kind = str(request.get("kind") or "window")
                t0_ns = int(request.get("t0_ns", 0))
                if not suite or not t0_ns:
                    self._send(conn, {"ok": False, "error": "missing suite or t0_ns"})
                    return

                def _do_mark() -> None:
                    delay = max(0.0, (t0_ns - time.time_ns()) / 1e9)
                    if delay:
                        time.sleep(delay)
                    proxy = None
                    monitors = None
                    monitor: Optional[HighSpeedMonitor] = None
                    suite_outdir_fn = None
                    with state_lock:
                        proxy = self.state.get("proxy")
                        monitors = self.state.get("monitors")
                        suite_outdir_fn = self.state.get("suite_outdir")
                        monitor = self.state.get("high_speed_monitor")
                    proxy_running = bool(proxy and proxy.poll() is None)
                    if monitor and suite and monitor.current_suite != suite:
                        monitor.current_suite = suite
                    if proxy_running and monitors and suite_outdir_fn and proxy:
                        outdir = suite_outdir_fn(suite)
                        monitors.rotate(proxy.pid, outdir, suite)
                    else:
                        write_marker(suite)
                    self._append_mark_entry([
                        "mark",
                        str(time.time_ns()),
                        kind,
                        suite or "",
                        "",
                    ])

                threading.Thread(target=_do_mark, daemon=True).start()
                self._send(conn, {"ok": True, "scheduled": suite, "t0_ns": t0_ns})
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "schedule_mark",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "t0_ns": t0_ns,
                            "kind": kind,
                            "requested_suite": suite,
                        },
                    )
                return
            if cmd == "power_capture":
                manager = self.state.get("power_manager")
                if not isinstance(manager, PowerCaptureManager):
                    self._send(conn, {"ok": False, "error": "power_monitor_unavailable"})
                    return
                duration_s = request.get("duration_s")
                suite = request.get("suite") or self.state.get("suite") or "unknown"
                try:
                    duration_val = float(duration_s)
                except (TypeError, ValueError):
                    self._send(conn, {"ok": False, "error": "invalid_duration"})
                    return
                start_ns = request.get("start_ns")
                try:
                    start_ns_val = int(start_ns) if start_ns is not None else None
                except (TypeError, ValueError):
                    start_ns_val = None
                ok, error = manager.start_capture(suite, duration_val, start_ns_val)
                if ok:
                    self._send(
                        conn,
                        {
                            "ok": True,
                            "scheduled": True,
                            "suite": suite,
                            "duration_s": duration_val,
                            "start_ns": start_ns_val,
                        },
                    )
                    telemetry = self.state.get("telemetry")
                    if telemetry:
                        telemetry.publish(
                            "power_capture_request",
                            {
                                "timestamp_ns": time.time_ns(),
                                "suite": suite,
                                "duration_s": duration_val,
                                "start_ns": start_ns_val,
                            },
                        )
                else:
                    self._send(conn, {"ok": False, "error": error or "power_capture_failed"})
                return
            if cmd == "power_status":
                manager = self.state.get("power_manager")
                if not isinstance(manager, PowerCaptureManager):
                    self._send(conn, {"ok": False, "error": "power_monitor_unavailable"})
                    return
                status = manager.status()
                self._send(conn, {"ok": True, **status})
                return
            if cmd == "stop":
                self.state["monitors"].stop()
                self.state["stop_event"].set()
                self._send(conn, {"ok": True, "stopping": True})
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "stop",
                        {"timestamp_ns": time.time_ns()},
                    )
                return
            self._send(conn, {"ok": False, "error": "unknown_cmd"})
        finally:
            try:
                conn.close()
            except Exception:
                pass

    @staticmethod
    def _send(conn: socket.socket, obj: dict) -> None:
        conn.sendall((json.dumps(obj) + "\n").encode())

    def _append_mark_entry(self, row: list[str]) -> None:
        monitor = self.state.get("high_speed_monitor")
        if monitor and hasattr(monitor, "_append_rekey_mark"):
            try:
                monitor._append_rekey_mark(row)
                return
            except Exception:
                pass
        session_dir = self.state.get("session_dir")
        session_id = self.state.get("session_id")
        if not session_dir or not session_id:
            return
        path = Path(session_dir) / f"rekey_marks_{session_id}.csv"
        lock = self.state.setdefault("_marks_lock", threading.Lock())
        try:
            lock_acquired = lock.acquire(timeout=1.5)
        except TypeError:
            lock.acquire()
            lock_acquired = True
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            new_file = not path.exists()
            with path.open("a", newline="", encoding="utf-8") as handle:
                writer = csv.writer(handle)
                if new_file:
                    writer.writerow(["kind", "timestamp_ns", "field1", "field2", "field3"])
                writer.writerow(row)
        except Exception as exc:
            print(f"[{ts()}] follower mark append failed: {exc}", flush=True)
        finally:
            if lock_acquired:
                lock.release()


def main(argv: Optional[list[str]] = None) -> None:
    args = _parse_args(argv)
    device_generation = "pi5" if args.pi5 else "pi4"
    os.environ.setdefault("DRONE_DEVICE_GENERATION", device_generation)

    log_runtime_environment("follower")
    if hasattr(os, "geteuid"):
        try:
            if os.geteuid() == 0:
                print(
                    f"[{ts()}] follower running as root; ensure venv packages are available",
                    flush=True,
                )
        except Exception:
            pass

    OUTDIR.mkdir(parents=True, exist_ok=True)
    MARK_DIR.mkdir(parents=True, exist_ok=True)

    default_suite = discover_initial_suite()
    auto = AUTO_DRONE_CONFIG

    session_prefix = str(auto.get("session_prefix") or "session")
    session_id = os.environ.get("DRONE_SESSION_ID") or f"{session_prefix}_{int(time.time())}"
    stop_event = threading.Event()

    monitor_base_cfg = auto.get("monitor_output_base")
    if monitor_base_cfg:
        monitor_base = Path(monitor_base_cfg).expanduser()
    else:
        monitor_base = DEFAULT_MONITOR_BASE.expanduser()
    monitor_base = monitor_base.resolve()
    session_dir = monitor_base / session_id
    session_dir.mkdir(parents=True, exist_ok=True)
    print(f"[follower] session_id={session_id}")
    print(f"[follower] monitor output -> {session_dir}")
    print(f"[follower] device generation={device_generation}")

    for env_key, env_value in auto.get("power_env", {}).items():
        if env_value is None:
            continue
        os.environ.setdefault(env_key, str(env_value))

    telemetry: Optional[TelemetryPublisher] = None
    telemetry_enabled = bool(auto.get("telemetry_enabled", True))
    telemetry_host_cfg = auto.get("telemetry_host")
    if telemetry_host_cfg:
        telemetry_host = telemetry_host_cfg
    else:
        telemetry_host = TELEMETRY_DEFAULT_HOST
    telemetry_port_cfg = auto.get("telemetry_port")
    telemetry_port = TELEMETRY_DEFAULT_PORT if telemetry_port_cfg in (None, "") else int(telemetry_port_cfg)

    expected_bind = str(CONFIG.get("GCS_TELEMETRY_BIND") or "").strip()
    if expected_bind and expected_bind not in {"0.0.0.0", "::", ""} and telemetry_host != expected_bind:
        raise RuntimeError(
            f"Telemetry target {telemetry_host}:{telemetry_port} differs from GCS bind {expected_bind}; update AUTO_DRONE.telemetry_host"
        )
    print(f"[follower] telemetry target {telemetry_host}:{telemetry_port}")

    if telemetry_enabled:
        telemetry = TelemetryPublisher(telemetry_host, telemetry_port, session_id)
        telemetry.start()
        print(f"[follower] telemetry publisher started (session={session_id})")
    else:
        print("[follower] telemetry disabled via AUTO_DRONE configuration")

    if bool(auto.get("cpu_optimize", True)):
        target_khz = PI5_TARGET_KHZ if args.pi5 else PI4_TARGET_KHZ
        optimize_cpu_performance(target_khz=target_khz)
        print(
            f"[follower] cpu governor target ~{target_khz / 1000:.0f} MHz ({device_generation})",
            flush=True,
        )

    power_dir = session_dir / "power"
    power_manager = PowerCaptureManager(power_dir, session_id, telemetry)

    high_speed_monitor = HighSpeedMonitor(session_dir, session_id, telemetry)
    high_speed_monitor.start()

    initial_suite = auto.get("initial_suite") or default_suite
    proxy, proxy_log = start_drone_proxy(initial_suite)
    monitors_enabled = bool(auto.get("monitors_enabled", True))
    if not monitors_enabled:
        print("[follower] monitors disabled via AUTO_DRONE configuration")
    monitors = Monitors(enabled=monitors_enabled, telemetry=telemetry)
    time.sleep(1)
    if proxy.poll() is None:
        monitors.start(proxy.pid, suite_outdir(initial_suite), initial_suite)
        high_speed_monitor.attach_proxy(proxy.pid)
        high_speed_monitor.current_suite = initial_suite

    echo = UdpEcho(
        APP_BIND_HOST,
        APP_RECV_PORT,
        APP_SEND_HOST,
        APP_SEND_PORT,
        stop_event,
        high_speed_monitor,
        session_dir,
        telemetry,
    )
    echo.start()

    state = {
        "proxy": proxy,
        "suite": initial_suite,
        "suite_outdir": suite_outdir,
        "monitors": monitors,
        "stop_event": stop_event,
        "high_speed_monitor": high_speed_monitor,
        "telemetry": telemetry,
        "prev_suite": None,
        "pending_suite": None,
        "last_requested_suite": initial_suite,
        "power_manager": power_manager,
        "device_generation": device_generation,
        "lock": threading.Lock(),
        "session_id": session_id,
        "session_dir": session_dir,
    }
    control = ControlServer(CONTROL_HOST, CONTROL_PORT, state)
    control.start()

    try:
        while not stop_event.is_set():
            if proxy.poll() is not None:
                print(f"[follower] proxy exited with {proxy.returncode}", flush=True)
                stop_event.set()
                break
            time.sleep(0.5)
    except KeyboardInterrupt:
        stop_event.set()
    finally:
        monitors.stop()
        high_speed_monitor.stop()
        if proxy:
            try:
                proxy.send_signal(signal.SIGTERM)
            except Exception:
                pass
            killtree(proxy)
        if proxy_log:
            try:
                proxy_log.close()
            except Exception:
                pass
        if telemetry:
            telemetry.stop()


if __name__ == "__main__":
    # Test plan:
    # 1. Start the follower before the scheduler and confirm telemetry connects after retries.
    # 2. Run the Windows scheduler to drive a full suite cycle without rekey failures.
    # 3. Remove the logs/auto/drone/<suite> directory and confirm it is recreated automatically.
    # 4. Stop the telemetry collector mid-run and verify the follower reconnects without crashing.
    main()
======================================
#!/usr/bin/env python3
"""GCS scheduler that drives rekeys and UDP traffic using central configuration."""

from __future__ import annotations

import bisect
import csv
import errno
import io
import json
import math
import os
import socket
import struct
import subprocess
import sys
import threading
import time
from collections import deque, OrderedDict
from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, IO, Iterable, List, Optional, Set, Tuple

try:
    from openpyxl import Workbook
    from openpyxl.chart import BarChart, LineChart, Reference
except ImportError:  # pragma: no cover
    Workbook = None
    BarChart = None
    LineChart = None
    Reference = None

def _ensure_core_importable() -> Path:
    root = Path(__file__).resolve().parents[2]
    root_str = str(root)
    if root_str not in sys.path:
        sys.path.insert(0, root_str)
    try:
        __import__("core")
    except ModuleNotFoundError as exc:
        raise RuntimeError(
            f"Unable to import 'core'; repo root {root} missing from sys.path."
        ) from exc
    return root


ROOT = _ensure_core_importable()

from core import suites as suites_mod
from core.config import CONFIG
from tools.blackout_metrics import compute_blackout
from tools.merge_power import extract_power_fields
from tools.power_utils import calculate_transient_energy


DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_SEND_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
APP_RECV_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))

OUTDIR = ROOT / "logs/auto/gcs"
SUITES_OUTDIR = OUTDIR / "suites"
SECRETS_DIR = ROOT / "secrets/matrix"

EXCEL_OUTPUT_DIR = ROOT / Path(
    CONFIG.get("GCS_EXCEL_OUTPUT")
    or os.getenv("GCS_EXCEL_OUTPUT", "output/gcs")
)

COMBINED_OUTPUT_DIR = ROOT / Path(
    CONFIG.get("GCS_COMBINED_OUTPUT_BASE")
    or os.getenv("GCS_COMBINED_OUTPUT_BASE", "output/gcs")
)

DRONE_MONITOR_BASE = ROOT / Path(
    CONFIG.get("DRONE_MONITOR_OUTPUT_BASE")
    or os.getenv("DRONE_MONITOR_OUTPUT_BASE", "output/drone")
)

TELEMETRY_BIND_HOST = CONFIG.get("GCS_TELEMETRY_BIND", "0.0.0.0")
TELEMETRY_PORT = int(
    CONFIG.get("GCS_TELEMETRY_PORT")
    or CONFIG.get("DRONE_TELEMETRY_PORT")
    or 52080
)

PROXY_STATUS_PATH = OUTDIR / "gcs_status.json"
PROXY_SUMMARY_PATH = OUTDIR / "gcs_summary.json"
SUMMARY_CSV = OUTDIR / "summary.csv"
EVENTS_FILENAME = "blaster_events.jsonl"
BLACKOUT_CSV = OUTDIR / "gcs_blackouts.csv"
STEP_RESULTS_PATH = OUTDIR / "step_results.jsonl"

SEQ_TS_OVERHEAD_BYTES = 12
UDP_HEADER_BYTES = 8
IPV4_HEADER_BYTES = 20
IPV6_HEADER_BYTES = 40
MIN_DELAY_SAMPLES = 30
HYSTERESIS_WINDOW = 3
MAX_BISECT_STEPS = 3
WARMUP_FRACTION = 0.1
MAX_WARMUP_SECONDS = 1.0
SATURATION_COARSE_RATES = [5, 25, 50, 75, 100, 125, 150, 175, 200]
SATURATION_LINEAR_RATES = [
    5,
    10,
    15,
    20,
    25,
    30,
    35,
    40,
    45,
    50,
    60,
    70,
    80,
    90,
    100,
    125,
    150,
    175,
    200,
]
SATURATION_SIGNALS = ("owd_p95_spike", "delivery_degraded", "loss_excess")
TELEMETRY_BUFFER_MAXLEN_DEFAULT = 100_000
REKEY_SETTLE_SECONDS = 1.5
CLOCK_OFFSET_THRESHOLD_NS = 50_000_000
CONSTANT_RATE_MBPS_DEFAULT = 8.0


def _compute_sampling_params(duration_s: float, event_sample: int, min_delay_samples: int) -> Tuple[int, int]:
    if event_sample <= 0:
        return 0, 0
    effective_sample = event_sample
    effective_min = max(0, min_delay_samples)
    if duration_s < 20.0:
        effective_sample = max(1, min(event_sample, 20))
        scale = max(duration_s, 5.0) / 20.0
        effective_min = max(10, int(math.ceil(effective_min * scale))) if effective_min else 0
    return effective_sample, effective_min


def _close_socket(sock: Optional[socket.socket]) -> None:
    if sock is None:
        return
    try:
        sock.close()
    except Exception:
        pass


def _close_file(handle: Optional[IO[str]]) -> None:
    if handle is None:
        return
    try:
        handle.flush()
    except Exception:
        pass
    try:
        handle.close()
    except Exception:
        pass


class P2Quantile:
    def __init__(self, p: float) -> None:
        if not 0.0 < p < 1.0:
            raise ValueError("p must be between 0 and 1")
        self.p = p
        self._initial: List[float] = []
        self._q: List[float] = []
        self._n: List[int] = []
        self._np: List[float] = []
        self._dn = [0.0, p / 2.0, p, (1.0 + p) / 2.0, 1.0]
        self.count = 0

    def add(self, sample: float) -> None:
        x = float(sample)
        self.count += 1
        if self.count <= 5:
            bisect.insort(self._initial, x)
            if self.count == 5:
                self._q = list(self._initial)
                self._n = [1, 2, 3, 4, 5]
                self._np = [1.0, 1.0 + 2.0 * self.p, 1.0 + 4.0 * self.p, 3.0 + 2.0 * self.p, 5.0]
            return

        if not self._q:
            # Should not happen, but guard for consistency
            self._q = list(self._initial)
            self._n = [1, 2, 3, 4, 5]
            self._np = [1.0, 1.0 + 2.0 * self.p, 1.0 + 4.0 * self.p, 3.0 + 2.0 * self.p, 5.0]

        if x < self._q[0]:
            self._q[0] = x
            k = 0
        elif x >= self._q[4]:
            self._q[4] = x
            k = 3
        else:
            k = 0
            for idx in range(4):
                if self._q[idx] <= x < self._q[idx + 1]:
                    k = idx
                    break

        for idx in range(k + 1, 5):
            self._n[idx] += 1

        for idx in range(5):
            self._np[idx] += self._dn[idx]

        for idx in range(1, 4):
            d = self._np[idx] - self._n[idx]
            if (d >= 1 and self._n[idx + 1] - self._n[idx] > 1) or (d <= -1 and self._n[idx - 1] - self._n[idx] < -1):
                step = 1 if d > 0 else -1
                candidate = self._parabolic(idx, step)
                if self._q[idx - 1] < candidate < self._q[idx + 1]:
                    self._q[idx] = candidate
                else:
                    self._q[idx] = self._linear(idx, step)
                self._n[idx] += step

    def value(self) -> float:
        if self.count == 0:
            return 0.0
        if self.count <= 5 and self._initial:
            rank = (self.count - 1) * self.p
            idx = max(0, min(len(self._initial) - 1, int(round(rank))))
            return float(self._initial[idx])
        if not self._q:
            return 0.0
        return float(self._q[2])

    def _parabolic(self, idx: int, step: int) -> float:
        numerator_left = self._n[idx] - self._n[idx - 1] + step
        numerator_right = self._n[idx + 1] - self._n[idx] - step
        denominator = self._n[idx + 1] - self._n[idx - 1]
        if denominator == 0:
            return self._q[idx]
        return self._q[idx] + (step / denominator) * (
            numerator_left * (self._q[idx + 1] - self._q[idx]) / max(self._n[idx + 1] - self._n[idx], 1)
            + numerator_right * (self._q[idx] - self._q[idx - 1]) / max(self._n[idx] - self._n[idx - 1], 1)
        )

    def _linear(self, idx: int, step: int) -> float:
        target = idx + step
        denominator = self._n[target] - self._n[idx]
        if denominator == 0:
            return self._q[idx]
        return self._q[idx] + step * (self._q[target] - self._q[idx]) / denominator


def wilson_interval(successes: int, n: int, z: float = 1.96) -> Tuple[float, float]:
    if n <= 0:
        return (0.0, 1.0)
    proportion = successes / n
    z2 = z * z
    denom = 1.0 + z2 / n
    center = (proportion + z2 / (2.0 * n)) / denom
    margin = (z * math.sqrt((proportion * (1.0 - proportion) / n) + (z2 / (4.0 * n * n)))) / denom
    return (max(0.0, center - margin), min(1.0, center + margin))


def ip_header_bytes_for_host(host: str) -> int:
    return IPV6_HEADER_BYTES if ":" in host else IPV4_HEADER_BYTES


APP_IP_HEADER_BYTES = ip_header_bytes_for_host(APP_SEND_HOST)


def ts() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def log_runtime_environment(component: str) -> None:
    preview = ";".join(sys.path[:5])
    print(f"[{ts()}] {component} python_exe={sys.executable}")
    print(f"[{ts()}] {component} cwd={Path.cwd()}")
    print(f"[{ts()}] {component} sys.path_prefix={preview}")


def _merge_defaults(defaults: dict, override: Optional[dict]) -> dict:
    result = deepcopy(defaults)
    if isinstance(override, dict):
        for key, value in override.items():
            if isinstance(value, dict) and isinstance(result.get(key), dict):
                merged = result[key].copy()
                merged.update(value)
                result[key] = merged
            else:
                result[key] = value
    return result


AUTO_GCS_DEFAULTS = {
    "session_prefix": "session",
    "traffic": "constant",
    "duration_s": 45.0,
    "pre_gap_s": 1.0,
    "inter_gap_s": 15.0,
    "payload_bytes": 256,
    "event_sample": 100,
    "passes": 1,
    "rate_pps": 0,
    "bandwidth_mbps": 0.0,
    "max_rate_mbps": 200.0,
    "sat_search": "auto",
    "sat_delivery_threshold": 0.85,
    "sat_loss_threshold_pct": 5.0,
    "sat_rtt_spike_factor": 1.6,
    "suites": None,
    "launch_proxy": True,
    "monitors_enabled": True,
    "telemetry_enabled": True,
    "telemetry_bind_host": TELEMETRY_BIND_HOST,
    "telemetry_port": TELEMETRY_PORT,
    "export_combined_excel": True,
    "power_capture": True,
}

AUTO_GCS_CONFIG = _merge_defaults(AUTO_GCS_DEFAULTS, CONFIG.get("AUTO_GCS"))

SATURATION_SEARCH_MODE = str(AUTO_GCS_CONFIG.get("sat_search") or "auto").lower()
SATURATION_RTT_SPIKE = float(AUTO_GCS_CONFIG.get("sat_rtt_spike_factor") or 1.6)
SATURATION_DELIVERY_THRESHOLD = float(AUTO_GCS_CONFIG.get("sat_delivery_threshold") or 0.85)
SATURATION_LOSS_THRESHOLD = float(AUTO_GCS_CONFIG.get("sat_loss_threshold_pct") or 5.0)


def mkdirp(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def _atomic_write_bytes(path: Path, data: bytes, *, tmp_suffix: str = ".tmp", retries: int = 6, backoff: float = 0.05) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_name(path.name + tmp_suffix)
    fd = os.open(str(tmp_path), os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644)
    try:
        with os.fdopen(fd, "wb", closefd=True) as handle:
            handle.write(data)
            try:
                handle.flush()
                os.fsync(handle.fileno())
            except Exception:
                pass
    except Exception:
        try:
            os.remove(tmp_path)
        except Exception:
            pass
        raise

    delay = backoff
    last_exc: Optional[Exception] = None
    for attempt in range(retries):
        try:
            os.replace(tmp_path, path)
            return
        except PermissionError as exc:  # pragma: no cover - platform specific
            last_exc = exc
            if attempt == retries - 1:
                try:
                    os.remove(path)
                except FileNotFoundError:
                    pass
                except Exception:
                    pass
                try:
                    os.replace(tmp_path, path)
                    return
                except Exception as final_exc:
                    last_exc = final_exc
                    break
        except OSError as exc:  # pragma: no cover - platform specific
            if exc.errno not in (errno.EACCES, errno.EPERM):
                raise
            last_exc = exc
            if attempt == retries - 1:
                try:
                    os.remove(path)
                except FileNotFoundError:
                    pass
                except Exception:
                    pass
                try:
                    os.replace(tmp_path, path)
                    return
                except Exception as final_exc:
                    last_exc = final_exc
                    break
        time.sleep(delay)
        delay = min(delay * 2, 0.5)

    try:
        os.remove(tmp_path)
    except Exception:
        pass
    if last_exc is not None:
        raise last_exc


def _robust_copy(src: Path, dst: Path, attempts: int = 3, delay: float = 0.05) -> bool:
    for attempt in range(1, attempts + 1):
        try:
            data = src.read_bytes()
        except FileNotFoundError:
            return False
        except OSError as exc:
            print(f"[WARN] failed to read {src}: {exc}", file=sys.stderr)
            if attempt == attempts:
                return False
            time.sleep(delay)
            continue
        try:
            _atomic_write_bytes(dst, data)
            return True
        except Exception as exc:  # pragma: no cover - platform specific
            print(f"[WARN] failed to update {dst}: {exc}", file=sys.stderr)
            if attempt == attempts:
                return False
            time.sleep(delay)
    return False


def suite_outdir(suite: str) -> Path:
    target = SUITES_OUTDIR / suite
    mkdirp(target)
    return target


def _as_float(value: object) -> Optional[float]:
    if value in (None, ""):
        return None
    try:
        result = float(value)
    except (TypeError, ValueError):
        return None
    if math.isnan(result):
        return None
    return result


def _rounded(value: object, digits: int) -> object:
    num = _as_float(value)
    if num is None:
        return ""
    return round(num, digits)


def resolve_suites(requested: Optional[Iterable[str]]) -> List[str]:
    suite_listing = suites_mod.list_suites()
    if isinstance(suite_listing, dict):
        available = list(suite_listing.keys())
    else:
        available = list(suite_listing)
    if not available:
        raise RuntimeError("No suites registered in core.suites; cannot proceed")

    if not requested:
        return available

    resolved: List[str] = []
    seen: Set[str] = set()
    for name in requested:
        info = suites_mod.get_suite(name)
        suite_id = info["suite_id"]
        if suite_id not in available:
            raise RuntimeError(f"Suite {name} not present in core registry")
        if suite_id not in seen:
            resolved.append(suite_id)
            seen.add(suite_id)
    return resolved


def preferred_initial_suite(candidates: List[str]) -> Optional[str]:
    configured = CONFIG.get("SIMPLE_INITIAL_SUITE")
    if not configured:
        return None
    try:
        suite_id = suites_mod.get_suite(configured)["suite_id"]
    except NotImplementedError:
        return None
    return suite_id if suite_id in candidates else None


def ctl_send(obj: dict, timeout: float = 2.0, retries: int = 4, backoff: float = 0.5) -> dict:
    last_exc: Optional[Exception] = None
    for attempt in range(1, retries + 1):
        try:
            with socket.create_connection((DRONE_HOST, CONTROL_PORT), timeout=timeout) as sock:
                sock.sendall((json.dumps(obj) + "\n").encode())
                sock.shutdown(socket.SHUT_WR)
                line = sock.makefile().readline()
                return json.loads(line.strip()) if line else {}
        except Exception as exc:
            last_exc = exc
            if attempt < retries:
                time.sleep(backoff * attempt)
                continue
            raise
    if last_exc:
        raise last_exc
    return {}


def request_power_capture(suite: str, duration_s: float, start_ns: Optional[int]) -> dict:
    payload = {
        "cmd": "power_capture",
        "suite": suite,
        "duration_s": duration_s,
    }
    if start_ns is not None:
        payload["start_ns"] = int(start_ns)
    try:
        resp = ctl_send(payload, timeout=1.5, retries=2, backoff=0.4)
    except Exception as exc:
        print(f"[WARN] power_capture request failed: {exc}", file=sys.stderr)
        return {"ok": False, "error": str(exc)}
    return resp


def poll_power_status(max_wait_s: float = 12.0, poll_s: float = 0.6) -> dict:
    deadline = time.time() + max_wait_s
    last: dict = {}
    while time.time() < deadline:
        try:
            resp = ctl_send({"cmd": "power_status"}, timeout=1.5, retries=1, backoff=0.3)
        except Exception as exc:
            last = {"ok": False, "error": str(exc)}
            time.sleep(poll_s)
            continue
        last = resp
        if not resp.get("ok"):
            break
        if not resp.get("available", True):
            break
        if not resp.get("busy", False):
            break
        time.sleep(poll_s)
    return last


class Blaster:
    """High-rate UDP blaster with RTT sampling and throughput accounting."""

    def __init__(
        self,
        send_host: str,
        send_port: int,
        recv_host: str,
        recv_port: int,
        events_path: Optional[Path],
        payload_bytes: int,
        sample_every: int,
        offset_ns: int,
    ) -> None:
        self.payload_bytes = max(12, int(payload_bytes))
        self.sample_every = max(0, int(sample_every))
        self.offset_ns = offset_ns

        send_info = socket.getaddrinfo(send_host, send_port, 0, socket.SOCK_DGRAM)
        if not send_info:
            raise OSError(f"Unable to resolve send address {send_host}:{send_port}")
        send_family, _stype, _proto, _canon, send_sockaddr = send_info[0]

        recv_info = socket.getaddrinfo(recv_host, recv_port, send_family, socket.SOCK_DGRAM)
        if not recv_info:
            recv_info = socket.getaddrinfo(recv_host, recv_port, 0, socket.SOCK_DGRAM)
        if not recv_info:
            raise OSError(f"Unable to resolve recv address {recv_host}:{recv_port}")
        recv_family, _rstype, _rproto, _rcanon, recv_sockaddr = recv_info[0]

        self.tx = socket.socket(send_family, socket.SOCK_DGRAM)
        self.rx = socket.socket(recv_family, socket.SOCK_DGRAM)
        self.send_addr = send_sockaddr
        self.recv_addr = recv_sockaddr
        self.rx.bind(self.recv_addr)
        self.rx.settimeout(0.001)
        self.rx_burst = max(1, int(os.getenv("GCS_RX_BURST", "32")))
        self._lock = threading.Lock()
        self._run_active = threading.Event()
        self._rx_thread: Optional[threading.Thread] = None
        self._stop_event: Optional[threading.Event] = None
        self._closed = False
        try:
            # Allow overriding socket buffer sizes via environment variables
            # Use GCS_SOCK_SNDBUF and GCS_SOCK_RCVBUF if present, otherwise default to 1 MiB
            sndbuf = int(os.getenv("GCS_SOCK_SNDBUF", str(1 << 20)))
            rcvbuf = int(os.getenv("GCS_SOCK_RCVBUF", str(1 << 20)))
            self.tx.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, sndbuf)
            self.rx.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, rcvbuf)
            actual_snd = self.tx.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF)
            actual_rcv = self.rx.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)
            print(
                f"[{ts()}] blaster UDP socket buffers: snd={actual_snd} rcv={actual_rcv}",
                flush=True,
            )
        except Exception:
            # best-effort; continue even if setting buffers fails
            pass

        family = self.tx.family if self.tx.family in (socket.AF_INET, socket.AF_INET6) else self.rx.family
        ip_bytes = IPV6_HEADER_BYTES if family == socket.AF_INET6 else IPV4_HEADER_BYTES
        self.wire_header_bytes = UDP_HEADER_BYTES + ip_bytes

        self.events_path = events_path
        self.events: Optional[IO[str]] = None
        if events_path is not None:
            mkdirp(events_path.parent)
            self.events = open(events_path, "w", encoding="utf-8")

        self.truncated = 0
        self.sent = 0
        self.rcvd = 0
        self.sent_bytes = 0
        self.rcvd_bytes = 0
        self.rtt_sum_ns = 0
        self.rtt_samples = 0
        self.rtt_max_ns = 0
        self.rtt_min_ns: Optional[int] = None
        self.pending: Dict[int, int] = {}
        self.rtt_p50 = P2Quantile(0.5)
        self.rtt_p95 = P2Quantile(0.95)
        self.owd_p50 = P2Quantile(0.5)
        self.owd_p95 = P2Quantile(0.95)
        self.owd_samples = 0
        self.owd_p50_ns = 0.0
        self.owd_p95_ns = 0.0
        self.rtt_p50_ns = 0.0
        self.rtt_p95_ns = 0.0

    def _log_event(self, payload: dict) -> None:
        # Buffered write; caller flushes at end of run()
        if self.events is None:
            return
        self.events.write(json.dumps(payload) + "\n")

    def _now(self) -> int:
        return time.time_ns() + self.offset_ns

    def _maybe_log(self, kind: str, seq: int, t_ns: int) -> None:
        if self.sample_every == 0:
            return
        if kind == "send":
            if seq % self.sample_every:
                return
        else:
            with self._lock:
                rcvd_count = self.rcvd
            if rcvd_count % self.sample_every:
                return
        self._log_event({"event": kind, "seq": seq, "t_ns": t_ns})

    def run(self, duration_s: float, rate_pps: int, max_packets: Optional[int] = None) -> None:
        if self._closed:
            raise RuntimeError("Blaster is closed")
        if self._run_active.is_set():
            raise RuntimeError("Blaster.run is already in progress")

        stop_at = self._now() + int(max(0.0, duration_s) * 1e9)
        payload_pad = b"\x00" * (self.payload_bytes - 12)
        interval_ns = 0.0 if rate_pps <= 0 else 1_000_000_000.0 / max(1, rate_pps)

        stop_event = threading.Event()
        self._stop_event = stop_event
        self._run_active.set()
        rx_thread = threading.Thread(target=self._rx_loop, args=(stop_event,), daemon=True)
        self._rx_thread = rx_thread
        rx_thread.start()

        with self._lock:
            self.pending.clear()

        seq = 0
        burst = 32 if interval_ns == 0.0 else 1
        next_send_ns = float(self._now())
        try:
            while self._now() < stop_at:
                if max_packets is not None:
                    with self._lock:
                        if self.sent >= max_packets:
                            break
                loop_progress = False
                sends_this_loop = burst
                while sends_this_loop > 0:
                    now_ns = self._now()
                    if now_ns >= stop_at:
                        break
                    if interval_ns > 0.0:
                        wait_ns = next_send_ns - now_ns
                        if wait_ns > 0:
                            time.sleep(min(wait_ns / 1_000_000_000.0, 0.001))
                            break
                    t_send = self._now()
                    packet = seq.to_bytes(4, "big") + int(t_send).to_bytes(8, "big") + payload_pad
                    try:
                        self.tx.sendto(packet, self.send_addr)
                    except Exception as exc:  # pragma: no cover - hard to surface in tests
                        self._log_event({"event": "send_error", "err": str(exc), "seq": seq, "ts": ts()})
                        break
                    t_send_int = int(t_send)
                    with self._lock:
                        if self.sample_every and (seq % self.sample_every == 0):
                            self.pending[seq] = t_send_int
                        self.sent += 1
                        self.sent_bytes += len(packet)
                    loop_progress = True
                    self._maybe_log("send", seq, t_send_int)
                    seq += 1
                    sends_this_loop -= 1
                    if interval_ns > 0.0:
                        next_send_ns = max(next_send_ns + interval_ns, float(t_send) + interval_ns)
                    if max_packets is not None:
                        with self._lock:
                            if self.sent >= max_packets:
                                break
                if interval_ns == 0.0 and (seq & 0x3FFF) == 0:
                    time.sleep(0)
                if not loop_progress:
                    time.sleep(0.0005)

            tail_deadline = self._now() + int(0.25 * 1e9)
            while self._now() < tail_deadline:
                time.sleep(0.0005)
        finally:
            stop_event.set()
            rx_thread.join(timeout=0.5)
            self._run_active.clear()
            self._rx_thread = None
            self._stop_event = None
            self.owd_p50_ns = self.owd_p50.value()
            self.owd_p95_ns = self.owd_p95.value()
            self.rtt_p50_ns = self.rtt_p50.value()
            self.rtt_p95_ns = self.rtt_p95.value()
            self._cleanup()
        _close_socket(self.tx)
        _close_socket(self.rx)

    def _rx_loop(self, stop_event: threading.Event) -> None:
        while not stop_event.is_set():
            if not self._run_active.is_set():
                break
            progressed = False
            for _ in range(self.rx_burst):
                if self._rx_once():
                    progressed = True
                else:
                    break
            if not progressed:
                time.sleep(0.0005)

    def _rx_once(self) -> bool:
        try:
            data, _ = self.rx.recvfrom(65535)
        except socket.timeout:
            return False
        except (socket.error, OSError) as exc:
            # Only log unexpected socket failures
            if not isinstance(exc, (ConnectionResetError, ConnectionRefusedError)):
                self._log_event({"event": "rx_error", "err": str(exc), "ts": ts()})
            return False

        t_recv = self._now()
        data_len = len(data)
        if data_len < 4:
            with self._lock:
                self.rcvd += 1
                self.rcvd_bytes += data_len
                self.truncated += 1
            return True

        seq = int.from_bytes(data[:4], "big")
        header_t_send = int.from_bytes(data[4:12], "big") if data_len >= 12 else None
        drone_recv_ns = int.from_bytes(data[-8:], "big") if data_len >= 20 else None

        log_recv = False
        with self._lock:
            self.rcvd += 1
            self.rcvd_bytes += data_len
            t_send = self.pending.pop(seq, None)
            if t_send is None:
                t_send = header_t_send

            if t_send is not None:
                rtt = t_recv - t_send
                if rtt >= 0:
                    self.rtt_sum_ns += rtt
                    self.rtt_samples += 1
                    if rtt > self.rtt_max_ns:
                        self.rtt_max_ns = rtt
                    if self.rtt_min_ns is None or rtt < self.rtt_min_ns:
                        self.rtt_min_ns = rtt
                    self.rtt_p50.add(rtt)
                    self.rtt_p95.add(rtt)
                    log_recv = True

            if t_send is not None and drone_recv_ns is not None:
                owd_up_ns = drone_recv_ns - t_send
                if 0 <= owd_up_ns <= 5_000_000_000:
                    self.owd_samples += 1
                    self.owd_p50.add(owd_up_ns)
                    self.owd_p95.add(owd_up_ns)
            if data_len < 20:
                self.truncated += 1

        if log_recv:
            self._maybe_log("recv", seq, int(t_recv))
        return True

    def _cleanup(self) -> None:
        if self.events:
            try:
                self.events.flush()
                self.events.close()
            except Exception:
                pass
            self.events = None


def wait_handshake(timeout: float = 20.0) -> bool:
    deadline = time.time() + timeout
    while time.time() < deadline:
        if PROXY_STATUS_PATH.exists():
            try:
                with open(PROXY_STATUS_PATH, encoding="utf-8") as handle:
                    js = json.load(handle)
            except Exception:
                js = {}
            state = js.get("state") or js.get("status")
            if state in {"running", "completed", "ready", "handshake_ok"}:
                return True
        time.sleep(0.3)
    return False


def wait_active_suite(target: str, timeout: float = 10.0) -> bool:
    return wait_rekey_transition(target, timeout=timeout)


def wait_pending_suite(target: str, timeout: float = 18.0, stable_checks: int = 2) -> bool:
    deadline = time.time() + timeout
    stable = 0
    while time.time() < deadline:
        try:
            status = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status = {}
        pending = status.get("pending_suite")
        suite = status.get("suite")
        if pending == target:
            stable += 1
            if stable >= stable_checks:
                return True
        elif suite == target and pending in (None, "", target):
            # Rekey may have already completed; treat as success.
            return True
        else:
            stable = 0
        time.sleep(0.2)
    return False


def wait_rekey_transition(target: str, timeout: float = 20.0, stable_checks: int = 3) -> bool:
    deadline = time.time() + timeout
    last_status: dict = {}
    stable = 0
    while time.time() < deadline:
        try:
            status = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status = {}
        last_status = status
        suite = status.get("suite")
        pending = status.get("pending_suite")
        last_requested = status.get("last_requested_suite")
        if suite == target and (pending in (None, "", target)):
            stable += 1
            if stable >= stable_checks:
                if last_requested and last_requested not in (suite, target):
                    print(
                        f"[{ts()}] follower reports suite={suite} but last_requested={last_requested}; continuing anyway",
                        file=sys.stderr,
                    )
                return True
        else:
            stable = 0
        time.sleep(0.2)
    if last_status:
        print(
            f"[{ts()}] follower status before timeout: suite={last_status.get('suite')} pending={last_status.get('pending_suite')}",
            file=sys.stderr,
        )
    return False


def timesync() -> dict:
    t1 = time.time_ns()
    resp = ctl_send({"cmd": "timesync", "t1_ns": t1})
    t4 = time.time_ns()
    t2 = int(resp.get("t2_ns", t1))
    t3 = int(resp.get("t3_ns", t4))
    delay_ns = (t4 - t1) - (t3 - t2)
    offset_ns = ((t2 - t1) + (t3 - t4)) // 2
    return {"offset_ns": offset_ns, "rtt_ns": delay_ns}


def snapshot_proxy_artifacts(suite: str) -> None:
    target_dir = suite_outdir(suite)
    if PROXY_STATUS_PATH.exists():
        _robust_copy(PROXY_STATUS_PATH, target_dir / "gcs_status.json")
    if PROXY_SUMMARY_PATH.exists():
        _robust_copy(PROXY_SUMMARY_PATH, target_dir / "gcs_summary.json")


def start_gcs_proxy(initial_suite: str) -> tuple[subprocess.Popen, IO[str]]:
    key_path = SECRETS_DIR / initial_suite / "gcs_signing.key"
    if not key_path.exists():
        raise FileNotFoundError(f"Missing GCS signing key for suite {initial_suite}: {key_path}")

    mkdirp(OUTDIR)
    log_path = OUTDIR / f"gcs_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_handle: IO[str] = open(log_path, "w", encoding="utf-8", errors="replace")

    env = os.environ.copy()
    env["DRONE_HOST"] = DRONE_HOST
    env["GCS_HOST"] = GCS_HOST
    env["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    env["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"

    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str

    proc = subprocess.Popen(
        [
            sys.executable,
            "-m",
            "core.run_proxy",
            "gcs",
            "--suite",
            initial_suite,
            "--gcs-secret-file",
            str(key_path),
            "--control-manual",
            "--status-file",
            str(PROXY_STATUS_PATH),
            "--json-out",
            str(PROXY_SUMMARY_PATH),
        ],
        stdin=subprocess.PIPE,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
        env=env,
        cwd=str(ROOT),
    )
    return proc, log_handle


def read_proxy_stats_live() -> dict:
    try:
        with open(PROXY_STATUS_PATH, encoding="utf-8") as handle:
            js = json.load(handle)
    except Exception:
        return {}
    if isinstance(js, dict):
        counters = js.get("counters")
        if isinstance(counters, dict):
            return counters
        if any(k in js for k in ("enc_out", "enc_in")):
            return js
    return {}


def read_proxy_summary() -> dict:
    if not PROXY_SUMMARY_PATH.exists():
        return {}
    try:
        with open(PROXY_SUMMARY_PATH, encoding="utf-8") as handle:
            return json.load(handle)
    except Exception:
        return {}



def _read_proxy_counters() -> dict:

    counters = read_proxy_stats_live()

    if isinstance(counters, dict) and counters:

        return counters

    summary = read_proxy_summary()

    if isinstance(summary, dict):

        summary_counters = summary.get("counters")

        if isinstance(summary_counters, dict):

            return summary_counters

        if any(key in summary for key in ("enc_out", "enc_in", "rekeys_ok", "rekeys_fail", "last_rekey_suite")):

            return summary

    return {}





def wait_proxy_rekey(

    target_suite: str,

    baseline: Dict[str, object],

    *,

    timeout: float = 20.0,

    poll_interval: float = 0.4,

    proc: subprocess.Popen,

) -> str:

    start = time.time()

    baseline_ok = int(baseline.get("rekeys_ok", 0) or 0)

def _extract_companion_metrics(
    samples: List[dict],
    *,
    suite: str,
    start_ns: int,
    end_ns: int,
) -> Dict[str, object]:
    cpu_max = 0.0
    rss_max_bytes = 0
    pfc_sum = 0.0
    vh_sum = 0.0
    vv_sum = 0.0
    kin_count = 0

    for sample in samples:
        try:
            ts_ns = int(sample.get("timestamp_ns"))
        except (TypeError, ValueError):
            continue
        if ts_ns < start_ns or ts_ns > end_ns:
            continue
        sample_suite = str(sample.get("suite") or "").strip()
        if sample_suite and sample_suite != suite:
            continue

        kind = str(sample.get("kind") or "").lower()
        if kind == "system_sample":
            cpu_val = _as_float(sample.get("cpu_percent"))
            if cpu_val is not None:
                cpu_max = max(cpu_max, cpu_val)
            mem_mb = _as_float(sample.get("mem_used_mb"))
            if mem_mb is not None:
                rss_candidate = int(mem_mb * 1024 * 1024)
                rss_max_bytes = max(rss_max_bytes, rss_candidate)
        elif kind == "psutil_sample":
            cpu_val = _as_float(sample.get("cpu_percent"))
            if cpu_val is not None:
                cpu_max = max(cpu_max, cpu_val)
            rss_val = _as_float(sample.get("rss_bytes"))
            if rss_val is not None:
                rss_candidate = int(rss_val)
                rss_max_bytes = max(rss_max_bytes, rss_candidate)
        elif kind == "kinematics":
            pfc_val = _as_float(sample.get("predicted_flight_constraint_w"))
            vh_val = _as_float(sample.get("velocity_horizontal_mps"))
            vv_val = _as_float(sample.get("velocity_vertical_mps"))
            if pfc_val is not None:
                pfc_sum += pfc_val
            if vh_val is not None:
                vh_sum += vh_val
            if vv_val is not None:
                vv_sum += vv_val
            kin_count += 1

    avg_vh = vh_sum / kin_count if kin_count else 0.0
    avg_vv = vv_sum / kin_count if kin_count else 0.0
    avg_pfc = pfc_sum / kin_count if kin_count else 0.0

    return {
        "cpu_max_percent": round(cpu_max, 3),
        "max_rss_bytes": int(max(0, rss_max_bytes)),
        "pfc_watts": round(avg_pfc, 3),
        "kinematics_vh": round(avg_vh, 3),
        "kinematics_vv": round(avg_vv, 3),
    }


    baseline_fail = int(baseline.get("rekeys_fail", 0) or 0)

    while time.time() - start < timeout:

        if proc.poll() is not None:

            raise RuntimeError("GCS proxy exited during rekey")

        counters = _read_proxy_counters()

        if counters:

            rekeys_ok = int(counters.get("rekeys_ok", 0) or 0)

            rekeys_fail = int(counters.get("rekeys_fail", 0) or 0)

            last_suite = counters.get("last_rekey_suite") or counters.get("suite") or ""

            if rekeys_fail > baseline_fail:

                return "fail"

            if rekeys_ok > baseline_ok and (not last_suite or last_suite == target_suite):

                return "ok"

        time.sleep(poll_interval)

    return "timeout"


def activate_suite(
    gcs: subprocess.Popen,
    suite: str,
    is_first: bool,
) -> Tuple[float, Optional[int], Optional[int]]:

    if gcs.poll() is not None:

        raise RuntimeError("GCS proxy is not running; cannot continue")

    start_ns = time.time_ns()
    mark_ns: Optional[int] = None
    rekey_complete_ns: Optional[int] = None

    if is_first:
        mark_ns = None
        rekey_complete_ns = None

        if not wait_rekey_transition(suite, timeout=12.0):
            raise RuntimeError(f"Follower did not confirm initial suite {suite}")

    else:

        assert gcs.stdin is not None

        try:
            status_snapshot = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status_snapshot = {}
        previous_suite = status_snapshot.get("suite")

        print(f"[{ts()}] rekey -> {suite}")

        gcs.stdin.write(suite + "\n")
        gcs.stdin.flush()

        baseline = _read_proxy_counters()

        mark_ns = time.time_ns()
        try:
            ctl_send({"cmd": "mark", "suite": suite, "kind": "rekey"})
        except Exception as exc:
            print(f"[WARN] control mark failed for {suite}: {exc}", file=sys.stderr)
        pending_ack = False
        pending_ack_error: Optional[str] = None
        try:
            pending_ack = wait_pending_suite(suite, timeout=12.0)
        except Exception as exc:
            pending_ack_error = str(exc)

        rekey_status = "timeout"

        try:

            result = wait_proxy_rekey(suite, baseline, timeout=24.0, proc=gcs)

            rekey_status = result

            if result == "timeout":

                print(f"[WARN] timed out waiting for proxy to activate suite {suite}", file=sys.stderr)

            elif result == "fail":

                print(f"[WARN] proxy reported failed rekey for suite {suite}", file=sys.stderr)

        except RuntimeError as exc:
            rekey_status = "error"
            raise
        except Exception as exc:
            rekey_status = "error"
            print(f"[WARN] error while waiting for proxy rekey {suite}: {exc}", file=sys.stderr)
        finally:
            try:
                rekey_complete_ns = time.time_ns()
                ctl_send({"cmd": "rekey_complete", "suite": suite, "status": rekey_status})
            except Exception as exc:
                print(f"[WARN] rekey_complete failed for {suite}: {exc}", file=sys.stderr)

        if rekey_status != "ok":
            if not pending_ack and pending_ack_error:
                print(
                    f"[WARN] follower pending status check failed for suite {suite}: {pending_ack_error}",
                    file=sys.stderr,
                )
            elif not pending_ack:
                print(
                    f"[WARN] follower did not acknowledge pending suite {suite} before proxy reported {rekey_status}",
                    file=sys.stderr,
                )
            if not previous_suite:
                raise RuntimeError(f"Proxy rekey to {suite} reported {rekey_status}; previous suite unknown")
            expected_suite = previous_suite
        else:
            expected_suite = suite

        if not wait_rekey_transition(expected_suite, timeout=24.0):
            raise RuntimeError(
                f"Follower did not confirm suite {expected_suite} after rekey status {rekey_status}"
            )

        if rekey_status != "ok":
            raise RuntimeError(f"Proxy reported rekey status {rekey_status} for suite {suite}")

    if REKEY_SETTLE_SECONDS > 0:
        time.sleep(REKEY_SETTLE_SECONDS)

    elapsed_ms = (time.time_ns() - start_ns) / 1_000_000
    return elapsed_ms, mark_ns, rekey_complete_ns




def run_suite(
    gcs: subprocess.Popen,
    suite: str,
    is_first: bool,
    duration_s: float,
    payload_bytes: int,
    event_sample: int,
    offset_ns: int,
    pass_index: int,
    traffic_mode: str,
    pre_gap: float,
    inter_gap_s: float,
    rate_pps: int,
    target_bandwidth_mbps: float,
    power_capture_enabled: bool,
    clock_offset_warmup_s: float,
    min_delay_samples: int,
) -> dict:
    rekey_duration_ms, rekey_mark_ns, rekey_complete_ns = activate_suite(gcs, suite, is_first)

    effective_sample_every, effective_min_delay = _compute_sampling_params(
        duration_s,
        event_sample,
        min_delay_samples,
    )

    events_path = suite_outdir(suite) / EVENTS_FILENAME
    start_mark_ns = time.time_ns() + offset_ns + int(0.150 * 1e9) + int(max(pre_gap, 0.0) * 1e9)
    try:
        ctl_send(
            {
                "cmd": "schedule_mark",
                "suite": suite,
                "t0_ns": start_mark_ns,
                "kind": "window",
            }
        )
    except Exception as exc:
        print(f"[WARN] schedule_mark failed for {suite}: {exc}", file=sys.stderr)

    power_request_ok = False
    power_request_error: Optional[str] = None
    power_status: dict = {}
    if power_capture_enabled:
        power_start_ns = time.time_ns() + offset_ns + int(max(pre_gap, 0.0) * 1e9)
        power_resp = request_power_capture(suite, duration_s, power_start_ns)
        power_request_ok = bool(power_resp.get("ok"))
        power_request_error = power_resp.get("error") if not power_request_ok else None
        if not power_request_ok and power_request_error:
            print(f"[WARN] power capture not scheduled: {power_request_error}", file=sys.stderr)
        banner = f"[{ts()}] ===== POWER: START in {pre_gap:.1f}s | suite={suite} | duration={duration_s:.1f}s mode={traffic_mode} ====="
    else:
        power_request_error = "disabled"
        banner = (
            f"[{ts()}] ===== TRAFFIC: START in {pre_gap:.1f}s | suite={suite} | duration={duration_s:.1f}s "
            f"mode={traffic_mode} (power capture disabled) ====="
        )
    print(banner)
    if pre_gap > 0:
        time.sleep(pre_gap)

    warmup_s = max(clock_offset_warmup_s, min(MAX_WARMUP_SECONDS, duration_s * WARMUP_FRACTION))
    start_wall_ns = time.time_ns()
    start_perf_ns = time.perf_counter_ns()
    sent_packets = 0
    rcvd_packets = 0
    rcvd_bytes = 0
    avg_rtt_ns = 0
    max_rtt_ns = 0
    rtt_samples = 0
    blaster_sent_bytes = 0

    wire_header_bytes = UDP_HEADER_BYTES + APP_IP_HEADER_BYTES

    if traffic_mode in {"blast", "constant"}:
        if warmup_s > 0:
            warmup_blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path=None,
                payload_bytes=payload_bytes,
                sample_every=0,
                offset_ns=offset_ns,
            )
            warmup_blaster.run(duration_s=warmup_s, rate_pps=rate_pps)
        start_wall_ns = time.time_ns()
        start_perf_ns = time.perf_counter_ns()
        blaster = Blaster(
            APP_SEND_HOST,
            APP_SEND_PORT,
            APP_RECV_HOST,
            APP_RECV_PORT,
            events_path,
            payload_bytes=payload_bytes,
            sample_every=effective_sample_every if effective_sample_every > 0 else 0,
            offset_ns=offset_ns,
        )
        blaster.run(duration_s=duration_s, rate_pps=rate_pps)
        sent_packets = blaster.sent
        rcvd_packets = blaster.rcvd
        rcvd_bytes = blaster.rcvd_bytes
        blaster_sent_bytes = blaster.sent_bytes
        wire_header_bytes = getattr(blaster, "wire_header_bytes", wire_header_bytes)
        sample_count = max(1, blaster.rtt_samples)
        avg_rtt_ns = blaster.rtt_sum_ns // sample_count
        max_rtt_ns = blaster.rtt_max_ns
        rtt_samples = blaster.rtt_samples
    else:
        time.sleep(duration_s)

    end_wall_ns = time.time_ns()
    end_perf_ns = time.perf_counter_ns()
    if power_capture_enabled:
        print(f"[{ts()}] ===== POWER: STOP | suite={suite} =====")
    else:
        print(f"[{ts()}] ===== TRAFFIC: STOP | suite={suite} =====")

    snapshot_proxy_artifacts(suite)
    proxy_stats = read_proxy_stats_live() or read_proxy_summary()

    if power_capture_enabled and power_request_ok:
        power_status = poll_power_status(max_wait_s=max(6.0, duration_s * 0.25))
        if power_status.get("error"):
            print(f"[WARN] power status error: {power_status['error']}", file=sys.stderr)
        if power_status.get("busy"):
            power_note = "capture_incomplete:busy"
            if not power_status.get("error") and not power_request_error:
                power_status.setdefault("error", "capture_incomplete")

    power_summary = power_status.get("last_summary") if isinstance(power_status, dict) else None
    status_for_extract: Dict[str, Any] = {}
    if isinstance(power_status, dict) and power_status:
        status_for_extract = power_status
    elif power_summary:
        status_for_extract = {"last_summary": power_summary}
    power_fields = extract_power_fields(status_for_extract) if status_for_extract else {}
    power_capture_complete = bool(power_summary)
    power_error = None
    if not power_capture_complete:
        if isinstance(power_status, dict):
            power_error = power_status.get("error")
            if not power_error and power_status.get("busy"):
                power_error = "capture_incomplete"
        if power_error is None:
            power_error = power_request_error

    if not power_capture_enabled:
        power_note = "disabled"
    elif not power_request_ok:
        power_note = f"request_error:{power_error}" if power_error else "request_error"
    elif power_capture_complete:
        power_note = "ok"
    else:
        power_note = f"capture_incomplete:{power_error}" if power_error else "capture_incomplete"

    elapsed_s = max(1e-9, (end_perf_ns - start_perf_ns) / 1e9)
    pps = sent_packets / elapsed_s if elapsed_s > 0 else 0.0
    throughput_mbps = (rcvd_bytes * 8) / (elapsed_s * 1_000_000) if elapsed_s > 0 else 0.0
    sent_mbps = (blaster_sent_bytes * 8) / (elapsed_s * 1_000_000) if blaster_sent_bytes else 0.0
    delivered_ratio = throughput_mbps / sent_mbps if sent_mbps > 0 else 0.0
    avg_rtt_ms = avg_rtt_ns / 1_000_000
    max_rtt_ms = max_rtt_ns / 1_000_000

    app_packet_bytes = payload_bytes + SEQ_TS_OVERHEAD_BYTES
    wire_packet_bytes_est = app_packet_bytes + wire_header_bytes
    goodput_mbps = (rcvd_packets * payload_bytes * 8) / (elapsed_s * 1_000_000) if elapsed_s > 0 else 0.0
    wire_throughput_mbps_est = (
        (rcvd_packets * wire_packet_bytes_est * 8) / (elapsed_s * 1_000_000)
        if elapsed_s > 0
        else 0.0
    )
    if sent_mbps > 0:
        goodput_ratio = goodput_mbps / sent_mbps
        goodput_ratio = max(0.0, min(1.0, goodput_ratio))
    else:
        goodput_ratio = 0.0

    owd_p50_ms = 0.0
    owd_p95_ms = 0.0
    rtt_p50_ms = 0.0
    rtt_p95_ms = 0.0
    sample_quality = "disabled" if effective_sample_every == 0 else "low"
    owd_samples = 0

    if traffic_mode in {"blast", "constant"}:
        owd_p50_ms = blaster.owd_p50_ns / 1_000_000
        owd_p95_ms = blaster.owd_p95_ns / 1_000_000
        rtt_p50_ms = blaster.rtt_p50_ns / 1_000_000
        rtt_p95_ms = blaster.rtt_p95_ns / 1_000_000
        owd_samples = blaster.owd_samples
        if effective_sample_every > 0:
            if (
                effective_min_delay == 0
                or (blaster.rtt_samples >= effective_min_delay and blaster.owd_samples >= effective_min_delay)
            ):
                sample_quality = "ok"

    loss_pct = 0.0
    if sent_packets:
        loss_pct = max(0.0, (sent_packets - rcvd_packets) * 100.0 / sent_packets)
    loss_successes = max(0, sent_packets - rcvd_packets)
    loss_low, loss_high = wilson_interval(loss_successes, sent_packets)

    power_avg_w_val = power_fields.get("avg_power_w") if power_fields else None
    if power_avg_w_val is None and power_summary:
        power_avg_w_val = power_summary.get("avg_power_w")
    if power_avg_w_val is not None:
        try:
            power_avg_w_val = float(power_avg_w_val)
        except (TypeError, ValueError):
            power_avg_w_val = None
    power_energy_val = power_fields.get("energy_j") if power_fields else None
    if power_energy_val is None and power_summary:
        power_energy_val = power_summary.get("energy_j")
    if power_energy_val is not None:
        try:
            power_energy_val = float(power_energy_val)
        except (TypeError, ValueError):
            power_energy_val = None
    power_duration_val = power_fields.get("duration_s") if power_fields else None
    if power_duration_val is None and power_summary:
        power_duration_val = power_summary.get("duration_s")
    if power_duration_val is not None:
        try:
            power_duration_val = float(power_duration_val)
        except (TypeError, ValueError):
            power_duration_val = None
    power_summary_path_val = ""
    if power_fields and power_fields.get("summary_json_path"):
        power_summary_path_val = str(power_fields.get("summary_json_path") or "")
    elif power_summary:
        power_summary_path_val = str(power_summary.get("summary_json_path") or power_summary.get("csv_path") or "")
    power_csv_path_val = power_summary.get("csv_path") if power_summary else ""
    power_samples_val = power_summary.get("samples") if power_summary else 0
    power_avg_current_val = (
        round(power_summary.get("avg_current_a", 0.0), 6) if power_summary else 0.0
    )
    power_avg_voltage_val = (
        round(power_summary.get("avg_voltage_v", 0.0), 6) if power_summary else 0.0
    )
    power_sample_rate_val = (
        round(power_summary.get("sample_rate_hz", 0.0), 3) if power_summary else 0.0
    )

    companion_metrics = {
        "cpu_max_percent": 0.0,
        "max_rss_bytes": 0,
        "pfc_watts": 0.0,
        "kinematics_vh": 0.0,
        "kinematics_vv": 0.0,
    }
    if telemetry_collector and telemetry_collector.enabled:
        try:
            companion_metrics = _extract_companion_metrics(
                telemetry_collector.snapshot(),
                suite=suite,
                start_ns=start_wall_ns,
                end_ns=end_wall_ns,
            )
        except Exception as exc:
            print(f"[WARN] telemetry aggregation failed for suite {suite}: {exc}", file=sys.stderr)

    row = {
        "pass": pass_index,
        "suite": suite,
        "traffic_mode": traffic_mode,
        "pre_gap_s": round(pre_gap, 3),
    "inter_gap_s": round(inter_gap_s, 3),
        "duration_s": round(elapsed_s, 3),
        "sent": sent_packets,
        "rcvd": rcvd_packets,
        "pps": round(pps, 1),
        "target_rate_pps": rate_pps,
        "target_bandwidth_mbps": round(target_bandwidth_mbps, 3) if target_bandwidth_mbps else 0.0,
        "throughput_mbps": round(throughput_mbps, 3),
        "sent_mbps": round(sent_mbps, 3),
        "delivered_ratio": round(delivered_ratio, 3) if sent_mbps > 0 else 0.0,
        "goodput_mbps": round(goodput_mbps, 3),
        "wire_throughput_mbps_est": round(wire_throughput_mbps_est, 3),
        "app_packet_bytes": app_packet_bytes,
        "wire_packet_bytes_est": wire_packet_bytes_est,
    "cpu_max_percent": companion_metrics["cpu_max_percent"],
    "max_rss_bytes": companion_metrics["max_rss_bytes"],
    "pfc_watts": companion_metrics["pfc_watts"],
    "kinematics_vh": companion_metrics["kinematics_vh"],
    "kinematics_vv": companion_metrics["kinematics_vv"],
        "goodput_ratio": round(goodput_ratio, 3),
        "rtt_avg_ms": round(avg_rtt_ms, 3),
        "rtt_max_ms": round(max_rtt_ms, 3),
        "rtt_p50_ms": round(rtt_p50_ms, 3),
        "rtt_p95_ms": round(rtt_p95_ms, 3),
        "owd_p50_ms": round(owd_p50_ms, 3),
        "owd_p95_ms": round(owd_p95_ms, 3),
        "rtt_samples": rtt_samples,
        "owd_samples": owd_samples,
        "sample_every": effective_sample_every,
        "min_delay_samples": effective_min_delay,
        "sample_quality": sample_quality,
        "loss_pct": round(loss_pct, 3),
        "loss_pct_wilson_low": round(loss_low * 100.0, 3),
        "loss_pct_wilson_high": round(loss_high * 100.0, 3),
        "enc_out": proxy_stats.get("enc_out", 0),
        "enc_in": proxy_stats.get("enc_in", 0),
        "drops": proxy_stats.get("drops", 0),
        "rekeys_ok": proxy_stats.get("rekeys_ok", 0),
        "rekeys_fail": proxy_stats.get("rekeys_fail", 0),
        "start_ns": start_wall_ns,
        "end_ns": end_wall_ns,
        "scheduled_mark_ns": start_mark_ns,
        "rekey_mark_ns": rekey_mark_ns,
        "rekey_ok_ns": rekey_complete_ns,
        "rekey_ms": round(rekey_duration_ms, 3),
    "rekey_energy_mJ": 0.0,
        "power_request_ok": power_request_ok,
        "power_capture_ok": power_capture_complete,
        "power_note": power_note,
        "power_error": power_error,
        "power_avg_w": round(power_avg_w_val, 6) if power_avg_w_val is not None else 0.0,
        "power_energy_j": round(power_energy_val, 6) if power_energy_val is not None else 0.0,
        "power_samples": power_samples_val,
        "power_avg_current_a": power_avg_current_val,
        "power_avg_voltage_v": power_avg_voltage_val,
        "power_sample_rate_hz": power_sample_rate_val,
        "power_duration_s": round(power_duration_val, 3) if power_duration_val is not None else 0.0,
        "power_csv_path": power_csv_path_val or "",
        "power_summary_path": power_summary_path_val or "",
        "blackout_ms": None,
        "gap_max_ms": None,
        "gap_p99_ms": None,
        "steady_gap_ms": None,
        "recv_rate_kpps_before": None,
        "recv_rate_kpps_after": None,
        "proc_ns_p95": None,
        "pair_start_ns": None,
        "pair_end_ns": None,
        "blackout_error": None,
        "timing_guard_ms": None,
        "timing_guard_violation": False,
    }

    rekey_energy_error: Optional[str] = None
    if (
        power_csv_path_val
        and isinstance(power_csv_path_val, str)
        and rekey_mark_ns is not None
        and rekey_complete_ns is not None
        and rekey_complete_ns > rekey_mark_ns
    ):
        try:
            energy_mj = calculate_transient_energy(
                power_csv_path_val,
                int(rekey_mark_ns),
                int(rekey_complete_ns),
            )
            row["rekey_energy_mJ"] = round(energy_mj, 3)
        except FileNotFoundError as exc:
            rekey_energy_error = str(exc)
        except ValueError as exc:
            rekey_energy_error = str(exc)
        except Exception as exc:
            rekey_energy_error = str(exc)

    if rekey_energy_error:
        row["rekey_energy_error"] = rekey_energy_error

    if power_summary:
        print(
            f"[{ts()}] power summary suite={suite} avg={power_summary.get('avg_power_w', 0.0):.3f} W "
            f"energy={power_summary.get('energy_j', 0.0):.3f} J samples={power_summary.get('samples', 0)}"
        )
    elif power_capture_enabled and power_request_ok and power_error:
        print(f"[{ts()}] power summary unavailable for suite={suite}: {power_error}")

    target_desc = f" target={target_bandwidth_mbps:.2f} Mb/s" if target_bandwidth_mbps > 0 else ""
    print(
        f"[{ts()}] <<< FINISH suite={suite} mode={traffic_mode} sent={sent_packets} rcvd={rcvd_packets} "
        f"pps~{pps:.0f} thr~{throughput_mbps:.2f} Mb/s sent~{sent_mbps:.2f} Mb/s loss={loss_pct:.2f}% "
        f"rtt_avg={avg_rtt_ms:.3f}ms rtt_max={max_rtt_ms:.3f}ms rekey={rekey_duration_ms:.2f}ms "
        f"enc_out={row['enc_out']} enc_in={row['enc_in']}{target_desc} >>>"
    )

    return row


def write_summary(rows: List[dict]) -> None:
    if not rows:
        return
    mkdirp(OUTDIR)
    headers = list(rows[0].keys())
    for attempt in range(3):
        try:
            buffer = io.StringIO()
            writer = csv.DictWriter(buffer, fieldnames=headers)
            writer.writeheader()
            writer.writerows(rows)
            _atomic_write_bytes(SUMMARY_CSV, buffer.getvalue().encode("utf-8"))
            print(f"[{ts()}] wrote {SUMMARY_CSV}")
            return
        except Exception as exc:
            if attempt == 2:
                print(f"[WARN] failed to write {SUMMARY_CSV}: {exc}", file=sys.stderr)
            time.sleep(0.1)


def _append_blackout_records(records: List[Dict[str, Any]]) -> None:
    if not records:
        return
    try:
        BLACKOUT_CSV.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = [
            "timestamp_utc",
            "session_id",
            "index",
            "pass",
            "suite",
            "traffic_mode",
            "rekey_mark_ns",
            "rekey_ok_ns",
            "scheduled_mark_ns",
            "blackout_ms",
            "gap_max_ms",
            "gap_p99_ms",
            "steady_gap_ms",
            "recv_rate_kpps_before",
            "recv_rate_kpps_after",
            "proc_ns_p95",
            "pair_start_ns",
            "pair_end_ns",
            "blackout_error",
        ]
        new_file = not BLACKOUT_CSV.exists()
        with BLACKOUT_CSV.open("a", newline="", encoding="utf-8") as handle:
            writer = csv.DictWriter(handle, fieldnames=fieldnames)
            if new_file:
                writer.writeheader()
            for record in records:
                writer.writerow(record)
        print(f"[{ts()}] updated {BLACKOUT_CSV} ({len(records)} rows)")
    except Exception as exc:
        print(f"[WARN] blackout log append failed: {exc}", file=sys.stderr)


def _append_step_results(payloads: List[Dict[str, Any]]) -> None:
    if not payloads:
        return
    try:
        STEP_RESULTS_PATH.parent.mkdir(parents=True, exist_ok=True)
        with STEP_RESULTS_PATH.open("a", encoding="utf-8") as handle:
            for payload in payloads:
                handle.write(json.dumps(payload) + "\n")
        print(f"[{ts()}] appended {len(payloads)} step records -> {STEP_RESULTS_PATH}")
    except Exception as exc:
        print(f"[WARN] step_results append failed: {exc}", file=sys.stderr)


def _enrich_summary_rows(
    rows: List[dict],
    *,
    session_id: str,
    drone_session_dir: Optional[Path],
    traffic_mode: str,
    pre_gap_s: float,
    duration_s: float,
    inter_gap_s: float,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    blackout_records: List[Dict[str, Any]] = []
    step_payloads: List[Dict[str, Any]] = []
    session_dir_exists = bool(drone_session_dir and drone_session_dir.exists())
    session_dir_str = str(drone_session_dir) if drone_session_dir else ""
    for index, row in enumerate(rows):
        mark_ns = row.get("rekey_mark_ns")
        ok_ns = row.get("rekey_ok_ns")
        metrics: Dict[str, Any] = {}
        blackout_error: Optional[str] = None
        if session_dir_exists and mark_ns and ok_ns and ok_ns >= mark_ns:
            try:
                metrics = compute_blackout(drone_session_dir, int(mark_ns), int(ok_ns))
            except Exception as exc:
                blackout_error = str(exc)
                metrics = {}
        else:
            if not session_dir_exists:
                blackout_error = "session_dir_unavailable"
            elif not mark_ns or not ok_ns:
                blackout_error = "missing_mark_or_ok"
            elif ok_ns is not None and mark_ns is not None and ok_ns < mark_ns:
                blackout_error = "invalid_timestamp_order"

        row["blackout_ms"] = metrics.get("blackout_ms")
        row["gap_max_ms"] = metrics.get("gap_max_ms")
        row["gap_p99_ms"] = metrics.get("gap_p99_ms")
        row["steady_gap_ms"] = metrics.get("steady_gap_ms")
        row["recv_rate_kpps_before"] = metrics.get("recv_rate_kpps_before")
        row["recv_rate_kpps_after"] = metrics.get("recv_rate_kpps_after")
        row["proc_ns_p95"] = metrics.get("proc_ns_p95")
        row["pair_start_ns"] = metrics.get("pair_start_ns")
        row["pair_end_ns"] = metrics.get("pair_end_ns")
        if blackout_error is None:
            blackout_error = metrics.get("error")
        row["blackout_error"] = blackout_error

        guard_ms = int(
            max(row.get("pre_gap_s", pre_gap_s) or 0.0, 0.0) * 1000.0
            + max(row.get("duration_s", duration_s) or 0.0, 0.0) * 1000.0
            + 10_000
        )
        row["timing_guard_ms"] = guard_ms
        rekey_ms = row.get("rekey_ms") or 0.0
        try:
            rekey_ms_val = float(rekey_ms)
        except (TypeError, ValueError):
            rekey_ms_val = 0.0
        timing_violation = bool(rekey_ms_val and rekey_ms_val > guard_ms)
        row["timing_guard_violation"] = timing_violation
        if timing_violation:
            print(
                f"[WARN] rekey duration {rekey_ms_val:.2f} ms exceeds guard {guard_ms} ms (suite={row.get('suite')} pass={row.get('pass')})",
                file=sys.stderr,
            )

        row.setdefault("traffic_mode", traffic_mode)
        row.setdefault("pre_gap_s", pre_gap_s)
        row.setdefault("inter_gap_s", inter_gap_s)

        blackout_records.append(
            {
                "timestamp_utc": ts(),
                "session_id": session_id,
                "index": index,
                "pass": row.get("pass"),
                "suite": row.get("suite"),
                "traffic_mode": row.get("traffic_mode"),
                "rekey_mark_ns": mark_ns or "",
                "rekey_ok_ns": ok_ns or "",
                "scheduled_mark_ns": row.get("scheduled_mark_ns") or "",
                "blackout_ms": row.get("blackout_ms"),
                "gap_max_ms": row.get("gap_max_ms"),
                "gap_p99_ms": row.get("gap_p99_ms"),
                "steady_gap_ms": row.get("steady_gap_ms"),
                "recv_rate_kpps_before": row.get("recv_rate_kpps_before"),
                "recv_rate_kpps_after": row.get("recv_rate_kpps_after"),
                "proc_ns_p95": row.get("proc_ns_p95"),
                "pair_start_ns": row.get("pair_start_ns"),
                "pair_end_ns": row.get("pair_end_ns"),
                "blackout_error": blackout_error or "",
            }
        )

        payload = dict(row)
        payload["ts_utc"] = ts()
        payload["session_id"] = session_id
        payload["session_dir"] = session_dir_str
        payload["index"] = index
        payload["blackout_error"] = blackout_error
        payload["timing_guard_ms"] = guard_ms
        payload["timing_guard_violation"] = timing_violation
        step_payloads.append(payload)

    return blackout_records, step_payloads


class SaturationTester:
    def __init__(
        self,
        suite: str,
        payload_bytes: int,
        duration_s: float,
        event_sample: int,
        offset_ns: int,
        output_dir: Path,
        max_rate_mbps: int,
        search_mode: str,
        delivery_threshold: float,
        loss_threshold: float,
        spike_factor: float,
        min_delay_samples: int,
    ) -> None:
        self.suite = suite
        self.payload_bytes = payload_bytes
        self.duration_s = duration_s
        self.event_sample = max(0, int(event_sample))
        self.offset_ns = offset_ns
        self.output_dir = output_dir
        self.max_rate_mbps = max_rate_mbps
        self.search_mode = search_mode
        self.delivery_threshold = delivery_threshold
        self.loss_threshold = loss_threshold
        self.spike_factor = spike_factor
        self.min_delay_samples = max(0, int(min_delay_samples))
        self.records: List[Dict[str, float]] = []
        self._rate_cache: Dict[int, Tuple[Dict[str, float], bool, Optional[str]]] = {}
        self._baseline: Optional[Dict[str, float]] = None
        self._signal_history = {key: deque(maxlen=HYSTERESIS_WINDOW) for key in SATURATION_SIGNALS}
        self._last_ok_rate: Optional[int] = None
        self._first_bad_rate: Optional[int] = None
        self._stop_cause: Optional[str] = None
        self._stop_samples = 0

    def run(self) -> Dict[str, Optional[float]]:
        self.records = []
        self._rate_cache.clear()
        self._baseline = None
        self._signal_history = {key: deque(maxlen=HYSTERESIS_WINDOW) for key in SATURATION_SIGNALS}
        self._last_ok_rate = None
        self._first_bad_rate = None
        self._stop_cause = None
        self._stop_samples = 0

        used_mode = self.search_mode
        if self.search_mode == "linear":
            self._linear_search()
        else:
            self._coarse_search()
            if self._first_bad_rate is not None and self._last_ok_rate is not None:
                self._bisect_search()
            elif self.search_mode == "bisect" and self._first_bad_rate is None:
                self._linear_search()
                used_mode = "linear"

        resolution = None
        if self._first_bad_rate is not None and self._last_ok_rate is not None:
            resolution = max(0, self._first_bad_rate - self._last_ok_rate)
        saturation_point = self._last_ok_rate if self._last_ok_rate is not None else self._first_bad_rate
        confidence = min(1.0, self._stop_samples / 200.0) if self._stop_samples > 0 else 0.0

        baseline = self._baseline or {}
        return {
            "suite": self.suite,
            "baseline_owd_p50_ms": baseline.get("owd_p50_ms"),
            "baseline_owd_p95_ms": baseline.get("owd_p95_ms"),
            "baseline_rtt_p50_ms": baseline.get("rtt_p50_ms"),
            "baseline_rtt_p95_ms": baseline.get("rtt_p95_ms"),
            "saturation_point_mbps": saturation_point,
            "stop_cause": self._stop_cause,
            "confidence": round(confidence, 3),
            "search_mode": used_mode,
            "resolution_mbps": resolution,
        }

    def _linear_search(self) -> None:
        for rate in SATURATION_LINEAR_RATES:
            if rate > self.max_rate_mbps:
                break
            _, is_bad, _ = self._evaluate_rate(rate)
            if is_bad:
                break

    def _coarse_search(self) -> None:
        for rate in SATURATION_COARSE_RATES:
            if rate > self.max_rate_mbps:
                break
            _, is_bad, _ = self._evaluate_rate(rate)
            if is_bad:
                break

    def _bisect_search(self) -> None:
        if self._first_bad_rate is None:
            return
        lo = self._last_ok_rate if self._last_ok_rate is not None else 0
        hi = self._first_bad_rate
        steps = 0
        while hi - lo > 5 and steps < MAX_BISECT_STEPS:
            mid = max(1, int(round((hi + lo) / 2)))
            if mid == hi or mid == lo:
                break
            _, is_bad, _ = self._evaluate_rate(mid)
            steps += 1
            metrics = self._rate_cache[mid][0]
            sample_ok = metrics.get("sample_quality") == "ok"
            if not sample_ok:
                is_bad = True
            if is_bad:
                if mid < hi:
                    hi = mid
                if self._first_bad_rate is None or mid < self._first_bad_rate:
                    self._first_bad_rate = mid
            else:
                if mid > lo:
                    lo = mid
                if self._last_ok_rate is None or mid > self._last_ok_rate:
                    self._last_ok_rate = mid

    def _evaluate_rate(self, rate: int) -> Tuple[Dict[str, float], bool, Optional[str]]:
        cached = self._rate_cache.get(rate)
        if cached:
            return cached

        metrics = self._run_rate(rate)
        metrics["suite"] = self.suite
        self.records.append(metrics)

        if self._baseline is None and metrics.get("sample_quality") == "ok":
            self._baseline = {
                "owd_p50_ms": metrics.get("owd_p50_ms"),
                "owd_p95_ms": metrics.get("owd_p95_ms"),
                "rtt_p50_ms": metrics.get("rtt_p50_ms"),
                "rtt_p95_ms": metrics.get("rtt_p95_ms"),
            }

        signals = self._classify_signals(metrics)
        is_bad = any(signals.values())
        cause = self._update_history(signals, rate, metrics)
        if is_bad:
            if self._first_bad_rate is None or rate < self._first_bad_rate:
                self._first_bad_rate = rate
        else:
            if metrics.get("sample_quality") == "ok":
                if self._last_ok_rate is None or rate > self._last_ok_rate:
                    self._last_ok_rate = rate

        result = (metrics, is_bad, cause)
        self._rate_cache[rate] = result
        return result

    def _classify_signals(self, metrics: Dict[str, float]) -> Dict[str, bool]:
        signals = {key: False for key in SATURATION_SIGNALS}
        baseline = self._baseline
        owd_spike = False
        if baseline:
            baseline_p95 = baseline.get("owd_p95_ms") or 0.0
            if baseline_p95 > 0:
                owd_p95 = metrics.get("owd_p95_ms", 0.0)
                owd_spike = owd_p95 >= baseline_p95 * self.spike_factor
        signals["owd_p95_spike"] = owd_spike

        goodput_ratio = metrics.get("goodput_ratio", 0.0)
        ratio_drop = goodput_ratio < self.delivery_threshold
        delivery_degraded = ratio_drop and owd_spike
        signals["delivery_degraded"] = delivery_degraded

        loss_flag = metrics.get("loss_pct", 0.0) > self.loss_threshold
        if metrics.get("sample_quality") != "ok" and loss_flag and not (delivery_degraded or owd_spike):
            loss_flag = False
        signals["loss_excess"] = loss_flag
        return signals

    def _update_history(
        self,
        signals: Dict[str, bool],
        rate: int,
        metrics: Dict[str, float],
    ) -> Optional[str]:
        cause = None
        for key in SATURATION_SIGNALS:
            history = self._signal_history[key]
            history.append(bool(signals.get(key)))
            if self._stop_cause is None and sum(history) >= 2:
                self._stop_cause = key
                self._stop_samples = max(metrics.get("rtt_samples", 0), metrics.get("owd_samples", 0))
                cause = key
        return cause

    def _run_rate(self, rate_mbps: int) -> Dict[str, float]:
        denominator = max(self.payload_bytes * 8, 1)
        rate_pps = int((rate_mbps * 1_000_000) / denominator)
        if rate_pps <= 0:
            rate_pps = 1
        events_path = self.output_dir / f"saturation_{rate_mbps}Mbps.jsonl"
        warmup_s = min(MAX_WARMUP_SECONDS, self.duration_s * WARMUP_FRACTION)
        effective_sample_every, effective_min_delay = _compute_sampling_params(
            self.duration_s,
            self.event_sample,
            self.min_delay_samples,
        )
        if warmup_s > 0:
            warmup_blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path=None,
                payload_bytes=self.payload_bytes,
                sample_every=0,
                offset_ns=self.offset_ns,
            )
            warmup_blaster.run(duration_s=warmup_s, rate_pps=rate_pps)
        blaster = Blaster(
            APP_SEND_HOST,
            APP_SEND_PORT,
            APP_RECV_HOST,
            APP_RECV_PORT,
            events_path,
            payload_bytes=self.payload_bytes,
            sample_every=effective_sample_every if effective_sample_every > 0 else 0,
            offset_ns=self.offset_ns,
        )
        start = time.perf_counter()
        blaster.run(duration_s=self.duration_s, rate_pps=rate_pps)
        elapsed = max(1e-9, time.perf_counter() - start)

        sent_packets = blaster.sent
        rcvd_packets = blaster.rcvd
        sent_bytes = blaster.sent_bytes
        rcvd_bytes = blaster.rcvd_bytes

        pps_actual = sent_packets / elapsed if elapsed > 0 else 0.0
        throughput_mbps = (rcvd_bytes * 8) / (elapsed * 1_000_000) if elapsed > 0 else 0.0
        sent_mbps = (sent_bytes * 8) / (elapsed * 1_000_000) if sent_bytes else 0.0
        delivered_ratio = throughput_mbps / sent_mbps if sent_mbps > 0 else 0.0

        avg_rtt_ms = (blaster.rtt_sum_ns / max(1, blaster.rtt_samples)) / 1_000_000 if blaster.rtt_samples else 0.0
        min_rtt_ms = (blaster.rtt_min_ns or 0) / 1_000_000
        max_rtt_ms = blaster.rtt_max_ns / 1_000_000

        app_packet_bytes = self.payload_bytes + SEQ_TS_OVERHEAD_BYTES
        wire_header_bytes = getattr(blaster, "wire_header_bytes", UDP_HEADER_BYTES + APP_IP_HEADER_BYTES)
        wire_packet_bytes_est = app_packet_bytes + wire_header_bytes
        goodput_mbps = (
            (rcvd_packets * self.payload_bytes * 8) / (elapsed * 1_000_000)
            if elapsed > 0
            else 0.0
        )
        wire_throughput_mbps_est = (
            (rcvd_packets * wire_packet_bytes_est * 8) / (elapsed * 1_000_000)
            if elapsed > 0
            else 0.0
        )
        if sent_mbps > 0:
            goodput_ratio = goodput_mbps / sent_mbps
            goodput_ratio = max(0.0, min(1.0, goodput_ratio))
        else:
            goodput_ratio = 0.0

        loss_pct = 0.0
        if sent_packets:
            loss_pct = max(0.0, (sent_packets - rcvd_packets) * 100.0 / sent_packets)
        loss_low, loss_high = wilson_interval(max(0, sent_packets - rcvd_packets), sent_packets)

        sample_quality = "disabled" if effective_sample_every == 0 else "low"
        if effective_sample_every > 0:
            if (
                effective_min_delay == 0
                or (blaster.rtt_samples >= effective_min_delay and blaster.owd_samples >= effective_min_delay)
            ):
                sample_quality = "ok"
            if getattr(blaster, "truncated", 0) > 0:
                sample_quality = "low"

        return {
            "rate_mbps": float(rate_mbps),
            "pps": float(rate_pps),
            "pps_actual": round(pps_actual, 1),
            "sent_mbps": round(sent_mbps, 3),
            "throughput_mbps": round(throughput_mbps, 3),
            "goodput_mbps": round(goodput_mbps, 3),
            "wire_throughput_mbps_est": round(wire_throughput_mbps_est, 3),
            "goodput_ratio": round(goodput_ratio, 3),
            "loss_pct": round(loss_pct, 3),
            "loss_pct_wilson_low": round(loss_low * 100.0, 3),
            "loss_pct_wilson_high": round(loss_high * 100.0, 3),
            "delivered_ratio": round(delivered_ratio, 3) if sent_mbps > 0 else 0.0,
            "avg_rtt_ms": round(avg_rtt_ms, 3),
            "min_rtt_ms": round(min_rtt_ms, 3),
            "max_rtt_ms": round(max_rtt_ms, 3),
            "rtt_p50_ms": round(blaster.rtt_p50_ns / 1_000_000, 3),
            "rtt_p95_ms": round(blaster.rtt_p95_ns / 1_000_000, 3),
            "owd_p50_ms": round(blaster.owd_p50_ns / 1_000_000, 3),
            "owd_p95_ms": round(blaster.owd_p95_ns / 1_000_000, 3),
            "rtt_samples": blaster.rtt_samples,
            "owd_samples": blaster.owd_samples,
            "sample_every": effective_sample_every,
            "min_delay_samples": effective_min_delay,
            "sample_quality": sample_quality,
            "app_packet_bytes": app_packet_bytes,
            "wire_packet_bytes_est": wire_packet_bytes_est,
        }

    def export_excel(self, session_id: str, output_base: Path) -> Optional[Path]:
        if Workbook is None:
            print("[WARN] openpyxl not available; skipping Excel export")
            return None
        output_base.mkdir(parents=True, exist_ok=True)
        path = output_base / f"saturation_{self.suite}_{session_id}.xlsx"
        wb = Workbook()
        ws = wb.active
        ws.title = "Saturation"
        ws.append([
            "rate_mbps",
            "pps",
            "pps_actual",
            "sent_mbps",
            "throughput_mbps",
            "goodput_mbps",
            "wire_throughput_mbps_est",
            "goodput_ratio",
            "loss_pct",
            "loss_pct_wilson_low",
            "loss_pct_wilson_high",
            "delivered_ratio",
            "avg_rtt_ms",
            "min_rtt_ms",
            "max_rtt_ms",
            "rtt_p50_ms",
            "rtt_p95_ms",
            "owd_p50_ms",
            "owd_p95_ms",
            "rtt_samples",
            "owd_samples",
            "sample_quality",
            "app_packet_bytes",
            "wire_packet_bytes_est",
        ])
        for record in self.records:
            ws.append([
                record.get("rate_mbps", 0.0),
                record.get("pps", 0.0),
                record.get("pps_actual", 0.0),
                record.get("sent_mbps", 0.0),
                record.get("throughput_mbps", 0.0),
                record.get("goodput_mbps", 0.0),
                record.get("wire_throughput_mbps_est", 0.0),
                record.get("goodput_ratio", 0.0),
                record.get("loss_pct", 0.0),
                record.get("loss_pct_wilson_low", 0.0),
                record.get("loss_pct_wilson_high", 0.0),
                record.get("delivered_ratio", 0.0),
                record.get("avg_rtt_ms", 0.0),
                record.get("min_rtt_ms", 0.0),
                record.get("max_rtt_ms", 0.0),
                record.get("rtt_p50_ms", 0.0),
                record.get("rtt_p95_ms", 0.0),
                record.get("owd_p50_ms", 0.0),
                record.get("owd_p95_ms", 0.0),
                record.get("rtt_samples", 0),
                record.get("owd_samples", 0),
                record.get("sample_quality", "low"),
                record.get("app_packet_bytes", 0),
                record.get("wire_packet_bytes_est", 0),
            ])
        for attempt in range(3):
            try:
                buffer = io.BytesIO()
                wb.save(buffer)
                _atomic_write_bytes(path, buffer.getvalue())
                return path
            except OSError as exc:  # pragma: no cover - platform specific
                if attempt == 2:
                    print(f"[WARN] failed to save {path}: {exc}", file=sys.stderr)
            except Exception as exc:  # pragma: no cover - platform specific
                if attempt == 2:
                    print(f"[WARN] failed to write saturation workbook {path}: {exc}", file=sys.stderr)
            time.sleep(0.1)
        return None


class TelemetryCollector:
    def __init__(self, host: str, port: int) -> None:
        self.host = host
        self.port = port
        self.stop_event = threading.Event()
        self.server: Optional[socket.socket] = None
        self.accept_thread: Optional[threading.Thread] = None
        self.client_threads: List[threading.Thread] = []
        # Bug #9 fix: Use deque with maxlen to prevent unbounded memory growth
        env_maxlen = os.getenv("GCS_TELEM_MAXLEN")
        maxlen = TELEMETRY_BUFFER_MAXLEN_DEFAULT
        if env_maxlen:
            try:
                candidate = int(env_maxlen)
                if candidate <= 0:
                    raise ValueError
                if candidate < 1000:
                    candidate = 1000
                if candidate > 1_000_000:
                    print(
                        f"[WARN] GCS_TELEM_MAXLEN={candidate} capped at 1000000", file=sys.stderr
                    )
                maxlen = min(candidate, 1_000_000)
            except ValueError:
                print(
                    f"[WARN] invalid GCS_TELEM_MAXLEN={env_maxlen!r}; using default {TELEMETRY_BUFFER_MAXLEN_DEFAULT}",
                    file=sys.stderr,
                )
                maxlen = TELEMETRY_BUFFER_MAXLEN_DEFAULT
        self.samples: deque = deque(maxlen=maxlen)  # ~10MB limit for long tests
        self.lock = threading.Lock()
        self.enabled = True

    def start(self) -> None:
        try:
            addrinfo = socket.getaddrinfo(
                self.host,
                self.port,
                0,
                socket.SOCK_STREAM,
                proto=0,
                flags=socket.AI_PASSIVE if not self.host else 0,
            )
        except socket.gaierror as exc:
            print(f"[WARN] telemetry collector disabled: {exc}", file=sys.stderr)
            self.enabled = False
            return

        last_exc: Optional[Exception] = None
        for family, socktype, proto, _canon, sockaddr in addrinfo:
            try:
                srv = socket.socket(family, socktype, proto)
                try:
                    srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    if family == socket.AF_INET6:
                        srv.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)
                    srv.bind(sockaddr)
                    srv.listen(8)
                    srv.settimeout(0.5)
                except Exception:
                    srv.close()
                    raise
            except Exception as exc:
                last_exc = exc
                continue

            self.server = srv
            self.accept_thread = threading.Thread(target=self._accept_loop, daemon=True)
            self.accept_thread.start()
            print(f"[{ts()}] telemetry collector listening on {self.host}:{self.port}")
            return

        self.enabled = False
        message = last_exc or RuntimeError("no suitable address family")
        print(f"[WARN] telemetry collector disabled: {message}", file=sys.stderr)
        if self.server:
            try:
                self.server.close()
            except Exception:
                pass
            self.server = None

    def _accept_loop(self) -> None:
        assert self.server is not None
        while not self.stop_event.is_set():
            try:
                conn, addr = self.server.accept()
            except socket.timeout:
                continue
            except OSError:
                break
            except Exception as exc:
                if not self.stop_event.is_set():
                    print(f"[WARN] telemetry accept error: {exc}", file=sys.stderr)
                continue
            thread = threading.Thread(target=self._client_loop, args=(conn, addr), daemon=True)
            thread.start()
            self.client_threads.append(thread)

    def _client_loop(self, conn: socket.socket, addr) -> None:
        peer = f"{addr[0]}:{addr[1]}"
        try:
            conn.settimeout(1.0)
            with conn, conn.makefile("r", encoding="utf-8") as reader:
                for line in reader:
                    if self.stop_event.is_set():
                        break
                    data = line.strip()
                    if not data:
                        continue
                    try:
                        payload = json.loads(data)
                    except json.JSONDecodeError:
                        continue
                    payload.setdefault("collector_ts_ns", time.time_ns())
                    payload.setdefault("source", "drone")
                    payload.setdefault("peer", peer)
                    with self.lock:
                        self.samples.append(payload)
        except Exception:
            # drop connection silently
            pass

    def snapshot(self) -> List[dict]:
        with self.lock:
            # Convert deque to list for compatibility
            return list(self.samples)

    def stop(self) -> None:
        self.stop_event.set()
        if self.server:
            try:
                self.server.close()
            except Exception:
                pass
        if self.accept_thread and self.accept_thread.is_alive():
            self.accept_thread.join(timeout=1.5)
        for thread in self.client_threads:
            if thread.is_alive():
                thread.join(timeout=1.0)

def resolve_under_root(path: Path) -> Path:
    expanded = path.expanduser()
    return expanded if expanded.is_absolute() else ROOT / expanded


def safe_sheet_name(name: str) -> str:
    sanitized = "".join("_" if ch in '[]:*?/\\' else ch for ch in name).strip()
    if not sanitized:
        sanitized = "Sheet"
    return sanitized[:31]


def unique_sheet_name(workbook, base_name: str) -> str:
    base = safe_sheet_name(base_name)
    if base not in workbook.sheetnames:
        return base
    index = 1
    while True:
        suffix = f"_{index}"
        name = base[: 31 - len(suffix)] + suffix
        if name not in workbook.sheetnames:
            return name
        index += 1


def append_dict_sheet(workbook, title: str, rows: List[dict]) -> None:
    if not rows:
        return
    sheet_name = unique_sheet_name(workbook, title)
    ws = workbook.create_sheet(sheet_name)
    headers: List[str] = []
    for row in rows:
        for key in row.keys():
            if key not in headers:
                headers.append(key)
    ws.append(headers)
    for row in rows:
        ws.append([row.get(header, "") for header in headers])


def append_csv_sheet(workbook, path: Path, title: str) -> None:
    if not path.exists():
        return
    rows = None
    for attempt in range(3):
        try:
            with open(path, newline="", encoding="utf-8") as handle:
                reader = csv.reader(handle)
                rows = list(reader)
            break
        except OSError as exc:
            if attempt == 2:
                print(f"[WARN] failed to read CSV {path}: {exc}", file=sys.stderr)
            time.sleep(0.1)
        except Exception as exc:
            print(f"[WARN] failed to parse CSV {path}: {exc}", file=sys.stderr)
            return
    if not rows:
        return
    sheet_name = unique_sheet_name(workbook, title)
    ws = workbook.create_sheet(sheet_name)
    for row in rows:
        ws.append(row)


def locate_drone_session_dir(session_id: str) -> Optional[Path]:
    candidates = []
    try:
        candidates.append(resolve_under_root(DRONE_MONITOR_BASE) / session_id)
    except Exception:
        pass
    fallback = Path("/home/dev/research/output/drone") / session_id
    candidates.append(fallback)
    repo_default = ROOT / "output" / "drone" / session_id
    candidates.append(repo_default)
    seen = set()
    for candidate in candidates:
        if candidate in seen:
            continue
        seen.add(candidate)
        try:
            if candidate.exists():
                return candidate
        except Exception:
            continue
    return None


def export_combined_excel(
    session_id: str,
    summary_rows: List[dict],
    saturation_overview: List[dict],
    saturation_samples: List[dict],
    telemetry_samples: List[dict],
    drone_session_dir: Optional[Path] = None,
    *,
    traffic_mode: str,
    payload_bytes: int,
    event_sample: int,
    min_delay_samples: int,
    pre_gap_s: float,
    duration_s: float,
    inter_gap_s: float,
    sat_search: str,
    sat_delivery_threshold: float,
    sat_loss_threshold_pct: float,
    sat_rtt_spike_factor: float,
) -> Optional[Path]:
    if Workbook is None:
        print("[WARN] openpyxl not available; skipping combined Excel export", file=sys.stderr)
        return None

    workbook = Workbook()
    info_sheet = workbook.active
    info_sheet.title = "run_info"
    info_sheet.append(["generated_utc", ts()])
    info_sheet.append(["session_id", session_id])

    append_dict_sheet(workbook, "gcs_summary", summary_rows)
    append_dict_sheet(workbook, "saturation_overview", saturation_overview)
    append_dict_sheet(workbook, "saturation_samples", saturation_samples)
    append_dict_sheet(workbook, "telemetry_samples", telemetry_samples)

    def _summarize_kinematics(samples: List[dict]) -> List[dict]:
        aggregates: dict[str, dict[str, float]] = {}
        for sample in samples:
            kind = str(sample.get("kind") or "").lower()
            if kind != "kinematics":
                continue
            suite = str(sample.get("suite") or "unknown").strip() or "unknown"
            bucket = aggregates.setdefault(
                suite,
                {
                    "count": 0.0,
                    "pfc_sum": 0.0,
                    "pfc_max": 0.0,
                    "speed_sum": 0.0,
                    "speed_max": 0.0,
                    "altitude_min": float("inf"),
                    "altitude_max": float("-inf"),
                },
            )

            pfc = _as_float(sample.get("predicted_flight_constraint_w"))
            speed = _as_float(sample.get("speed_mps"))
            altitude = _as_float(sample.get("altitude_m"))

            bucket["count"] += 1.0
            if pfc is not None:
                bucket["pfc_sum"] += pfc
                bucket["pfc_max"] = max(bucket["pfc_max"], pfc)
            if speed is not None:
                bucket["speed_sum"] += speed
                bucket["speed_max"] = max(bucket["speed_max"], speed)
            if altitude is not None:
                bucket["altitude_min"] = min(bucket["altitude_min"], altitude)
                bucket["altitude_max"] = max(bucket["altitude_max"], altitude)

        summary_rows: List[dict] = []
        for suite, data in sorted(aggregates.items()):
            count = max(1.0, data["count"])
            altitude_min = "" if math.isinf(data["altitude_min"]) else data["altitude_min"]
            altitude_max = "" if math.isinf(data["altitude_max"]) else data["altitude_max"]
            summary_rows.append(
                {
                    "suite": suite,
                    "samples": int(data["count"]),
                    "pfc_avg_w": _rounded(data["pfc_sum"] / count, 3),
                    "pfc_max_w": _rounded(data["pfc_max"], 3),
                    "speed_avg_mps": _rounded(data["speed_sum"] / count, 3),
                    "speed_max_mps": _rounded(data["speed_max"], 3),
                    "altitude_min_m": _rounded(altitude_min, 3) if altitude_min != "" else "",
                    "altitude_max_m": _rounded(altitude_max, 3) if altitude_max != "" else "",
                }
            )
        return summary_rows

    kinematics_summary = _summarize_kinematics(telemetry_samples)
    append_dict_sheet(workbook, "kinematics_summary", kinematics_summary)

    paper_header = [
        "suite",
        "rekey_ms",
        "blackout_ms",
        "gap_p99_ms",
        "goodput_mbps",
        "loss_pct",
        "rtt_p50_ms",
        "rtt_p95_ms",
        "owd_p50_ms",
        "owd_p95_ms",
        "power_avg_w",
        "power_energy_j",
    ]
    paper_sheet = workbook.create_sheet("paper_tables")
    paper_sheet.append(paper_header)
    ordered_rows: "OrderedDict[str, dict]" = OrderedDict()
    for row in summary_rows:
        suite_name = str(row.get("suite") or "").strip()
        if not suite_name:
            continue
        ordered_rows[suite_name] = row
    paper_rows = list(ordered_rows.items())
    for suite_name, source_row in paper_rows:
        paper_sheet.append([
            suite_name,
            _rounded(source_row.get("rekey_ms"), 3),
            _rounded(source_row.get("blackout_ms"), 3),
            _rounded(source_row.get("gap_p99_ms"), 3),
            _rounded(source_row.get("goodput_mbps"), 3),
            _rounded(source_row.get("loss_pct"), 3),
            _rounded(source_row.get("rtt_p50_ms"), 3),
            _rounded(source_row.get("rtt_p95_ms"), 3),
            _rounded(source_row.get("owd_p50_ms"), 3),
            _rounded(source_row.get("owd_p95_ms"), 3),
            _rounded(source_row.get("power_avg_w"), 6),
            _rounded(source_row.get("power_energy_j"), 6),
        ])

    notes_header = [
        "generated_utc",
        "session_id",
        "traffic_mode",
        "payload_bytes",
        "event_sample",
        "min_delay_samples",
        "pre_gap_s",
        "duration_s",
        "inter_gap_s",
        "sat_search",
        "sat_delivery_threshold",
        "sat_loss_threshold_pct",
        "sat_rtt_spike_factor",
    ]
    notes_sheet = workbook.create_sheet("paper_notes")
    notes_sheet.append(notes_header)
    notes_sheet.append([
        ts(),
        session_id,
        traffic_mode,
        payload_bytes,
        event_sample,
        min_delay_samples,
        round(pre_gap_s, 3),
        round(duration_s, 3),
        round(inter_gap_s, 3),
        sat_search,
        sat_delivery_threshold,
        sat_loss_threshold_pct,
        sat_rtt_spike_factor,
    ])

    if SUMMARY_CSV.exists():
        append_csv_sheet(workbook, SUMMARY_CSV, "gcs_summary_csv")

    if drone_session_dir is None:
        drone_session_dir = locate_drone_session_dir(session_id)
    if drone_session_dir:
        info_sheet.append(["drone_session_dir", str(drone_session_dir)])
        for csv_path in sorted(drone_session_dir.glob("*.csv")):
            append_csv_sheet(workbook, csv_path, csv_path.stem[:31])
    else:
        info_sheet.append(["drone_session_dir", "not_found"])

    if paper_rows and BarChart is not None and Reference is not None:
        row_count = len(paper_rows) + 1
        suite_categories = Reference(paper_sheet, min_col=1, min_row=2, max_row=row_count)

        rekey_chart = BarChart()
        rekey_chart.title = "Rekey vs Blackout (ms)"
        rekey_chart.add_data(
            Reference(paper_sheet, min_col=2, max_col=3, min_row=1, max_row=row_count),
            titles_from_data=True,
        )
        rekey_chart.set_categories(suite_categories)
        rekey_chart.y_axis.title = "Milliseconds"
        rekey_chart.x_axis.title = "Suite"
        paper_sheet.add_chart(rekey_chart, "H2")

        power_chart = BarChart()
        power_chart.title = "Avg Power (W)"
        power_chart.add_data(
            Reference(paper_sheet, min_col=11, max_col=11, min_row=1, max_row=row_count),
            titles_from_data=True,
        )
        power_chart.set_categories(suite_categories)
        power_chart.y_axis.title = "Watts"
        power_chart.x_axis.title = "Suite"
        paper_sheet.add_chart(power_chart, "H18")

    if summary_rows and LineChart is not None and Reference is not None and "gcs_summary" in workbook.sheetnames:
        summary_sheet = workbook["gcs_summary"]
        header_row = next(summary_sheet.iter_rows(min_row=1, max_row=1, values_only=True), None)
        if header_row:
            try:
                pass_col = header_row.index("pass") + 1
                throughput_col = header_row.index("throughput_mbps") + 1
            except ValueError:
                pass_col = throughput_col = None
            if pass_col and throughput_col and len(summary_rows) >= 1:
                chart = LineChart()
                chart.title = "Throughput (Mb/s) vs pass index"
                chart.add_data(
                    Reference(
                        summary_sheet,
                        min_col=throughput_col,
                        min_row=1,
                        max_row=len(summary_rows) + 1,
                    ),
                    titles_from_data=True,
                )
                chart.set_categories(
                    Reference(
                        summary_sheet,
                        min_col=pass_col,
                        min_row=2,
                        max_row=len(summary_rows) + 1,
                    )
                )
                chart.x_axis.title = "Pass"
                chart.y_axis.title = "Throughput (Mb/s)"
                summary_sheet.add_chart(chart, "L2")

    combined_root = resolve_under_root(COMBINED_OUTPUT_DIR)
    combined_dir = combined_root / session_id
    combined_dir.mkdir(parents=True, exist_ok=True)
    info_sheet.append(["gcs_session_dir", str(combined_dir)])
    target_path = combined_dir / f"{session_id}_combined.xlsx"
    for attempt in range(3):
        try:
            buffer = io.BytesIO()
            workbook.save(buffer)
            _atomic_write_bytes(target_path, buffer.getvalue())
            return target_path
        except Exception as exc:  # pragma: no cover - platform specific
            if attempt == 2:
                print(f"[WARN] failed to write combined workbook {target_path}: {exc}", file=sys.stderr)
            time.sleep(0.1)
    return None


def main() -> None:
    log_runtime_environment("gcs_scheduler")
    OUTDIR.mkdir(parents=True, exist_ok=True)
    SUITES_OUTDIR.mkdir(parents=True, exist_ok=True)
    PROXY_STATUS_PATH.parent.mkdir(parents=True, exist_ok=True)
    PROXY_SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)

    auto = AUTO_GCS_CONFIG

    traffic_mode = str(auto.get("traffic") or "blast").lower()
    pre_gap = float(auto.get("pre_gap_s") or 1.0)
    inter_gap = float(auto.get("inter_gap_s") or 15.0)
    duration = float(auto.get("duration_s") or 15.0)
    payload_bytes = int(auto.get("payload_bytes") or 256)
    configured_event_sample = int(auto.get("event_sample") or 100)
    event_sample = max(0, configured_event_sample)
    passes = int(auto.get("passes") or 1)
    rate_pps = int(auto.get("rate_pps") or 0)
    bandwidth_mbps = float(auto.get("bandwidth_mbps") or 0.0)
    constant_rate_defaulted = False
    max_rate_mbps = float(auto.get("max_rate_mbps") or 200.0)
    if traffic_mode == "constant" and bandwidth_mbps <= 0 and rate_pps <= 0:
        bandwidth_mbps = CONSTANT_RATE_MBPS_DEFAULT
        constant_rate_defaulted = True
    if bandwidth_mbps > 0:
        denominator = max(payload_bytes * 8, 1)
        rate_pps = max(1, int((bandwidth_mbps * 1_000_000) / denominator))
    if traffic_mode == "constant" and rate_pps <= 0:
        raise ValueError("AUTO_GCS.rate_pps or bandwidth_mbps must be positive for constant traffic")

    sat_search_cfg = str(auto.get("sat_search") or SATURATION_SEARCH_MODE).lower()
    if sat_search_cfg not in {"auto", "linear", "bisect"}:
        sat_search_cfg = SATURATION_SEARCH_MODE
    sat_delivery_threshold = float(auto.get("sat_delivery_threshold") or SATURATION_DELIVERY_THRESHOLD)
    sat_loss_threshold = float(auto.get("sat_loss_threshold_pct") or SATURATION_LOSS_THRESHOLD)
    sat_spike_factor = float(auto.get("sat_rtt_spike_factor") or SATURATION_RTT_SPIKE)

    min_delay_samples = MIN_DELAY_SAMPLES

    if duration <= 0:
        raise ValueError("AUTO_GCS.duration_s must be positive")
    if pre_gap < 0:
        raise ValueError("AUTO_GCS.pre_gap_s must be >= 0")
    if inter_gap < 0:
        raise ValueError("AUTO_GCS.inter_gap_s must be >= 0")
    if rate_pps < 0:
        raise ValueError("AUTO_GCS.rate_pps must be >= 0")
    if passes <= 0:
        raise ValueError("AUTO_GCS.passes must be >= 1")

    if traffic_mode not in {"blast", "constant", "mavproxy", "saturation"}:
        raise ValueError(f"Unsupported traffic mode: {traffic_mode}")

    constant_target_bandwidth_mbps = 0.0
    if traffic_mode == "constant":
        if bandwidth_mbps > 0:
            constant_target_bandwidth_mbps = bandwidth_mbps
        elif rate_pps > 0:
            constant_target_bandwidth_mbps = (rate_pps * payload_bytes * 8) / 1_000_000
    run_target_bandwidth_mbps = (
        constant_target_bandwidth_mbps if traffic_mode == "constant" else max(0.0, bandwidth_mbps)
    )

    suites_override = auto.get("suites")
    suites = resolve_suites(suites_override)
    if not suites:
        raise RuntimeError("No suites selected for execution")

    session_prefix = str(auto.get("session_prefix") or "session")
    env_session_id = os.environ.get("GCS_SESSION_ID")
    session_id = env_session_id or f"{session_prefix}_{int(time.time())}"
    session_source = "env" if env_session_id else "generated"

    initial_suite = preferred_initial_suite(suites)
    if initial_suite and suites[0] != initial_suite:
        suites = [initial_suite] + [s for s in suites if s != initial_suite]
        print(f"[{ts()}] reordered suites to start with {initial_suite} (from CONFIG)")

    power_capture_enabled = bool(auto.get("power_capture", True))

    telemetry_enabled = bool(auto.get("telemetry_enabled", True))
    telemetry_bind_host = auto.get("telemetry_bind_host") or TELEMETRY_BIND_HOST
    telemetry_port_cfg = auto.get("telemetry_port")
    telemetry_port = TELEMETRY_PORT if telemetry_port_cfg in (None, "") else int(telemetry_port_cfg)

    print(
        f"[{ts()}] traffic={traffic_mode} duration={duration:.1f}s pre_gap={pre_gap:.1f}s "
        f"inter_gap={inter_gap:.1f}s payload={payload_bytes}B event_sample={event_sample} passes={passes} "
        f"rate_pps={rate_pps} sat_search={sat_search_cfg}"
    )
    if traffic_mode == "constant":
        target_msg = f"[{ts()}] constant-rate target {constant_target_bandwidth_mbps:.2f} Mbps (~{rate_pps} pps)"
        if constant_rate_defaulted:
            target_msg += " [default]"
        print(target_msg)
    elif bandwidth_mbps > 0:
        print(f"[{ts()}] bandwidth target {bandwidth_mbps:.2f} Mbps -> approx {rate_pps} pps")
    print(f"[{ts()}] power capture: {'enabled' if power_capture_enabled else 'disabled'}")

    reachable = False
    for attempt in range(8):
        try:
            resp = ctl_send({"cmd": "ping"}, timeout=1.0, retries=1)
            if resp.get("ok"):
                reachable = True
                break
        except Exception:
            pass
        time.sleep(0.5)
    follower_session_id: Optional[str] = None
    if reachable:
        print(f"[{ts()}] follower reachable at {DRONE_HOST}:{CONTROL_PORT}")
        try:
            session_resp = ctl_send({"cmd": "session_info"}, timeout=1.2, retries=2, backoff=0.3)
            if session_resp.get("ok"):
                candidate = str(session_resp.get("session_id") or "").strip()
                if candidate:
                    follower_session_id = candidate
        except Exception as exc:
            print(f"[WARN] session_info fetch failed: {exc}", file=sys.stderr)
    else:
        print(f"[WARN] follower not reachable at {DRONE_HOST}:{CONTROL_PORT}", file=sys.stderr)

    if follower_session_id:
        if env_session_id and follower_session_id != env_session_id:
            print(
                f"[WARN] follower session_id={follower_session_id} disagrees with GCS_SESSION_ID={env_session_id}; using env override",
                file=sys.stderr,
            )
        else:
            session_id = follower_session_id
            session_source = "drone"

    print(f"[{ts()}] session_id={session_id} (source={session_source})")
    os.environ["GCS_SESSION_ID"] = session_id

    drone_session_dir = locate_drone_session_dir(session_id)
    if drone_session_dir:
        print(f"[{ts()}] follower session dir -> {drone_session_dir}")
    else:
        print(f"[WARN] follower session dir missing for session {session_id}", file=sys.stderr)

    session_excel_dir = resolve_under_root(EXCEL_OUTPUT_DIR) / session_id

    offset_ns = 0
    offset_warmup_s = 0.0
    try:
        sync = timesync()
        offset_ns = sync["offset_ns"]
        print(f"[{ts()}] clocks synced: offset_ns={offset_ns} ns, link_rtt~{sync['rtt_ns']} ns")
        if abs(offset_ns) > CLOCK_OFFSET_THRESHOLD_NS:
            offset_warmup_s = 1.0
            print(
                f"[WARN] clock offset {offset_ns / 1_000_000:.1f} ms exceeds {CLOCK_OFFSET_THRESHOLD_NS / 1_000_000:.1f} ms; extending warmup",
                file=sys.stderr,
            )
            print(
                f"[{ts()}] clock skew banner: |offset|={offset_ns / 1_000_000:.1f} ms -> first measurement pass may be noisy",
                flush=True,
            )
    except Exception as exc:
        print(f"[WARN] timesync failed: {exc}", file=sys.stderr)

    telemetry_collector: Optional[TelemetryCollector] = None
    if telemetry_enabled:
        telemetry_collector = TelemetryCollector(telemetry_bind_host, telemetry_port)
        telemetry_collector.start()
        print(f"[{ts()}] telemetry collector -> {telemetry_bind_host}:{telemetry_port}")
    else:
        print(f"[{ts()}] telemetry collector disabled via AUTO_GCS configuration")

    if not bool(auto.get("launch_proxy", True)):
        raise NotImplementedError("AUTO_GCS.launch_proxy=False is not supported")

    gcs_proc: Optional[subprocess.Popen] = None
    log_handle = None
    gcs_proc, log_handle = start_gcs_proxy(suites[0])

    try:
        ready = wait_handshake(timeout=20.0)
        print(f"[{ts()}] initial handshake ready? {ready}")

        summary_rows: List[dict] = []
        saturation_reports: List[dict] = []
        all_rate_samples: List[dict] = []
        telemetry_samples: List[dict] = []

        if traffic_mode == "saturation":
            for idx, suite in enumerate(suites):
                rekey_ms, rekey_mark_ns, rekey_ok_ns = activate_suite(gcs_proc, suite, is_first=(idx == 0))
                outdir = suite_outdir(suite)
                tester = SaturationTester(
                    suite=suite,
                    payload_bytes=payload_bytes,
                    duration_s=duration,
                    event_sample=event_sample,
                    offset_ns=offset_ns,
                    output_dir=outdir,
                    max_rate_mbps=int(max_rate_mbps),
                    search_mode=sat_search_cfg,
                    delivery_threshold=sat_delivery_threshold,
                    loss_threshold=sat_loss_threshold,
                    spike_factor=sat_spike_factor,
                    min_delay_samples=min_delay_samples,
                )
                summary = tester.run()
                summary["rekey_ms"] = rekey_ms
                if rekey_mark_ns is not None:
                    summary["rekey_mark_ns"] = rekey_mark_ns
                if rekey_ok_ns is not None:
                    summary["rekey_ok_ns"] = rekey_ok_ns
                excel_path = tester.export_excel(session_id, session_excel_dir)
                if excel_path:
                    summary["excel_path"] = str(excel_path)
                saturation_reports.append(summary)
                all_rate_samples.extend(dict(record) for record in tester.records)
                if inter_gap > 0 and idx < len(suites) - 1:
                    time.sleep(inter_gap)
            report_path = OUTDIR / f"saturation_summary_{session_id}.json"
            summary_bytes = json.dumps(saturation_reports, indent=2).encode("utf-8")
            try:
                _atomic_write_bytes(report_path, summary_bytes)
                print(f"[{ts()}] saturation summary written to {report_path}")
            except Exception as exc:
                print(f"[WARN] failed to update {report_path}: {exc}", file=sys.stderr)
        else:
            for pass_index in range(passes):
                for idx, suite in enumerate(suites):
                    row = run_suite(
                        gcs_proc,
                        suite,
                        is_first=(pass_index == 0 and idx == 0),
                        duration_s=duration,
                        payload_bytes=payload_bytes,
                        event_sample=event_sample,
                        offset_ns=offset_ns,
                        pass_index=pass_index,
                        traffic_mode=traffic_mode,
                        pre_gap=pre_gap,
                        inter_gap_s=inter_gap,
                        rate_pps=rate_pps,
                        target_bandwidth_mbps=run_target_bandwidth_mbps,
                        power_capture_enabled=power_capture_enabled,
                        clock_offset_warmup_s=offset_warmup_s,
                        min_delay_samples=min_delay_samples,
                        telemetry_collector=telemetry_collector,
                    )
                    summary_rows.append(row)
                    is_last_suite = idx == len(suites) - 1
                    is_last_pass = pass_index == passes - 1
                    if inter_gap > 0 and not (is_last_suite and is_last_pass):
                        time.sleep(inter_gap)

            if summary_rows:
                blackout_records, step_payloads = _enrich_summary_rows(
                    summary_rows,
                    session_id=session_id,
                    drone_session_dir=drone_session_dir,
                    traffic_mode=traffic_mode,
                    pre_gap_s=pre_gap,
                    duration_s=duration,
                    inter_gap_s=inter_gap,
                )
                _append_blackout_records(blackout_records)
                _append_step_results(step_payloads)

            write_summary(summary_rows)

        if telemetry_collector and telemetry_collector.enabled:
            telemetry_samples = telemetry_collector.snapshot()

        if auto.get("export_combined_excel", True):
            combined_path = export_combined_excel(
                session_id=session_id,
                summary_rows=summary_rows,
                saturation_overview=saturation_reports,
                saturation_samples=all_rate_samples,
                telemetry_samples=telemetry_samples,
                drone_session_dir=drone_session_dir,
                traffic_mode=traffic_mode,
                payload_bytes=payload_bytes,
                event_sample=event_sample,
                min_delay_samples=min_delay_samples,
                pre_gap_s=pre_gap,
                duration_s=duration,
                inter_gap_s=inter_gap,
                sat_search=sat_search_cfg,
                sat_delivery_threshold=sat_delivery_threshold,
                sat_loss_threshold_pct=sat_loss_threshold,
                sat_rtt_spike_factor=sat_spike_factor,
            )
            if combined_path:
                print(f"[{ts()}] combined workbook written to {combined_path}")

    finally:
        try:
            ctl_send({"cmd": "stop"})
        except Exception:
            pass

        if gcs_proc and gcs_proc.stdin:
            try:
                gcs_proc.stdin.write("quit\n")
                gcs_proc.stdin.flush()
            except Exception:
                pass
        if gcs_proc:
            try:
                gcs_proc.wait(timeout=5)
            except Exception:
                gcs_proc.kill()

        if log_handle:
            try:
                log_handle.close()
            except Exception:
                pass

        if telemetry_collector:
            telemetry_collector.stop()


if __name__ == "__main__":
    # Test plan:
    # 1. Launch the scheduler with the follower running; verify telemetry collector binds and follower connects.
    # 2. Exercise multiple suites to confirm rekey waits for follower confirmation and no failed rekeys occur.
    # 3. Delete output directories before a run to ensure the scheduler recreates all paths automatically.
    # 4. Stop the telemetry collector briefly and confirm the follower reconnects without aborting the run.
    main()

================================================================================
END OF LOG
================================================================================
