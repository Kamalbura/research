PROJECT STRUCTURE AND PYTHON FILES LOG
================================================================================
Root Directory: C:\Users\burak\Desktop\research\core
Output File: C:\Users\burak\Desktop\research\core\project_structure_20251005_164713.txt
Generated: 2025-10-05 16:47:13
================================================================================

================================================================================
DIRECTORY TREE STRUCTURE
================================================================================
Root Directory: C:\Users\burak\Desktop\research\core
Generated: 2025-10-05 16:47:13

├── __pycache__/
│   ├── __init__.cpython-311.pyc (290 bytes)
│   ├── __init__.cpython-313.pyc (273 bytes)
│   ├── aead.cpython-311.pyc (14,355 bytes)
│   ├── aead.cpython-313.pyc (14,160 bytes)
│   ├── async_proxy.cpython-311.pyc (38,513 bytes)
│   ├── async_proxy.cpython-313.pyc (44,419 bytes)
│   ├── config.cpython-311.pyc (5,682 bytes)
│   ├── config.cpython-313.pyc (9,303 bytes)
│   ├── handshake.cpython-311.pyc (13,111 bytes)
│   ├── handshake.cpython-313.pyc (15,540 bytes)
│   ├── logging_utils.cpython-313.pyc (5,872 bytes)
│   ├── policy_engine.cpython-311.pyc (11,201 bytes)
│   ├── policy_engine.cpython-313.pyc (9,972 bytes)
│   ├── power_monitor.cpython-313.pyc (40,067 bytes)
│   ├── project_config.cpython-313.pyc (188 bytes)
│   ├── run_proxy.cpython-311.pyc (25,109 bytes)
│   ├── run_proxy.cpython-313.pyc (23,539 bytes)
│   ├── suites.cpython-311.pyc (13,950 bytes)
│   ├── suites.cpython-313.pyc (11,653 bytes)
│   └── temp-file.cpython-313.pyc (20,538 bytes)
├── __init__.py (121 bytes)
├── aead.py (11,344 bytes)
├── async_proxy.py (42,288 bytes)
├── config.py (13,291 bytes)
├── handshake.py (12,857 bytes)
├── logging_utils.py (2,957 bytes)
├── policy_engine.py (7,034 bytes)
├── power_monitor.py (34,456 bytes)
├── project_config.py (168 bytes)
├── project_structure_20251005_164713.txt (0 bytes)
├── run_proxy.py (20,487 bytes)
├── suites.py (11,618 bytes)
└── temp-file.py (18,859 bytes)


================================================================================
PYTHON FILE CONTENTS
================================================================================

Found 12 Python files:
   1. __init__.py
   2. aead.py
   3. async_proxy.py
   4. config.py
   5. handshake.py
   6. logging_utils.py
   7. policy_engine.py
   8. power_monitor.py
   9. project_config.py
  10. run_proxy.py
  11. suites.py
  12. temp-file.py

--------------------------------------------------------------------------------

FILE 1/12: __init__.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\__init__.py
Size: 121 bytes
Modified: 2025-09-24 05:23:26
------------------------------------------------------------
"""
PQC Drone-GCS Secure Proxy Core Package.

Provides post-quantum cryptography secure communication components.
"""

============================================================

FILE 2/12: aead.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\aead.py
Size: 11,344 bytes
Modified: 2025-10-03 03:17:13
------------------------------------------------------------
"""
AEAD framing for PQC drone-GCS secure proxy.

Provides authenticated encryption (AES-256-GCM) with wire header bound as AAD,
deterministic 96-bit counter IVs, sliding replay window, and epoch support for rekeys.
"""

import struct
from dataclasses import dataclass
from typing import Optional

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from cryptography.exceptions import InvalidTag

from .config import CONFIG
from .suites import header_ids_for_suite


# Exception types
class HeaderMismatch(Exception):
    """Header validation failed (version, IDs, or session_id mismatch)."""
    pass


class AeadAuthError(Exception):
    """AEAD authentication failed during decryption."""
    pass


class ReplayError(Exception):
    """Packet replay detected or outside acceptable window."""
    pass


# Constants
HEADER_STRUCT = "!BBBBB8sQB"
HEADER_LEN = 22
# IV is still logically 12 bytes (1 epoch + 11 seq bytes) but is NO LONGER transmitted on wire.
# Wire format: header(22) || ciphertext+tag
IV_LEN = 0  # length of IV bytes present on wire (0 after optimization)


@dataclass(frozen=True)
class AeadIds:
    kem_id: int
    kem_param: int
    sig_id: int
    sig_param: int

    def __post_init__(self):
        for field_name, value in [("kem_id", self.kem_id), ("kem_param", self.kem_param), 
                                  ("sig_id", self.sig_id), ("sig_param", self.sig_param)]:
            if not isinstance(value, int) or not (0 <= value <= 255):
                raise NotImplementedError(f"{field_name} must be int in range 0-255")


@dataclass
class Sender:
    version: int
    ids: AeadIds
    session_id: bytes
    epoch: int
    key_send: bytes
    _seq: int = 0

    def __post_init__(self):
        if not isinstance(self.version, int) or self.version != CONFIG["WIRE_VERSION"]:
            raise NotImplementedError(f"version must equal CONFIG WIRE_VERSION ({CONFIG['WIRE_VERSION']})")
        
        if not isinstance(self.ids, AeadIds):
            raise NotImplementedError("ids must be AeadIds instance")
        
        if not isinstance(self.session_id, bytes) or len(self.session_id) != 8:
            raise NotImplementedError("session_id must be exactly 8 bytes")
        
        if not isinstance(self.epoch, int) or not (0 <= self.epoch <= 255):
            raise NotImplementedError("epoch must be int in range 0-255")
        
        if not isinstance(self.key_send, bytes) or len(self.key_send) != 32:
            raise NotImplementedError("key_send must be exactly 32 bytes")
        
        if not isinstance(self._seq, int) or self._seq < 0:
            raise NotImplementedError("_seq must be non-negative int")
        
        self._aesgcm = AESGCM(self.key_send)

    @property
    def seq(self):
        """Current sequence number."""
        return self._seq

    def pack_header(self, seq: int) -> bytes:
        """Pack header with given sequence number."""
        if not isinstance(seq, int) or seq < 0:
            raise NotImplementedError("seq must be non-negative int")
        
        return struct.pack(
            HEADER_STRUCT,
            self.version,
            self.ids.kem_id,
            self.ids.kem_param, 
            self.ids.sig_id,
            self.ids.sig_param,
            self.session_id,
            seq,
            self.epoch
        )

    def encrypt(self, plaintext: bytes) -> bytes:
        """Encrypt plaintext returning: header || ciphertext + tag.

        Deterministic IV (epoch||seq) is derived locally and NOT sent on wire to
        reduce overhead (saves 12 bytes per packet). Receiver reconstructs it.
        """
        if not isinstance(plaintext, bytes):
            raise NotImplementedError("plaintext must be bytes")
        
        # Check for sequence overflow - header uses uint64, so check that limit
        # Bug #6 fix: Allow full uint64 range (0 to 2^64-1)
        if self._seq >= 2**64:
            raise NotImplementedError("packet_seq overflow; rekey/epoch bump required")
        
        # Pack header with current sequence
        header = self.pack_header(self._seq)
        
        # Derive deterministic IV = epoch (1 byte) || seq (11 bytes)
        iv = bytes([self.epoch & 0xFF]) + self._seq.to_bytes(11, "big")

        try:
            ciphertext = self._aesgcm.encrypt(iv, plaintext, header)
        except Exception as e:
            raise NotImplementedError(f"AEAD encryption failed: {e}")
        
        # Increment sequence on success
        self._seq += 1
        
        # Return optimized wire format: header || ciphertext+tag (IV omitted)
        return header + ciphertext

    def bump_epoch(self) -> None:
        """Increase epoch and reset sequence.

        Safety policy: forbid wrapping 255->0 with the same key to avoid IV reuse.
        Callers should perform a new handshake to rotate keys before wrap.
        """
        if self.epoch == 255:
            raise NotImplementedError("epoch wrap forbidden without rekey; perform handshake to rotate keys")
        self.epoch += 1
        self._seq = 0


@dataclass
class Receiver:
    version: int
    ids: AeadIds
    session_id: bytes
    epoch: int
    key_recv: bytes
    window: int
    strict_mode: bool = False  # True = raise exceptions, False = return None
    _high: int = -1
    _mask: int = 0

    def __post_init__(self):
        if not isinstance(self.version, int) or self.version != CONFIG["WIRE_VERSION"]:
            raise NotImplementedError(f"version must equal CONFIG WIRE_VERSION ({CONFIG['WIRE_VERSION']})")
        
        if not isinstance(self.ids, AeadIds):
            raise NotImplementedError("ids must be AeadIds instance")
        
        if not isinstance(self.session_id, bytes) or len(self.session_id) != 8:
            raise NotImplementedError("session_id must be exactly 8 bytes")
        
        if not isinstance(self.epoch, int) or not (0 <= self.epoch <= 255):
            raise NotImplementedError("epoch must be int in range 0-255")
        
        if not isinstance(self.key_recv, bytes) or len(self.key_recv) != 32:
            raise NotImplementedError("key_recv must be exactly 32 bytes")
        
        if not isinstance(self.window, int) or self.window < 64:
            raise NotImplementedError(f"window must be int >= 64")
        
        if not isinstance(self._high, int):
            raise NotImplementedError("_high must be int")
        
        if not isinstance(self._mask, int) or self._mask < 0:
            raise NotImplementedError("_mask must be non-negative int")
        
        self._aesgcm = AESGCM(self.key_recv)
        self._last_error: Optional[str] = None

    def _check_replay(self, seq: int) -> None:
        """Check if sequence number should be accepted (anti-replay)."""
        if seq > self._high:
            # Future packet - shift window forward
            shift = seq - self._high
            if shift >= self.window:
                # Window completely shifts
                self._mask = 1  # Only mark the current position
            else:
                # Partial shift
                self._mask = (self._mask << shift) | 1
                # Mask to window size to prevent overflow
                self._mask &= (1 << self.window) - 1
            self._high = seq
        elif seq > self._high - self.window:
            # Within window - check if already seen
            offset = self._high - seq
            bit_pos = offset
            if self._mask & (1 << bit_pos):
                raise ReplayError(f"duplicate packet seq={seq}")
            # Mark as seen
            self._mask |= (1 << bit_pos)
        else:
            # Too old - outside window
            raise ReplayError(f"packet too old seq={seq}, high={self._high}, window={self.window}")

    def decrypt(self, wire: bytes) -> bytes:
        """Validate header, perform anti-replay, reconstruct IV, decrypt.

        Returns plaintext bytes or None (silent mode) on failure.
        """
        if not isinstance(wire, bytes):
            raise NotImplementedError("wire must be bytes")
        
        if len(wire) < HEADER_LEN:
            raise NotImplementedError("wire too short for header")
        
        # Extract header
        header = wire[:HEADER_LEN]
        
        # Unpack and validate header
        try:
            fields = struct.unpack(HEADER_STRUCT, header)
            version, kem_id, kem_param, sig_id, sig_param, session_id, seq, epoch = fields
        except struct.error as e:
            raise NotImplementedError(f"header unpack failed: {e}")
        
        # Validate header fields
        if version != self.version:
            self._last_error = "header"
            if self.strict_mode:
                raise HeaderMismatch(f"version mismatch: expected {self.version}, got {version}")
            return None
        
        if (kem_id, kem_param, sig_id, sig_param) != (self.ids.kem_id, self.ids.kem_param, self.ids.sig_id, self.ids.sig_param):
            self._last_error = "header"
            if self.strict_mode:
                raise HeaderMismatch(f"crypto ID mismatch")
            return None
        
        if session_id != self.session_id:
            self._last_error = "session"
            return None  # Wrong session - always fail silently for security
        
        if epoch != self.epoch:
            self._last_error = "session"
            return None  # Wrong epoch - always fail silently for rekeying
        
        # Check replay protection
        try:
            self._check_replay(seq)
        except ReplayError:
            self._last_error = "replay"
            if self.strict_mode:
                raise
            return None
        
        # Reconstruct deterministic IV instead of reading from wire
        iv = bytes([epoch & 0xFF]) + seq.to_bytes(11, "big")
        ciphertext = wire[HEADER_LEN:]
        
        # Decrypt with header as AAD
        try:
            plaintext = self._aesgcm.decrypt(iv, ciphertext, header)
        except InvalidTag:
            self._last_error = "auth"
            if self.strict_mode:
                raise AeadAuthError("AEAD authentication failed")
            return None
        except Exception as e:
            raise NotImplementedError(f"AEAD decryption failed: {e}")
        self._last_error = None
        return plaintext

    def reset_replay(self) -> None:
        """Clear replay protection state."""
        self._high = -1
        self._mask = 0

    def bump_epoch(self) -> None:
        """Increase epoch and reset replay state.
        
        Safety policy: forbid wrapping 255->0 with the same key to avoid IV reuse.
        Callers should perform a new handshake to rotate keys before wrap.
        """
        if self.epoch == 255:
            raise NotImplementedError("epoch wrap forbidden without rekey; perform handshake to rotate keys")
        self.epoch += 1
        self.reset_replay()

    def last_error_reason(self) -> Optional[str]:
        return getattr(self, "_last_error", None)

============================================================

FILE 3/12: async_proxy.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\async_proxy.py
Size: 42,288 bytes
Modified: 2025-10-03 03:17:13
------------------------------------------------------------
"""
Selectors-based network transport proxy.

Responsibilities:
1. Perform authenticated TCP handshake (PQC KEM + signature) using `core.handshake`.
2. Bridge plaintext UDP <-> encrypted UDP (AEAD framing) both directions.
3. Enforce replay window and per-direction sequence via `core.aead`.

Note: This module uses the low-level `selectors` stdlib facility—not `asyncio`—to
remain dependency-light and fully deterministic for test harnesses. The filename
is retained for backward compatibility; a future refactor may rename it to
`selector_proxy.py` and/or introduce an asyncio variant.
"""

from __future__ import annotations

import hashlib
import json
import queue
import socket
import selectors
import struct
import sys
import threading
import time
from contextlib import contextmanager
from pathlib import Path
from typing import Dict, Optional, Tuple

from core.config import CONFIG
from core.suites import SUITES, get_suite, header_ids_for_suite, list_suites
try:
    # Optional helper (if you implemented it)
    from core.suites import header_ids_from_names  # type: ignore
except Exception:
    header_ids_from_names = None  # type: ignore

from core.handshake import HandshakeVerifyError, client_drone_handshake, server_gcs_handshake
from core.logging_utils import get_logger

from core.aead import (
    AeadAuthError,
    AeadIds,
    HeaderMismatch,
    Receiver,
    ReplayError,
    Sender,
)

from core.policy_engine import (
    ControlResult,
    ControlState,
    create_control_state,
    handle_control,
    record_rekey_result,
    request_prepare,
)

logger = get_logger("pqc")


class ProxyCounters:
    """Simple counters for proxy statistics."""

    def __init__(self) -> None:
        self.ptx_out = 0      # plaintext packets sent out to app
        self.ptx_in = 0       # plaintext packets received from app
        self.enc_out = 0      # encrypted packets sent to peer
        self.enc_in = 0       # encrypted packets received from peer
        self.drops = 0        # total drops
        # Granular drop reasons
        self.drop_replay = 0
        self.drop_auth = 0
        self.drop_header = 0
        self.drop_session_epoch = 0
        self.drop_other = 0
        self.drop_src_addr = 0
        self.rekeys_ok = 0
        self.rekeys_fail = 0
        self.last_rekey_ms = 0
        self.last_rekey_suite: Optional[str] = None

    def to_dict(self) -> Dict[str, object]:
        return {
            "ptx_out": self.ptx_out,
            "ptx_in": self.ptx_in,
            "enc_out": self.enc_out,
            "enc_in": self.enc_in,
            "drops": self.drops,
            "drop_replay": self.drop_replay,
            "drop_auth": self.drop_auth,
            "drop_header": self.drop_header,
            "drop_session_epoch": self.drop_session_epoch,
            "drop_other": self.drop_other,
            "drop_src_addr": self.drop_src_addr,
            "rekeys_ok": self.rekeys_ok,
            "rekeys_fail": self.rekeys_fail,
            "last_rekey_ms": self.last_rekey_ms,
            "last_rekey_suite": self.last_rekey_suite or "",
        }


def _dscp_to_tos(dscp: Optional[int]) -> Optional[int]:
    """Convert DSCP value to TOS byte for socket options."""
    if dscp is None:
        return None
    try:
        d = int(dscp)
        if 0 <= d <= 63:
            return d << 2  # DSCP occupies high 6 bits of TOS/Traffic Class
    except Exception:
        pass
    return None


def _parse_header_fields(
    expected_version: int,
    aead_ids: AeadIds,
    session_id: bytes,
    wire: bytes,
) -> Tuple[str, Optional[int]]:
    """
    Try to unpack the header and classify the most likely drop reason *without* AEAD work.
    Returns (reason, seq_if_available).
    """
    HEADER_STRUCT = "!BBBBB8sQB"
    HEADER_LEN = struct.calcsize(HEADER_STRUCT)
    if len(wire) < HEADER_LEN:
        return ("header_too_short", None)
    try:
        (version, kem_id, kem_param, sig_id, sig_param, sess, seq, epoch) = struct.unpack(
            HEADER_STRUCT, wire[:HEADER_LEN]
        )
    except struct.error:
        return ("header_unpack_error", None)
    if version != expected_version:
        return ("version_mismatch", seq)
    if (kem_id, kem_param, sig_id, sig_param) != (
        aead_ids.kem_id,
        aead_ids.kem_param,
        aead_ids.sig_id,
        aead_ids.sig_param,
    ):
        return ("crypto_id_mismatch", seq)
    if sess != session_id:
        return ("session_mismatch", seq)
    # If we got here, header matches; any decrypt failure that returns None is auth/tag failure.
    return ("auth_fail_or_replay", seq)


class _TokenBucket:
    """Per-IP rate limiter using token bucket algorithm."""
    def __init__(self, capacity: int, refill_per_sec: float) -> None:
        self.capacity = max(1, capacity)
        self.refill = max(0.01, float(refill_per_sec))
        self.tokens: Dict[str, float] = {}      # ip -> tokens
        self.last: Dict[str, float] = {}        # ip -> last timestamp

    def allow(self, ip: str) -> bool:
        """Check if request from IP should be allowed."""
        now = time.monotonic()
        t = self.tokens.get(ip, self.capacity)
        last = self.last.get(ip, now)
        # refill
        t = min(self.capacity, t + (now - last) * self.refill)
        self.last[ip] = now
        if t >= 1.0:
            t -= 1.0
            self.tokens[ip] = t
            return True
        self.tokens[ip] = t
        return False


def _validate_config(cfg: dict) -> None:
    """Validate required configuration keys are present."""
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX",
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX",
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW",
    ]
    for key in required_keys:
        if key not in cfg:
            raise NotImplementedError(f"CONFIG missing: {key}")


def _perform_handshake(
    role: str,
    suite: dict,
    gcs_sig_secret: Optional[bytes],
    gcs_sig_public: Optional[bytes],
    cfg: dict,
    stop_after_seconds: Optional[float] = None,
    ready_event: Optional[threading.Event] = None,
) -> Tuple[
    bytes,
    bytes,
    bytes,
    bytes,
    bytes,
    Optional[str],
    Optional[str],
    Tuple[str, int],
]:
    """Perform TCP handshake and return keys, session details, and authenticated peer address."""
    if role == "gcs":
        if gcs_sig_secret is None:
            raise NotImplementedError("GCS signature secret not provided")

        server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_sock.bind(("0.0.0.0", cfg["TCP_HANDSHAKE_PORT"]))
        server_sock.listen(32)

        if ready_event:
            ready_event.set()

        timeout = stop_after_seconds if stop_after_seconds is not None else 30.0
        deadline: Optional[float] = None
        if stop_after_seconds is not None:
            deadline = time.monotonic() + stop_after_seconds

        gate = _TokenBucket(
            cfg.get("HANDSHAKE_RL_BURST", 5),
            cfg.get("HANDSHAKE_RL_REFILL_PER_SEC", 1),
        )

        try:
            try:
                while True:
                    if deadline is not None:
                        remaining = deadline - time.monotonic()
                        if remaining <= 0:
                            raise socket.timeout
                        server_sock.settimeout(max(0.01, remaining))
                    else:
                        server_sock.settimeout(timeout)

                    conn, addr = server_sock.accept()
                    try:
                        ip, _port = addr
                        allowed_ips = {str(cfg["DRONE_HOST"])}
                        allowlist = cfg.get("DRONE_HOST_ALLOWLIST", []) or []
                        if isinstance(allowlist, (list, tuple, set)):
                            for entry in allowlist:
                                allowed_ips.add(str(entry))
                        else:
                            allowed_ips.add(str(allowlist))
                        if ip not in allowed_ips:
                            logger.warning(
                                "Rejected handshake from unauthorized IP",
                                extra={"role": role, "expected": sorted(allowed_ips), "received": ip},
                            )
                            conn.close()
                            continue

                        if not gate.allow(ip):
                            try:
                                conn.settimeout(0.2)
                                conn.sendall(b"\x00")
                            except Exception:
                                pass
                            finally:
                                conn.close()
                            logger.warning(
                                "Handshake rate-limit drop",
                                extra={"role": role, "ip": ip},
                            )
                            continue

                        try:
                            result = server_gcs_handshake(conn, suite, gcs_sig_secret)
                        except HandshakeVerifyError:
                            logger.warning(
                                "Rejected drone handshake with failed authentication",
                                extra={"role": role, "expected": cfg["DRONE_HOST"], "received": ip},
                            )
                            continue
                        # Support either 5-tuple or 7-tuple
                        if len(result) >= 7:
                            k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
                        else:
                            k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                            kem_name = sig_name = None
                        peer_addr = (ip, cfg["UDP_DRONE_RX"])
                        return (
                            k_d2g,
                            k_g2d,
                            nseed_d2g,
                            nseed_g2d,
                            session_id,
                            kem_name,
                            sig_name,
                            peer_addr,
                        )
                    finally:
                        try:
                            conn.close()
                        except Exception:
                            pass
            except socket.timeout:
                raise NotImplementedError("No drone connection received within timeout")
        finally:
            server_sock.close()

    elif role == "drone":
        if gcs_sig_public is None:
            raise NotImplementedError("GCS signature public key not provided")

        client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            client_sock.connect((cfg["GCS_HOST"], cfg["TCP_HANDSHAKE_PORT"]))
            peer_ip, _peer_port = client_sock.getpeername()
            result = client_drone_handshake(client_sock, suite, gcs_sig_public)
            if len(result) >= 7:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
            else:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                kem_name = sig_name = None
            peer_addr = (peer_ip, cfg["UDP_GCS_RX"])
            return (
                k_d2g,
                k_g2d,
                nseed_d2g,
                nseed_g2d,
                session_id,
                kem_name,
                sig_name,
                peer_addr,
            )
        finally:
            client_sock.close()
    else:
        raise ValueError(f"Invalid role: {role}")


@contextmanager
def _setup_sockets(role: str, cfg: dict, *, encrypted_peer: Optional[Tuple[str, int]] = None):
    """Setup and cleanup all UDP sockets for the proxy."""
    sockets = {}
    try:
        if role == "drone":
            # Encrypted socket - receive from GCS
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_DRONE_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind((cfg["DRONE_PLAINTEXT_HOST"], cfg["DRONE_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (no bind needed)
            ptx_out_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sockets["plaintext_out"] = ptx_out_sock

            # Peer addresses
            sockets["encrypted_peer"] = encrypted_peer or (cfg["GCS_HOST"], cfg["UDP_GCS_RX"])
            sockets["plaintext_peer"] = (cfg["DRONE_PLAINTEXT_HOST"], cfg["DRONE_PLAINTEXT_RX"])

        elif role == "gcs":
            # Encrypted socket - receive from Drone
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_GCS_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind((cfg["GCS_PLAINTEXT_HOST"], cfg["GCS_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (no bind needed)
            ptx_out_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sockets["plaintext_out"] = ptx_out_sock

            # Peer addresses
            sockets["encrypted_peer"] = encrypted_peer or (cfg["DRONE_HOST"], cfg["UDP_DRONE_RX"])
            sockets["plaintext_peer"] = (cfg["GCS_PLAINTEXT_HOST"], cfg["GCS_PLAINTEXT_RX"])
        else:
            raise ValueError(f"Invalid role: {role}")

        yield sockets
    finally:
        for sock in list(sockets.values()):
            if isinstance(sock, socket.socket):
                try:
                    sock.close()
                except Exception:
                    pass


def _compute_aead_ids(suite: dict, kem_name: Optional[str], sig_name: Optional[str]) -> AeadIds:
    if kem_name and sig_name and header_ids_from_names:
        ids_tuple = header_ids_from_names(kem_name, sig_name)  # type: ignore
    else:
        ids_tuple = header_ids_for_suite(suite)
    return AeadIds(*ids_tuple)


def _build_sender_receiver(
    role: str,
    ids: AeadIds,
    session_id: bytes,
    k_d2g: bytes,
    k_g2d: bytes,
    cfg: dict,
):
    if role == "drone":
        sender = Sender(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_d2g)
        receiver = Receiver(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_g2d, cfg["REPLAY_WINDOW"])
    else:
        sender = Sender(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_g2d)
        receiver = Receiver(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_d2g, cfg["REPLAY_WINDOW"])
    return sender, receiver


def _launch_manual_console(control_state: ControlState, *, quiet: bool) -> Tuple[threading.Event, Tuple[threading.Thread, ...]]:
    suites_catalog = sorted(list_suites().keys())
    stop_event = threading.Event()

    def status_loop() -> None:
        last_line = ""
        while not stop_event.is_set():
            with control_state.lock:
                state = control_state.state
                suite_id = control_state.current_suite
            line = f"[{state}] {suite_id}"
            if line != last_line and not quiet:
                sys.stderr.write(f"\r{line:<80}")
                sys.stderr.flush()
                last_line = line
            time.sleep(0.5)
        if not quiet:
            sys.stderr.write("\r" + " " * 80 + "\r")
            sys.stderr.flush()

    def operator_loop() -> None:
        if not quiet:
            print("Manual control ready. Type a suite ID, 'list', 'status', or 'quit'.")
        while not stop_event.is_set():
            try:
                line = input("rekey> ")
            except EOFError:
                break
            if line is None:
                continue
            line = line.strip()
            if not line:
                continue
            lowered = line.lower()
            if lowered in {"quit", "exit"}:
                break
            if lowered == "list":
                if not quiet:
                    print("Available suites:")
                    for sid in suites_catalog:
                        print(f"  {sid}")
                continue
            if lowered == "status":
                with control_state.lock:
                    summary = f"state={control_state.state} suite={control_state.current_suite}"
                    if control_state.last_status:
                        summary += f" last_status={control_state.last_status}"
                if not quiet:
                    print(summary)
                continue
            try:
                target_suite = get_suite(line)
                rid = request_prepare(control_state, target_suite["suite_id"])
                if not quiet:
                    print(f"prepare queued for {target_suite['suite_id']} rid={rid}")
            except RuntimeError as exc:
                if not quiet:
                    print(f"Busy: {exc}")
            except Exception as exc:
                if not quiet:
                    print(f"Invalid suite: {exc}")

        stop_event.set()

    status_thread = threading.Thread(target=status_loop, daemon=True)
    operator_thread = threading.Thread(target=operator_loop, daemon=True)
    status_thread.start()
    operator_thread.start()
    return stop_event, (status_thread, operator_thread)


def run_proxy(
    *,
    role: str,
    suite: dict,
    cfg: dict,
    gcs_sig_secret: Optional[bytes] = None,
    gcs_sig_public: Optional[bytes] = None,
    stop_after_seconds: Optional[float] = None,
    manual_control: bool = False,
    quiet: bool = False,
    ready_event: Optional[threading.Event] = None,
    status_file: Optional[str] = None,
) -> Dict[str, object]:
    """
    Start a blocking proxy process for `role` in {"drone","gcs"}.

    Performs the TCP handshake, bridges plaintext/encrypted UDP, and processes
    in-band control messages for rekey negotiation. Returns counters on clean exit.
    """
    if role not in {"drone", "gcs"}:
        raise ValueError(f"Invalid role: {role}")

    _validate_config(cfg)

    counters = ProxyCounters()
    counters_lock = threading.Lock()
    start_time = time.time()

    status_path: Optional[Path] = None
    if status_file:
        status_path = Path(status_file).expanduser()

    def write_status(payload: Dict[str, object]) -> None:
        if status_path is None:
            return
        try:
            status_path.parent.mkdir(parents=True, exist_ok=True)
            tmp_path = status_path.with_suffix(status_path.suffix + ".tmp")
            tmp_path.write_text(json.dumps(payload), encoding="utf-8")
            tmp_path.replace(status_path)
        except Exception as exc:
            logger.warning(
                "Failed to write status file",
                extra={"role": role, "error": str(exc), "path": str(status_path)},
            )

    handshake_result = _perform_handshake(
        role, suite, gcs_sig_secret, gcs_sig_public, cfg, stop_after_seconds, ready_event
    )

    (
        k_d2g,
        k_g2d,
        _nseed_d2g,
        _nseed_g2d,
        session_id,
        kem_name,
        sig_name,
        peer_addr,
    ) = handshake_result

    suite_id = suite.get("suite_id")
    if not suite_id:
        try:
            suite_id = next((sid for sid, s in SUITES.items() if dict(s) == suite), "unknown")
        except Exception:
            suite_id = "unknown"

    write_status({
        "status": "handshake_ok",
        "suite": suite_id,
        "session_id": session_id.hex(),
    })

    sess_display = (
        session_id.hex()
        if cfg.get("LOG_SESSION_ID", False)
        else hashlib.sha256(session_id).hexdigest()[:8] + "..."
    )

    logger.info(
        "PQC handshake completed successfully",
        extra={
            "suite_id": suite_id,
            "peer_role": ("drone" if role == "gcs" else "gcs"),
            "session_id": sess_display,
        },
    )

    # Periodically persist counters to the status file while the proxy runs.
    # This allows external automation (scheduler) to observe enc_in/enc_out
    # during long-running experiments without waiting for process exit.
    stop_status_writer = threading.Event()

    def _status_writer() -> None:
        while not stop_status_writer.is_set():
            try:
                with counters_lock:
                    payload = {
                        "status": "running",
                        "suite": suite_id,
                        "counters": counters.to_dict(),
                        "ts_ns": time.time_ns(),
                    }
                write_status(payload)
            except Exception:
                logger.debug("status writer failed", extra={"role": role})
            # sleep with event to allow quick shutdown
            stop_status_writer.wait(1.0)

    status_thread: Optional[threading.Thread] = None
    try:
        status_thread = threading.Thread(target=_status_writer, daemon=True)
        status_thread.start()
    except Exception:
        status_thread = None

    aead_ids = _compute_aead_ids(suite, kem_name, sig_name)
    sender, receiver = _build_sender_receiver(role, aead_ids, session_id, k_d2g, k_g2d, cfg)

    control_state = create_control_state(role, suite_id)
    context_lock = threading.RLock()
    active_context: Dict[str, object] = {
        "suite": suite_id,
        "suite_dict": suite,
        "session_id": session_id,
        "aead_ids": aead_ids,
        "sender": sender,
        "receiver": receiver,
        "peer_addr": peer_addr,
        "peer_match_strict": bool(cfg.get("STRICT_UDP_PEER_MATCH", True)),
    }

    active_rekeys: set[str] = set()
    rekey_guard = threading.Lock()

    if manual_control and role == "gcs" and not cfg.get("ENABLE_PACKET_TYPE"):
        logger.warning("ENABLE_PACKET_TYPE is disabled; control-plane packets may not be processed correctly.")

    manual_stop: Optional[threading.Event] = None
    manual_threads: Tuple[threading.Thread, ...] = ()
    if manual_control and role == "gcs":
        manual_stop, manual_threads = _launch_manual_console(control_state, quiet=quiet)

    def _launch_rekey(target_suite_id: str, rid: str) -> None:
        with rekey_guard:
            if rid in active_rekeys:
                return
            active_rekeys.add(rid)

        logger.info(
            "Control rekey negotiation started",
            extra={"role": role, "suite_id": target_suite_id, "rid": rid},
        )

        def worker() -> None:
            try:
                new_suite = get_suite(target_suite_id)
            except NotImplementedError as exc:
                with context_lock:
                    current_suite = active_context["suite"]
                with counters_lock:
                    counters.rekeys_fail += 1
                record_rekey_result(control_state, rid, current_suite, success=False)
                logger.warning(
                    "Control rekey rejected: unknown suite",
                    extra={"role": role, "suite_id": target_suite_id, "rid": rid, "error": str(exc)},
                )
                with rekey_guard:
                    active_rekeys.discard(rid)
                return

            try:
                timeout = cfg.get("REKEY_HANDSHAKE_TIMEOUT", 20.0)
                rk_result = _perform_handshake(role, new_suite, gcs_sig_secret, gcs_sig_public, cfg, timeout)
                (
                    new_k_d2g,
                    new_k_g2d,
                    _nd1,
                    _nd2,
                    new_session_id,
                    new_kem_name,
                    new_sig_name,
                    new_peer_addr,
                ) = rk_result

                new_ids = _compute_aead_ids(new_suite, new_kem_name, new_sig_name)
                new_sender, new_receiver = _build_sender_receiver(
                    role, new_ids, new_session_id, new_k_d2g, new_k_g2d, cfg
                )

                with context_lock:
                    active_context.update(
                        {
                            "sender": new_sender,
                            "receiver": new_receiver,
                            "session_id": new_session_id,
                            "aead_ids": new_ids,
                            "suite": new_suite["suite_id"],
                            "suite_dict": new_suite,
                            "peer_addr": new_peer_addr,
                        }
                    )
                    sockets["encrypted_peer"] = new_peer_addr

                with counters_lock:
                    counters.rekeys_ok += 1
                    counters.last_rekey_ms = int(time.time() * 1000)
                    counters.last_rekey_suite = new_suite["suite_id"]
                record_rekey_result(control_state, rid, new_suite["suite_id"], success=True)
                write_status(
                    {
                        "status": "rekey_ok",
                        "new_suite": new_suite["suite_id"],
                        "session_id": new_session_id.hex(),
                    }
                )
                new_sess_display = (
                    new_session_id.hex()
                    if cfg.get("LOG_SESSION_ID", False)
                    else hashlib.sha256(new_session_id).hexdigest()[:8] + "..."
                )
                logger.info(
                    "Control rekey successful",
                    extra={
                        "role": role,
                        "suite_id": new_suite["suite_id"],
                        "rid": rid,
                        "session_id": new_sess_display,
                    },
                )
            except Exception as exc:
                with context_lock:
                    current_suite = active_context["suite"]
                with counters_lock:
                    counters.rekeys_fail += 1
                record_rekey_result(control_state, rid, current_suite, success=False)
                logger.warning(
                    "Control rekey failed",
                    extra={"role": role, "suite_id": target_suite_id, "rid": rid, "error": str(exc)},
                )
            finally:
                with rekey_guard:
                    active_rekeys.discard(rid)

        threading.Thread(target=worker, daemon=True).start()

    with _setup_sockets(role, cfg, encrypted_peer=peer_addr) as sockets:
        selector = selectors.DefaultSelector()
        selector.register(sockets["encrypted"], selectors.EVENT_READ, data="encrypted")
        selector.register(sockets["plaintext_in"], selectors.EVENT_READ, data="plaintext_in")

        def send_control(payload: dict) -> None:
            body = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
            frame = b"\x02" + body
            with context_lock:
                current_sender = active_context["sender"]
            try:
                wire = current_sender.encrypt(frame)
            except Exception as exc:
                counters.drops += 1
                counters.drop_other += 1
                logger.warning("Failed to encrypt control payload", extra={"role": role, "error": str(exc)})
                return
            try:
                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                counters.enc_out += 1
            except socket.error as exc:
                counters.drops += 1
                counters.drop_other += 1
                logger.warning("Failed to send control payload", extra={"role": role, "error": str(exc)})

        try:
            while True:
                if stop_after_seconds is not None and (time.time() - start_time) >= stop_after_seconds:
                    break

                while True:
                    try:
                        control_payload = control_state.outbox.get_nowait()
                    except queue.Empty:
                        break
                    send_control(control_payload)

                events = selector.select(timeout=0.1)
                for key, _mask in events:
                    sock = key.fileobj
                    data_type = key.data

                    if data_type == "plaintext_in":
                        try:
                            payload, _addr = sock.recvfrom(16384)
                            if not payload:
                                continue
                            with counters_lock:
                                counters.ptx_in += 1

                            payload_out = (b"\x01" + payload) if cfg.get("ENABLE_PACKET_TYPE") else payload
                            with context_lock:
                                current_sender = active_context["sender"]
                            wire = current_sender.encrypt(payload_out)

                            try:
                                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                                with counters_lock:
                                    counters.enc_out += 1
                            except socket.error:
                                with counters_lock:
                                    counters.drops += 1
                        except socket.error:
                            continue

                    elif data_type == "encrypted":
                        try:
                            wire, addr = sock.recvfrom(16384)
                            if not wire:
                                continue

                            with context_lock:
                                current_receiver = active_context["receiver"]
                                expected_peer = active_context.get("peer_addr")
                                strict_match = bool(active_context.get("peer_match_strict", True))

                            src_ip, src_port = addr
                            if expected_peer is not None:
                                exp_ip, exp_port = expected_peer  # type: ignore[misc]
                                mismatch = False
                                if strict_match:
                                    mismatch = src_ip != exp_ip or src_port != exp_port
                                else:
                                    mismatch = src_ip != exp_ip
                                if mismatch:
                                    with counters_lock:
                                        counters.drops += 1
                                        counters.drop_src_addr += 1
                                    logger.debug(
                                        "Dropped encrypted packet from unauthorized source",
                                        extra={"role": role, "expected": expected_peer, "received": addr},
                                    )
                                    continue

                            with counters_lock:
                                counters.enc_in += 1

                            try:
                                plaintext = current_receiver.decrypt(wire)
                                if plaintext is None:
                                    with counters_lock:
                                        counters.drops += 1
                                        last_reason = current_receiver.last_error_reason()
                                        # Bug #7 fix: Proper error classification without redundancy
                                        if last_reason == "auth":
                                            counters.drop_auth += 1
                                        elif last_reason == "header":
                                            counters.drop_header += 1
                                        elif last_reason == "replay":
                                            counters.drop_replay += 1
                                        elif last_reason == "session":
                                            counters.drop_session_epoch += 1
                                        elif last_reason is None or last_reason == "unknown":
                                            # Only parse header if receiver didn't classify it
                                            reason, _seq = _parse_header_fields(
                                                CONFIG["WIRE_VERSION"],
                                                current_receiver.ids,
                                                current_receiver.session_id,
                                                wire,
                                            )
                                            if reason in (
                                                "version_mismatch",
                                                "crypto_id_mismatch",
                                                "header_too_short",
                                                "header_unpack_error",
                                            ):
                                                counters.drop_header += 1
                                            elif reason == "session_mismatch":
                                                counters.drop_session_epoch += 1
                                            elif reason == "auth_fail_or_replay":
                                                counters.drop_auth += 1
                                            else:
                                                counters.drop_other += 1
                                        else:
                                            # Unrecognized last_reason value
                                            counters.drop_other += 1
                                    continue
                            except ReplayError:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_replay += 1
                                continue
                            except HeaderMismatch:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_header += 1
                                continue
                            except AeadAuthError:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_auth += 1
                                continue
                            except NotImplementedError as exc:
                                with counters_lock:
                                    counters.drops += 1
                                    reason, _seq = _parse_header_fields(
                                        CONFIG["WIRE_VERSION"], current_receiver.ids, current_receiver.session_id, wire
                                    )
                                    if reason in (
                                        "version_mismatch",
                                        "crypto_id_mismatch",
                                        "header_too_short",
                                        "header_unpack_error",
                                    ):
                                        counters.drop_header += 1
                                    elif reason == "session_mismatch":
                                        counters.drop_session_epoch += 1
                                    else:
                                        counters.drop_auth += 1
                                logger.warning(
                                    "Decrypt failed (classified)",
                                    extra={
                                        "role": role,
                                        "reason": reason,
                                        "wire_len": len(wire),
                                        "error": str(exc),
                                    },
                                )
                                continue
                            except Exception as exc:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                                logger.warning(
                                    "Decrypt failed (other)",
                                    extra={"role": role, "error": str(exc), "wire_len": len(wire)},
                                )
                                continue

                            try:
                                if plaintext and plaintext[0] == 0x02:
                                    try:
                                        control_json = json.loads(plaintext[1:].decode("utf-8"))
                                    except (UnicodeDecodeError, json.JSONDecodeError):
                                        with counters_lock:
                                            counters.drops += 1
                                            counters.drop_other += 1
                                        continue
                                    result = handle_control(control_json, role, control_state)
                                    for note in result.notes:
                                        if note.startswith("prepare_fail"):
                                            with counters_lock:
                                                counters.rekeys_fail += 1
                                    for payload in result.send:
                                        control_state.outbox.put(payload)
                                    if result.start_handshake:
                                        suite_next, rid = result.start_handshake
                                        _launch_rekey(suite_next, rid)
                                    continue

                                if cfg.get("ENABLE_PACKET_TYPE") and plaintext:
                                    ptype = plaintext[0]
                                    if ptype == 0x01:
                                        out_bytes = plaintext[1:]
                                    else:
                                        with counters_lock:
                                            counters.drops += 1
                                            counters.drop_other += 1
                                        continue
                                else:
                                    out_bytes = plaintext

                                sockets["plaintext_out"].sendto(out_bytes, sockets["plaintext_peer"])
                                with counters_lock:
                                    counters.ptx_out += 1
                            except socket.error:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                        except socket.error:
                            continue
        except KeyboardInterrupt:
            pass
        finally:
            selector.close()
            if manual_stop:
                manual_stop.set()
                for thread in manual_threads:
                    thread.join(timeout=0.5)

        # Final status write and stop the status writer thread if running
        try:
            with counters_lock:
                write_status({
                    "status": "stopped",
                    "suite": suite_id,
                    "counters": counters.to_dict(),
                    "ts_ns": time.time_ns(),
                })
        except Exception:
            pass

        if 'stop_status_writer' in locals() and stop_status_writer is not None:
            try:
                stop_status_writer.set()
            except Exception:
                pass
        if 'status_thread' in locals() and status_thread is not None and status_thread.is_alive():
            try:
                status_thread.join(timeout=1.0)
            except Exception:
                pass

        return counters.to_dict()

============================================================

FILE 4/12: config.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\config.py
Size: 13,291 bytes
Modified: 2025-10-05 05:20:40
------------------------------------------------------------
"""
Core configuration constants for PQC drone-GCS secure proxy.

Single source of truth for all network ports, hosts, and runtime parameters.
"""

import os
from ipaddress import ip_address
from typing import Dict, Any


# Baseline host defaults reused throughout the configuration payload.
_DEFAULT_DRONE_HOST = "192.168.0.102"
_DEFAULT_GCS_HOST = "192.168.0.103"


# Default configuration - all required keys with correct types
CONFIG = {
    # Handshake (TCP)
    "TCP_HANDSHAKE_PORT": 46000,

    # Encrypted UDP data-plane (network)
    "UDP_DRONE_RX": 46012,   # drone binds here; GCS sends here
    "UDP_GCS_RX": 46011,     # gcs binds here; Drone sends here

    # Plaintext UDP (local loopback to apps/FC)
    "DRONE_PLAINTEXT_TX": 47003,  # app→drone-proxy (to encrypt out)
    "DRONE_PLAINTEXT_RX": 47004,  # drone-proxy→app (after decrypt)
    "GCS_PLAINTEXT_TX": 47001,    # app→gcs-proxy
    "GCS_PLAINTEXT_RX": 47002,    # gcs-proxy→app
    "DRONE_PLAINTEXT_HOST": "127.0.0.1",
    "GCS_PLAINTEXT_HOST": "127.0.0.1",

    # Hosts
    "DRONE_HOST": _DEFAULT_DRONE_HOST,
    "GCS_HOST": _DEFAULT_GCS_HOST,

    # Pre-shared key (hex) for drone authentication during handshake.
    # Default is a placeholder; override in production via environment variable.
    "DRONE_PSK": "0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef",

    # Crypto/runtime
    "REPLAY_WINDOW": 1024,
    "WIRE_VERSION": 1,      # header version byte (frozen)

    # --- Optional hardening / QoS knobs (NOT required; safe defaults) ---
    # Limit TCP handshake attempts accepted per IP at the GCS (server) side.
    # Model: token bucket; BURST tokens max, refilling at REFILL_PER_SEC tokens/sec.
    "HANDSHAKE_RL_BURST": 5,
    "HANDSHAKE_RL_REFILL_PER_SEC": 1,

    # Mark encrypted UDP with DSCP EF (46) to prioritize on WMM-enabled APs.
    # Set to None to disable. Implementation multiplies by 4 to form TOS.
    "ENCRYPTED_DSCP": 46,

    # Feature flag: if True, proxy prefixes app->proxy plaintext with 1 byte packet type.
    # 0x01 = MAVLink/data (forward to local app); 0x02 = control (route to policy engine).
    # When False (default), proxy passes bytes unchanged (backward compatible).
    "ENABLE_PACKET_TYPE": True,

    # Enforce strict matching of encrypted UDP peer IP/port with the authenticated handshake peer.
    # Disable (set to False) only when operating behind NAT where source ports may differ.
    "STRICT_UDP_PEER_MATCH": True,

    # Log real session IDs only when explicitly enabled (default False masks them to hashes).
    "LOG_SESSION_ID": False,

    # --- Simple automation defaults (tools/auto/*_simple.py) ---
    "DRONE_CONTROL_HOST": "0.0.0.0",
    "DRONE_CONTROL_PORT": 48080,
    "SIMPLE_VERIFY_TIMEOUT_S": 5.0,
    "SIMPLE_PACKETS_PER_SUITE": 1,
    "SIMPLE_PACKET_DELAY_S": 0.0,
    "SIMPLE_SUITE_DWELL_S": 0.0,
    "SIMPLE_INITIAL_SUITE": "cs-mlkem768-aesgcm-mldsa65",

    # Automation defaults for tools/auto orchestration scripts
    "AUTO_DRONE": {
        # Session IDs default to "<prefix>_<unix>" unless DRONE_SESSION_ID env overrides
        "session_prefix": "run",
        # Optional explicit initial suite override (None -> discover from secrets/config)
        "initial_suite": None,
        # Enable follower monitors (perf/pidstat/psutil) by default
        "monitors_enabled": True,
        # Apply CPU governor tweaks unless disabled
        "cpu_optimize": True,
        # Enable telemetry publisher back to the scheduler
        "telemetry_enabled": True,
        # Optional explicit telemetry host/port (None -> derive from CONFIG)
    "telemetry_host": _DEFAULT_GCS_HOST,
        "telemetry_port": 52080,
        # Override monitoring output base directory (None -> DEFAULT_MONITOR_BASE)
        "monitor_output_base": None,
        # Optional environment exports applied before creating the power monitor
        "power_env": {
            # Maintain 1 kHz sampling by default; backend remains auto unless overridden
            "DRONE_POWER_BACKEND": "ina219",
            "DRONE_POWER_SAMPLE_HZ": "1000",
            "INA219_I2C_BUS": "1",
            "INA219_ADDR": "0x40",
            "INA219_SHUNT_OHM": "0.1",
        },
    },

    "AUTO_GCS": {
        # Session IDs default to "<prefix>_<unix>" unless GCS_SESSION_ID env overrides
        "session_prefix": "run",
        # Traffic profile: "blast", "saturation", or "mavproxy" placeholder
        "traffic": "blast",
        # Duration for active traffic window per suite (seconds)
        "duration_s": 45.0,
        # Delay after rekey before starting traffic (seconds)
        "pre_gap_s": 1.0,
        # Delay between suites (seconds)
        "inter_gap_s": 15.0,
        # UDP payload size (bytes) for blaster calculations
        "payload_bytes": 256,
        # Sample every Nth send/receive event (0 disables)
        "event_sample": 100,
        # Number of full passes across suite list
        "passes": 1,
        # Explicit packets-per-second override; 0 means best-effort
        "rate_pps": 0,
        # Optional bandwidth target in Mbps (converted to PPS if > 0)
        "bandwidth_mbps": 0.0,
        # Max rate explored during saturation sweeps (Mbps)
        "max_rate_mbps": 200.0,
        # Optional ordered suite subset (None -> all suites)
        "suites": [
            "cs-mlkem512-aesgcm-mldsa44",
            "cs-mlkem512-aesgcm-mldsa65",
            "cs-mlkem512-aesgcm-mldsa87",
            "cs-mlkem512-aesgcm-falcon512",
            "cs-mlkem512-aesgcm-falcon1024",
            "cs-mlkem512-aesgcm-sphincs128fsha2",
            "cs-mlkem512-aesgcm-sphincs256fsha2",
            "cs-mlkem768-aesgcm-mldsa44",
            "cs-mlkem768-aesgcm-mldsa65",
            "cs-mlkem768-aesgcm-mldsa87",
            "cs-mlkem768-aesgcm-falcon512",
            "cs-mlkem768-aesgcm-falcon1024",
            "cs-mlkem768-aesgcm-sphincs128fsha2",
            "cs-mlkem768-aesgcm-sphincs256fsha2",
            "cs-mlkem1024-aesgcm-mldsa44",
            "cs-mlkem1024-aesgcm-mldsa65",
            "cs-mlkem1024-aesgcm-mldsa87",
            "cs-mlkem1024-aesgcm-falcon512",
            "cs-mlkem1024-aesgcm-falcon1024",
            "cs-mlkem1024-aesgcm-sphincs128fsha2",
            "cs-mlkem1024-aesgcm-sphincs256fsha2",
        ],
        # Launch local GCS proxy under scheduler control
        "launch_proxy": True,
        # Enable local proxy monitors (perf/pidstat/psutil)
        "monitors_enabled": True,
        # Start telemetry collector on the scheduler side
        "telemetry_enabled": True,
        # Bind/port for telemetry collector (defaults to CONFIG values)
        "telemetry_bind_host": "0.0.0.0",
        "telemetry_port": 52080,
        # Emit combined Excel workbook when run completes
        "export_combined_excel": True,
    },
}


# Required keys with their expected types
_REQUIRED_KEYS = {
    "TCP_HANDSHAKE_PORT": int,
    "UDP_DRONE_RX": int,
    "UDP_GCS_RX": int,
    "DRONE_PLAINTEXT_TX": int,
    "DRONE_PLAINTEXT_RX": int,
    "GCS_PLAINTEXT_TX": int,
    "GCS_PLAINTEXT_RX": int,
    "DRONE_HOST": str,
    "GCS_HOST": str,
    "DRONE_PLAINTEXT_HOST": str,
    "GCS_PLAINTEXT_HOST": str,
    "REPLAY_WINDOW": int,
    "WIRE_VERSION": int,
    "ENABLE_PACKET_TYPE": bool,
    "STRICT_UDP_PEER_MATCH": bool,
    "LOG_SESSION_ID": bool,
    "DRONE_PSK": str,
}

# Keys that can be overridden by environment variables
_ENV_OVERRIDABLE = {
    "TCP_HANDSHAKE_PORT",
    "UDP_DRONE_RX", 
    "UDP_GCS_RX",
    "DRONE_PLAINTEXT_TX",  # Added for testing/benchmarking flexibility
    "DRONE_PLAINTEXT_RX",  # Added for testing/benchmarking flexibility  
    "GCS_PLAINTEXT_TX",    # Added for testing/benchmarking flexibility
    "GCS_PLAINTEXT_RX",    # Added for testing/benchmarking flexibility
    "DRONE_PLAINTEXT_HOST",
    "GCS_PLAINTEXT_HOST",
    "DRONE_HOST",
    "GCS_HOST",
    "ENABLE_PACKET_TYPE",
    "STRICT_UDP_PEER_MATCH",
    "LOG_SESSION_ID",
    "DRONE_PSK",
}


def validate_config(cfg: Dict[str, Any]) -> None:
    """
    Ensure all required keys exist with correct types/ranges.
    Raise NotImplementedError("<reason>") on any violation.
    No return value on success.
    """
    # Check all required keys exist
    missing_keys = set(_REQUIRED_KEYS.keys()) - set(cfg.keys())
    if missing_keys:
        raise NotImplementedError(f"CONFIG missing required keys: {', '.join(sorted(missing_keys))}")
    
    # Check types for all keys
    for key, expected_type in _REQUIRED_KEYS.items():
        value = cfg[key]
        if not isinstance(value, expected_type):
            raise NotImplementedError(f"CONFIG[{key}] must be {expected_type.__name__}, got {type(value).__name__}")
    
    # Validate port ranges
    for key in _REQUIRED_KEYS:
        if key.endswith("_PORT") or key.endswith("_RX") or key.endswith("_TX"):
            port = cfg[key]
            if not (1 <= port <= 65535):
                raise NotImplementedError(f"CONFIG[{key}] must be valid port (1-65535), got {port}")
    
    # Validate specific constraints
    if cfg["WIRE_VERSION"] != 1:
        raise NotImplementedError(f"CONFIG[WIRE_VERSION] must be 1 (frozen), got {cfg['WIRE_VERSION']}")
    
    if cfg["REPLAY_WINDOW"] < 64:
        raise NotImplementedError(f"CONFIG[REPLAY_WINDOW] must be >= 64, got {cfg['REPLAY_WINDOW']}")
    if cfg["REPLAY_WINDOW"] > 8192:
        raise NotImplementedError(f"CONFIG[REPLAY_WINDOW] must be <= 8192, got {cfg['REPLAY_WINDOW']}")
    
    # Validate hosts are valid strings (basic check)
    for host_key in ["DRONE_HOST", "GCS_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise NotImplementedError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        try:
            ip_address(host)
        except ValueError as exc:
            raise NotImplementedError(f"CONFIG[{host_key}] must be a valid IP address: {exc}")

    # Loopback hosts for plaintext path may remain hostnames (e.g., 127.0.0.1).
    allow_non_loopback_plaintext = str(os.environ.get("ALLOW_NON_LOOPBACK_PLAINTEXT", "")).strip().lower() in {
        "1",
        "true",
        "yes",
        "on",
    }
    for host_key in ["DRONE_PLAINTEXT_HOST", "GCS_PLAINTEXT_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise NotImplementedError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        if allow_non_loopback_plaintext:
            continue
        try:
            parsed = ip_address(host)
            if not parsed.is_loopback:
                raise NotImplementedError(
                    f"CONFIG[{host_key}] must be a loopback address unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
        except ValueError:
            if host.lower() != "localhost":
                raise NotImplementedError(
                    f"CONFIG[{host_key}] must be loopback/localhost unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
    
    # Optional keys are intentionally not required; do light validation if present
    if "ENCRYPTED_DSCP" in cfg and cfg["ENCRYPTED_DSCP"] is not None:
        if not (0 <= int(cfg["ENCRYPTED_DSCP"]) <= 63):
            raise NotImplementedError("CONFIG[ENCRYPTED_DSCP] must be 0..63 or None")

    psk = cfg.get("DRONE_PSK", "")
    try:
        psk_bytes = bytes.fromhex(psk)
    except ValueError:
        raise NotImplementedError("CONFIG[DRONE_PSK] must be a hex string")
    if len(psk_bytes) != 32:
        raise NotImplementedError("CONFIG[DRONE_PSK] must decode to 32 bytes")


def _apply_env_overrides(cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Apply environment variable overrides to config."""
    result = cfg.copy()
    
    for key in _ENV_OVERRIDABLE:
        env_var = key
        if env_var in os.environ:
            env_value = os.environ[env_var]
            expected_type = _REQUIRED_KEYS[key]
            
            try:
                if expected_type == int:
                    result[key] = int(env_value)
                elif expected_type == str:
                    result[key] = str(env_value)
                elif expected_type == bool:
                    lowered = str(env_value).strip().lower()
                    if lowered in {"1", "true", "yes", "on"}:
                        result[key] = True
                    elif lowered in {"0", "false", "no", "off"}:
                        result[key] = False
                    else:
                        raise ValueError(f"invalid boolean literal: {env_value}")
                else:
                    raise NotImplementedError(f"Unsupported type for env override: {expected_type}")
            except ValueError:
                raise NotImplementedError(f"Invalid {expected_type.__name__} value for {env_var}: {env_value}")
    
    return result


# Apply environment overrides and validate
CONFIG = _apply_env_overrides(CONFIG)
validate_config(CONFIG)

============================================================

FILE 5/12: handshake.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\handshake.py
Size: 12,857 bytes
Modified: 2025-10-05 01:00:21
------------------------------------------------------------
from dataclasses import dataclass
import hashlib
import hmac
import os
import struct
from core.config import CONFIG
from core.suites import get_suite
from core.logging_utils import get_logger
from oqs.oqs import KeyEncapsulation, Signature

logger = get_logger("pqc")

class HandshakeFormatError(Exception):
    pass

class HandshakeVerifyError(Exception):
    pass

@dataclass(frozen=True)
class ServerHello:
    version: int
    kem_name: bytes
    sig_name: bytes
    session_id: bytes
    kem_pub: bytes
    signature: bytes
    challenge: bytes

@dataclass
class ServerEphemeral:
    kem_name: str
    sig_name: str
    session_id: bytes
    kem_obj: object  # oqs.KeyEncapsulation instance
    challenge: bytes

def build_server_hello(suite_id: str, server_sig_obj):
    suite = get_suite(suite_id)
    if not suite:
        raise NotImplementedError("suite_id not found")
    version = CONFIG["WIRE_VERSION"]
    kem_name = suite["kem_name"].encode("utf-8")
    sig_name = suite["sig_name"].encode("utf-8")
    if not kem_name or not sig_name:
        raise NotImplementedError("kem_name/sig_name empty")
    if not isinstance(server_sig_obj, Signature):
        raise NotImplementedError("server_sig_obj must be oqs.Signature")
    session_id = os.urandom(8)
    challenge = os.urandom(8)
    kem_obj = KeyEncapsulation(kem_name.decode("utf-8"))
    kem_pub = kem_obj.generate_keypair()
    # Include negotiated wire version as first byte of transcript to prevent downgrade
    transcript = (
        struct.pack("!B", version)
        + b"|pq-drone-gcs:v1|"
        + session_id
        + b"|"
        + kem_name
        + b"|"
        + sig_name
        + b"|"
        + kem_pub
        + b"|"
        + challenge
    )
    signature = server_sig_obj.sign(transcript)
    wire = struct.pack("!B", version)
    wire += struct.pack("!H", len(kem_name)) + kem_name
    wire += struct.pack("!H", len(sig_name)) + sig_name
    wire += session_id
    wire += challenge
    wire += struct.pack("!I", len(kem_pub)) + kem_pub
    wire += struct.pack("!H", len(signature)) + signature
    ephemeral = ServerEphemeral(
        kem_name=kem_name.decode("utf-8"),
        sig_name=sig_name.decode("utf-8"),
        session_id=session_id,
        kem_obj=kem_obj,
        challenge=challenge,
    )
    return wire, ephemeral

def parse_and_verify_server_hello(wire: bytes, expected_version: int, server_sig_pub: bytes) -> ServerHello:
    try:
        offset = 0
        version = wire[offset]
        offset += 1
        if version != expected_version:
            raise HandshakeFormatError("bad wire version")
        kem_name_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        kem_name = wire[offset:offset+kem_name_len]
        offset += kem_name_len
        sig_name_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        sig_name = wire[offset:offset+sig_name_len]
        offset += sig_name_len
        session_id = wire[offset:offset+8]
        offset += 8
        challenge = wire[offset:offset+8]
        offset += 8
        kem_pub_len = struct.unpack_from("!I", wire, offset)[0]
        offset += 4
        kem_pub = wire[offset:offset+kem_pub_len]
        offset += kem_pub_len
        sig_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        signature = wire[offset:offset+sig_len]
        offset += sig_len
    except Exception:
        raise HandshakeFormatError("malformed server hello")
    transcript = (
        struct.pack("!B", version)
        + b"|pq-drone-gcs:v1|"
        + session_id
        + b"|"
        + kem_name
        + b"|"
        + sig_name
        + b"|"
        + kem_pub
        + b"|"
        + challenge
    )
    try:
        sig = Signature(sig_name.decode("utf-8"))
        if not sig.verify(transcript, signature, server_sig_pub):
            raise HandshakeVerifyError("bad signature")
    except HandshakeVerifyError:
        raise
    except Exception:
        raise HandshakeVerifyError("signature verification failed")
    return ServerHello(
        version=version,
        kem_name=kem_name,
        sig_name=sig_name,
        session_id=session_id,
        kem_pub=kem_pub,
        signature=signature,
        challenge=challenge,
    )

def _drone_psk_bytes() -> bytes:
    psk_hex = CONFIG.get("DRONE_PSK", "")
    try:
        psk = bytes.fromhex(psk_hex)
    except ValueError as exc:
        raise NotImplementedError(f"Invalid DRONE_PSK hex: {exc}")
    if len(psk) != 32:
        raise NotImplementedError("DRONE_PSK must decode to 32 bytes")
    return psk


def client_encapsulate(server_hello: ServerHello):
    try:
        kem = KeyEncapsulation(server_hello.kem_name.decode("utf-8"))
        kem_ct, shared_secret = kem.encap_secret(server_hello.kem_pub)
        return kem_ct, shared_secret
    except Exception:
        raise NotImplementedError("client_encapsulate failed")

def server_decapsulate(ephemeral: ServerEphemeral, kem_ct: bytes):
    try:
        shared_secret = ephemeral.kem_obj.decap_secret(kem_ct)
        return shared_secret
    except Exception:
        raise NotImplementedError("server_decapsulate failed")

def derive_transport_keys(role: str, session_id: bytes, kem_name: bytes, sig_name: bytes, shared_secret: bytes):
    if role not in {"client", "server"}:
        raise NotImplementedError("invalid role")
    if not (isinstance(session_id, bytes) and len(session_id) == 8):
        raise NotImplementedError("session_id must be 8 bytes")
    if not kem_name or not sig_name:
        raise NotImplementedError("kem_name/sig_name empty")
    try:
        from cryptography.hazmat.primitives.kdf.hkdf import HKDF
        from cryptography.hazmat.primitives import hashes
    except ImportError:
        raise NotImplementedError("cryptography not available")
    info = b"pq-drone-gcs:kdf:v1|" + session_id + b"|" + kem_name + b"|" + sig_name
    hkdf = HKDF(
        algorithm=hashes.SHA256(),
        length=64,
        salt=b"pq-drone-gcs|hkdf|v1",
        info=info
    )
    okm = hkdf.derive(shared_secret)
    key_d2g = okm[:32]
    key_g2d = okm[32:64]

    if role == "client":
        # Drone acts as client; return (send_to_gcs, receive_from_gcs).
        return key_d2g, key_g2d
    else:  # server == GCS
        # GCS perspective: send_to_drone first, receive_from_drone second.
        return key_g2d, key_d2g
def server_gcs_handshake(conn, suite, gcs_sig_secret):
    """Authenticated GCS side handshake.

    Requires a ready oqs.Signature object (with generated key pair). Fails fast if not.
    """
    from oqs.oqs import Signature
    import struct

    conn.settimeout(10.0)

    if not isinstance(gcs_sig_secret, Signature):
        raise ValueError("gcs_sig_secret must be an oqs.Signature object with a loaded keypair")

    # Resolve suite_id by matching suite dict
    suite_id = None
    from core.suites import SUITES
    for sid, s in SUITES.items():
        if dict(s) == suite:
            suite_id = sid
            break
    if suite_id is None:
        raise ValueError("suite not found in registry")

    hello_wire, ephemeral = build_server_hello(suite_id, gcs_sig_secret)
    conn.sendall(struct.pack("!I", len(hello_wire)) + hello_wire)

    # Receive KEM ciphertext
    ct_len_bytes = b""
    while len(ct_len_bytes) < 4:
        chunk = conn.recv(4 - len(ct_len_bytes))
        if not chunk:
            raise ConnectionError("Connection closed reading ciphertext length")
        ct_len_bytes += chunk
    ct_len = struct.unpack("!I", ct_len_bytes)[0]
    kem_ct = b""
    while len(kem_ct) < ct_len:
        chunk = conn.recv(ct_len - len(kem_ct))
        if not chunk:
            raise ConnectionError("Connection closed reading ciphertext")
        kem_ct += chunk

    tag_len = hashlib.sha256().digest_size
    tag = b""
    while len(tag) < tag_len:
        chunk = conn.recv(tag_len - len(tag))
        if not chunk:
            raise ConnectionError("Connection closed reading drone authentication tag")
        tag += chunk

    expected_tag = hmac.new(_drone_psk_bytes(), hello_wire, hashlib.sha256).digest()
    if not hmac.compare_digest(tag, expected_tag):
        peer_ip = "unknown"
        try:
            peer_info = conn.getpeername()
            if isinstance(peer_info, tuple) and peer_info:
                peer_ip = str(peer_info[0])
            elif isinstance(peer_info, str) and peer_info:
                peer_ip = peer_info
        except (OSError, ValueError):
            peer_ip = "unknown"
        logger.warning(
            "Rejected drone handshake with bad authentication tag",
            extra={"role": "gcs", "expected_peer": CONFIG["DRONE_HOST"], "received": peer_ip},
        )
        raise HandshakeVerifyError("drone authentication failed")

    shared_secret = server_decapsulate(ephemeral, kem_ct)
    key_send, key_recv = derive_transport_keys(
        "server",
        ephemeral.session_id,
        ephemeral.kem_name.encode("utf-8"),
        ephemeral.sig_name.encode("utf-8"),
        shared_secret,
    )
    # Return (drone→gcs key, gcs→drone key, ...)
    return key_recv, key_send, b"", b"", ephemeral.session_id, ephemeral.kem_name, ephemeral.sig_name

def client_drone_handshake(client_sock, suite, gcs_sig_public):
    # Real handshake implementation with MANDATORY signature verification
    import struct
    
    # Add socket timeout to prevent hanging
    client_sock.settimeout(10.0)
    
    # Receive server hello with length prefix
    hello_len_bytes = b""
    while len(hello_len_bytes) < 4:
        chunk = client_sock.recv(4 - len(hello_len_bytes))
        if not chunk:
            raise NotImplementedError("Connection closed reading hello length")
        hello_len_bytes += chunk
        
    hello_len = struct.unpack("!I", hello_len_bytes)[0]
    hello_wire = b""
    while len(hello_wire) < hello_len:
        chunk = client_sock.recv(hello_len - len(hello_wire))
        if not chunk:
            raise NotImplementedError("Connection closed reading hello")
        hello_wire += chunk
    
    # Parse and VERIFY server hello - NO BYPASS ALLOWED
    # This is critical for security - verification failure must abort
    hello = parse_and_verify_server_hello(hello_wire, CONFIG["WIRE_VERSION"], gcs_sig_public)

    expected_kem = suite.get("kem_name") if isinstance(suite, dict) else None
    expected_sig = suite.get("sig_name") if isinstance(suite, dict) else None
    negotiated_kem = hello.kem_name.decode("utf-8") if isinstance(hello.kem_name, bytes) else hello.kem_name
    negotiated_sig = hello.sig_name.decode("utf-8") if isinstance(hello.sig_name, bytes) else hello.sig_name
    if expected_kem and negotiated_kem != expected_kem:
        logger.error(
            "Suite mismatch",
            extra={
                "expected_kem": expected_kem,
                "expected_sig": expected_sig,
                "negotiated_kem": negotiated_kem,
                "negotiated_sig": negotiated_sig,
            },
        )
        raise HandshakeVerifyError(
            f"Downgrade attempt detected: expected {expected_kem}, got {negotiated_kem}"
        )
    if expected_sig and negotiated_sig != expected_sig:
        logger.error(
            "Suite mismatch",
            extra={
                "expected_kem": expected_kem,
                "expected_sig": expected_sig,
                "negotiated_kem": negotiated_kem,
                "negotiated_sig": negotiated_sig,
            },
        )
        raise HandshakeVerifyError(
            f"Downgrade attempt detected: expected {expected_sig}, got {negotiated_sig}"
        )
    
    # Encapsulate and send KEM ciphertext + authentication tag
    kem_ct, shared_secret = client_encapsulate(hello)
    tag = hmac.new(_drone_psk_bytes(), hello_wire, hashlib.sha256).digest()
    client_sock.sendall(struct.pack("!I", len(kem_ct)) + kem_ct + tag)
    
    # Derive transport keys
    key_send, key_recv = derive_transport_keys("client", hello.session_id, 
                                              hello.kem_name, hello.sig_name, 
                                              shared_secret)
    
    # Return in expected format (nonce seeds are unused)
    return (
        key_send,
        key_recv,
        b"",
        b"",
        hello.session_id,
        hello.kem_name.decode() if isinstance(hello.kem_name, bytes) else hello.kem_name,
        hello.sig_name.decode() if isinstance(hello.sig_name, bytes) else hello.sig_name,
    )


============================================================

FILE 6/12: logging_utils.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\logging_utils.py
Size: 2,957 bytes
Modified: 2025-09-25 23:55:52
------------------------------------------------------------
import json, logging, sys, time
from pathlib import Path

class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        payload = {
            "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(record.created)),
            "level": record.levelname,
            "name": record.name,
            "msg": record.getMessage(),
        }
        if record.exc_info:
            payload["exc_info"] = self.formatException(record.exc_info)
        # Allow extra fields via record.__dict__ (filtered)
        for k, v in record.__dict__.items():
            if k not in ("msg", "args", "exc_info", "exc_text", "stack_info", "stack_level", "created",
                         "msecs", "relativeCreated", "levelno", "levelname", "pathname", "filename",
                         "module", "lineno", "funcName", "thread", "threadName", "processName", "process"):
                try:
                    json.dumps({k: v})
                    payload[k] = v
                except Exception:
                    payload[k] = str(v)
        return json.dumps(payload)

def get_logger(name: str = "pqc") -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger
    logger.setLevel(logging.INFO)
    h = logging.StreamHandler(sys.stdout)
    h.setFormatter(JsonFormatter())
    logger.addHandler(h)
    logger.propagate = False
    return logger


def configure_file_logger(role: str, logger: logging.Logger | None = None) -> Path:
    """Attach a JSON file handler and return log path."""

    active_logger = logger or get_logger()

    # Drop any previous file handlers we attached to avoid duplicate writes during tests.
    for handler in list(active_logger.handlers):
        if getattr(handler, "_pqc_file_handler", False):
            active_logger.removeHandler(handler)
            try:
                handler.close()
            except Exception:
                pass

    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    path = logs_dir / f"{role}-{timestamp}.log"

    file_handler = logging.FileHandler(path, encoding="utf-8")
    file_handler.setFormatter(JsonFormatter())
    file_handler._pqc_file_handler = True  # type: ignore[attr-defined]
    active_logger.addHandler(file_handler)

    return path

# Very small metrics hook (no deps)
class Counter:
    def __init__(self): self.value = 0
    def inc(self, n: int = 1): self.value += n

class Gauge:
    def __init__(self): self.value = 0
    def set(self, v: float): self.value = v

class Metrics:
    def __init__(self):
        self.counters = {}
        self.gauges = {}
    def counter(self, name: str) -> Counter:
        self.counters.setdefault(name, Counter()); return self.counters[name]
    def gauge(self, name: str) -> Gauge:
        self.gauges.setdefault(name, Gauge()); return self.gauges[name]

METRICS = Metrics()

============================================================

FILE 7/12: policy_engine.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\policy_engine.py
Size: 7,034 bytes
Modified: 2025-09-27 01:00:21
------------------------------------------------------------
"""
In-band control-plane state machine for interactive rekey negotiation.

Implements a two-phase commit protocol carried over packet type 0x02 payloads.
"""

from __future__ import annotations

import queue
import secrets
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from typing import Callable, Dict, List, Optional, Tuple


def _now_ms() -> int:
    """Return monotonic milliseconds for control timestamps."""

    return time.monotonic_ns() // 1_000_000


def _default_safe() -> bool:
    return True


@dataclass
class ControlState:
    """Mutable control-plane state shared between proxy threads."""

    role: str
    current_suite: str
    safe_guard: Callable[[], bool] = field(default_factory=_default_safe)
    lock: threading.Lock = field(default_factory=threading.Lock)
    outbox: "queue.Queue[dict]" = field(default_factory=queue.Queue)
    pending: Dict[str, str] = field(default_factory=dict)
    state: str = "RUNNING"
    active_rid: Optional[str] = None
    last_rekey_ms: Optional[int] = None
    last_rekey_suite: Optional[str] = None
    last_status: Optional[Dict[str, object]] = None
    stats: Dict[str, int] = field(default_factory=lambda: {
        "prepare_sent": 0,
        "prepare_received": 0,
        "rekeys_ok": 0,
        "rekeys_fail": 0,
    })
    seen_rids: deque[str] = field(default_factory=lambda: deque(maxlen=256))


@dataclass
class ControlResult:
    """Outcome of processing a control message."""

    send: List[dict] = field(default_factory=list)
    start_handshake: Optional[Tuple[str, str]] = None  # (suite_id, rid)
    notes: List[str] = field(default_factory=list)


def create_control_state(role: str, suite_id: str, *, safe_guard: Callable[[], bool] | None = None) -> ControlState:
    """Initialise ControlState with the provided role and suite."""

    guard = safe_guard or _default_safe
    return ControlState(role=role, current_suite=suite_id, safe_guard=guard)


def generate_rid() -> str:
    """Generate a random 64-bit hex request identifier."""

    return secrets.token_hex(8)


def enqueue_json(state: ControlState, payload: dict) -> None:
    """Place an outbound JSON payload onto the control outbox."""

    state.outbox.put(payload)


def request_prepare(state: ControlState, suite_id: str) -> str:
    """Queue a prepare_rekey message and transition to NEGOTIATING."""

    rid = generate_rid()
    now = _now_ms()
    with state.lock:
        if state.state != "RUNNING":
            raise RuntimeError("control-plane already negotiating")
        state.pending[rid] = suite_id
        state.active_rid = rid
        state.state = "NEGOTIATING"
        state.stats["prepare_sent"] += 1
    enqueue_json(
        state,
        {
            "type": "prepare_rekey",
            "suite": suite_id,
            "rid": rid,
            "t_ms": now,
        },
    )
    return rid


def record_rekey_result(state: ControlState, rid: str, suite_id: str, *, success: bool) -> None:
    """Record outcome of a rekey attempt and enqueue status update."""

    now = _now_ms()
    status_payload = {
        "type": "status",
        "state": "RUNNING",
        "suite": suite_id if success else state.current_suite,
        "rid": rid,
        "result": "ok" if success else "fail",
        "t_ms": now,
    }
    with state.lock:
        if success:
            state.current_suite = suite_id
            state.last_rekey_suite = suite_id
            state.last_rekey_ms = now
            state.stats["rekeys_ok"] += 1
        else:
            state.stats["rekeys_fail"] += 1
        state.pending.pop(rid, None)
        state.active_rid = None
        state.state = "RUNNING"
    enqueue_json(state, status_payload)


def handle_control(msg: dict, role: str, state: ControlState) -> ControlResult:
    """Process inbound control JSON and return actions for the proxy."""

    result = ControlResult()
    msg_type = msg.get("type")
    if not isinstance(msg_type, str):
        result.notes.append("missing_type")
        return result

    rid = msg.get("rid")
    now = _now_ms()

    if role == "gcs":
        if msg_type == "prepare_ok" and isinstance(rid, str):
            with state.lock:
                suite = state.pending.get(rid)
                if not suite:
                    result.notes.append("unknown_rid")
                    return result
                state.state = "SWAPPING"
                state.seen_rids.append(rid)
            result.send.append({
                "type": "commit_rekey",
                "suite": suite,
                "rid": rid,
                "t_ms": now,
            })
            result.start_handshake = (suite, rid)
        elif msg_type == "prepare_fail" and isinstance(rid, str):
            reason = msg.get("reason", "unknown")
            with state.lock:
                state.pending.pop(rid, None)
                state.active_rid = None
                state.state = "RUNNING"
                state.stats["rekeys_fail"] += 1
                state.seen_rids.append(rid)
            result.notes.append(f"prepare_fail:{reason}")
        elif msg_type == "status":
            with state.lock:
                state.last_status = msg
        else:
            result.notes.append(f"ignored:{msg_type}")
        return result

    if msg_type == "prepare_rekey":
        suite = msg.get("suite")
        if not isinstance(rid, str) or not isinstance(suite, str):
            result.notes.append("invalid_prepare")
            return result

        with state.lock:
            if rid in state.seen_rids:
                allow = False
            else:
                allow = state.state == "RUNNING" and state.safe_guard()
            if allow:
                state.pending[rid] = suite
                state.active_rid = rid
                state.state = "NEGOTIATING"
                state.stats["prepare_received"] += 1
                state.seen_rids.append(rid)
        if allow:
            result.send.append({
                "type": "prepare_ok",
                "rid": rid,
                "t_ms": now,
            })
        else:
            result.send.append({
                "type": "prepare_fail",
                "rid": rid,
                "reason": "unsafe",
                "t_ms": now,
            })
    elif msg_type == "commit_rekey" and isinstance(rid, str):
        with state.lock:
            suite = state.pending.get(rid)
            if not suite:
                result.notes.append("unknown_commit_rid")
                return result
            state.state = "SWAPPING"
        result.start_handshake = (suite, rid)
    elif msg_type == "status":
        with state.lock:
            state.last_status = msg
    else:
        result.notes.append(f"ignored:{msg_type}")

    return result

============================================================

FILE 8/12: power_monitor.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\power_monitor.py
Size: 34,456 bytes
Modified: 2025-10-05 02:57:41
------------------------------------------------------------
"""High-frequency power monitoring helpers for drone follower."""

from __future__ import annotations

import csv
import math
import os
import re
import shutil
import subprocess
import threading
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Optional, Protocol

try:  # Best-effort hardware import; unavailable on dev hosts.
    import smbus  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - exercised on non-Pi hosts
    try:
        import smbus2 as smbus  # type: ignore
    except ModuleNotFoundError:  # pragma: no cover - exercised on hosts without I2C libs
        smbus = None  # type: ignore[assignment]


_DEFAULT_SAMPLE_HZ = int(os.getenv("INA219_SAMPLE_HZ", "1000"))
_DEFAULT_SHUNT_OHM = float(os.getenv("INA219_SHUNT_OHM", "0.1"))
_DEFAULT_I2C_BUS = int(os.getenv("INA219_I2C_BUS", "1"))
_DEFAULT_ADDR = int(os.getenv("INA219_ADDR", "0x40"), 16)
_DEFAULT_SIGN_MODE = os.getenv("INA219_SIGN_MODE", "auto").lower()

_RPI5_HWMON_PATH_ENV = "RPI5_HWMON_PATH"
_RPI5_HWMON_NAME_ENV = "RPI5_HWMON_NAME"
_RPI5_VOLTAGE_FILE_ENV = "RPI5_VOLTAGE_FILE"
_RPI5_CURRENT_FILE_ENV = "RPI5_CURRENT_FILE"
_RPI5_POWER_FILE_ENV = "RPI5_POWER_FILE"
_RPI5_VOLTAGE_SCALE_ENV = "RPI5_VOLTAGE_SCALE"
_RPI5_CURRENT_SCALE_ENV = "RPI5_CURRENT_SCALE"
_RPI5_POWER_SCALE_ENV = "RPI5_POWER_SCALE"

_RPI5_VOLTAGE_CANDIDATES = (
    "in0_input",
    "in1_input",
    "voltage0_input",
    "voltage1_input",
    "voltage_input",
    "vbus_input",
)

_RPI5_CURRENT_CANDIDATES = (
    "curr0_input",
    "curr1_input",
    "current0_input",
    "current1_input",
    "current_input",
    "ibus_input",
)

_RPI5_POWER_CANDIDATES = (
    "power0_input",
    "power1_input",
    "power_input",
)


# Registers and config masks from INA219 datasheet.
_CFG_BUS_RANGE_32V = 0x2000
_CFG_GAIN_8_320MV = 0x1800
_CFG_MODE_SANDBUS_CONT = 0x0007

_ADC_PROFILES = {
    "highspeed": {"badc": 0x0080, "sadc": 0x0000, "settle": 0.0004, "hz": 1100},
    "balanced": {"badc": 0x0400, "sadc": 0x0018, "settle": 0.0010, "hz": 900},
    "precision": {"badc": 0x0400, "sadc": 0x0048, "settle": 0.0020, "hz": 450},
}


@dataclass
class PowerSummary:
    """Aggregate statistics for a capture window."""

    label: str
    duration_s: float
    samples: int
    avg_current_a: float
    avg_voltage_v: float
    avg_power_w: float
    energy_j: float
    sample_rate_hz: float
    csv_path: str
    start_ns: int
    end_ns: int


@dataclass
class PowerSample:
    """Single instantaneous power sample."""

    timestamp_ns: int
    current_a: float
    voltage_v: float
    power_w: float


class PowerMonitorUnavailable(RuntimeError):
    """Raised when a power monitor backend cannot be initialised."""


class PowerMonitor(Protocol):
    sample_hz: int

    @property
    def sign_factor(self) -> int:  # pragma: no cover - protocol definition only
        ...

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:  # pragma: no cover - protocol definition only
        ...

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:  # pragma: no cover - protocol definition only
        ...


def _pick_profile(sample_hz: float) -> tuple[str, dict]:
    profile_key = os.getenv("INA219_ADC_PROFILE", "auto").lower()
    if profile_key == "auto":
        if sample_hz >= 900:
            profile_key = "highspeed"
        elif sample_hz >= 500:
            profile_key = "balanced"
        else:
            profile_key = "precision"
    return profile_key if profile_key in _ADC_PROFILES else "balanced", _ADC_PROFILES.get(profile_key, _ADC_PROFILES["balanced"])


def _sanitize_label(label: str) -> str:
    return "".join(ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in label)[:64] or "capture"


class Ina219PowerMonitor:
    """Wraps basic INA219 sampling with CSV logging and summary stats."""

    def __init__(
        self,
        output_dir: Path,
        *,
        i2c_bus: int = _DEFAULT_I2C_BUS,
        address: int = _DEFAULT_ADDR,
        shunt_ohm: float = _DEFAULT_SHUNT_OHM,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        sign_mode: str = _DEFAULT_SIGN_MODE,
    ) -> None:
        if smbus is None:
            raise PowerMonitorUnavailable("smbus module not available on host")
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.address = address
        self.shunt_ohm = shunt_ohm
        self.sample_hz = sample_hz
        self._bus = None
        self._bus_lock = threading.Lock()
        self._sign_factor = 1
        self._sign_mode = sign_mode

        try:
            self._bus = smbus.SMBus(i2c_bus)
        except Exception as exc:  # pragma: no cover - requires hardware
            raise PowerMonitorUnavailable(f"failed to open I2C bus {i2c_bus}: {exc}") from exc

        try:
            self._configure(sample_hz)
            self._sign_factor = self._resolve_sign()
        except Exception as exc:  # pragma: no cover - requires hardware
            raise PowerMonitorUnavailable(f"INA219 init failed: {exc}") from exc

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")
        if self._bus is None:
            raise PowerMonitorUnavailable("power monitor not initialised")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while True:
                elapsed = time.perf_counter() - start_perf
                if elapsed >= duration_s:
                    break
                try:
                    current_a, voltage_v = self._read_current_voltage()
                except Exception as exc:  # pragma: no cover - hardware failure path
                    raise PowerMonitorUnavailable(f"INA219 read failed: {exc}") from exc

                power_w = current_a * voltage_v
                writer.writerow([time.time_ns(), f"{current_a:.6f}", f"{voltage_v:.6f}", f"{power_w:.6f}", self._sign_factor])
                if samples % 250 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = avg_power * elapsed_s
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        if self._bus is None:
            raise PowerMonitorUnavailable("power monitor not initialised")
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            timestamp_ns = time.time_ns()
            current_a, voltage_v = self._read_current_voltage()
            power_w = current_a * voltage_v
            yield PowerSample(
                timestamp_ns=timestamp_ns,
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    def _configure(self, sample_hz: float) -> None:
        profile_key, profile = _pick_profile(sample_hz)
        cfg = (
            _CFG_BUS_RANGE_32V
            | _CFG_GAIN_8_320MV
            | profile["badc"]
            | profile["sadc"]
            | _CFG_MODE_SANDBUS_CONT
        )
        payload = [(cfg >> 8) & 0xFF, cfg & 0xFF]
        with self._bus_lock:
            self._bus.write_i2c_block_data(self.address, 0x00, payload)  # type: ignore[union-attr]
        time.sleep(profile["settle"])

    def _resolve_sign(self) -> int:
        mode = self._sign_mode
        if mode.startswith("pos"):
            return 1
        if mode.startswith("neg"):
            return -1
        probe_deadline = time.time() + float(os.getenv("INA219_SIGN_PROBE_SEC", "2"))
        readings = []
        while time.time() < probe_deadline:
            vsh = self._read_shunt_voltage()
            readings.append(vsh)
            time.sleep(0.005)
        if not readings:
            return 1
        readings.sort()
        median = readings[len(readings) // 2]
        return -1 if median < -20e-6 else 1

    def _read_current_voltage(self) -> tuple[float, float]:
        vsh = self._read_shunt_voltage()
        current = (vsh / self.shunt_ohm) * self._sign_factor
        voltage = self._read_bus_voltage()
        return current, voltage

    def _read_shunt_voltage(self) -> float:
        raw = self._read_s16(0x01)
        return raw * 10e-6

    def _read_bus_voltage(self) -> float:
        raw = self._read_u16(0x02)
        return ((raw >> 3) & 0x1FFF) * 0.004

    def _read_u16(self, register: int) -> int:
        with self._bus_lock:
            hi, lo = self._bus.read_i2c_block_data(self.address, register, 2)  # type: ignore[union-attr]
        return (hi << 8) | lo

    def _read_s16(self, register: int) -> int:
        val = self._read_u16(register)
        if val & 0x8000:
            val -= 1 << 16
        return val


class Rpi5PowerMonitor:
    """Power monitor backend using Raspberry Pi 5 onboard telemetry via hwmon."""

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        sign_mode: str = _DEFAULT_SIGN_MODE,
        hwmon_path: Optional[str] = None,
        hwmon_name_hint: Optional[str] = None,
        voltage_file: Optional[str] = None,
        current_file: Optional[str] = None,
        power_file: Optional[str] = None,
        voltage_scale: Optional[float] = None,
        current_scale: Optional[float] = None,
        power_scale: Optional[float] = None,
    ) -> None:
        del sign_mode  # Pi 5 telemetry reports already-correct sign
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self._sign_factor = 1
        self._hwmon_dir = self._find_hwmon_dir(hwmon_path, hwmon_name_hint, strict=True)
        self._voltage_path, self._current_path, self._power_path = self._resolve_channels(
            voltage_file,
            current_file,
            power_file,
        )
        self._voltage_scale = self._resolve_scale(voltage_scale, _RPI5_VOLTAGE_SCALE_ENV, 1e-6)
        self._current_scale = self._resolve_scale(current_scale, _RPI5_CURRENT_SCALE_ENV, 1e-6)
        self._power_scale = self._resolve_scale(power_scale, _RPI5_POWER_SCALE_ENV, 1e-6)

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while True:
                elapsed = time.perf_counter() - start_perf
                if elapsed >= duration_s:
                    break
                current_a, voltage_v, power_w = self._read_measurements()
                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}",
                    f"{voltage_v:.6f}",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 250 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = avg_power * elapsed_s
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            timestamp_ns = time.time_ns()
            current_a, voltage_v, power_w = self._read_measurements()
            yield PowerSample(
                timestamp_ns=timestamp_ns,
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    @staticmethod
    def is_supported(
        hwmon_path: Optional[str] = None,
        hwmon_name_hint: Optional[str] = None,
    ) -> bool:
        try:
            return Rpi5PowerMonitor._find_hwmon_dir(hwmon_path, hwmon_name_hint, strict=False) is not None
        except PowerMonitorUnavailable:
            return False

    @staticmethod
    def _find_hwmon_dir(
        hwmon_path: Optional[str],
        hwmon_name_hint: Optional[str],
        *,
        strict: bool,
    ) -> Optional[Path]:
        candidates = []
        if hwmon_path:
            candidates.append(hwmon_path)
        env_path = os.getenv(_RPI5_HWMON_PATH_ENV)
        if env_path:
            candidates.append(env_path)

        for candidate in candidates:
            path = Path(candidate).expanduser()
            if path.is_dir():
                return path
            if strict:
                raise PowerMonitorUnavailable(f"hwmon path not found: {path}")

        hwmon_root = Path("/sys/class/hwmon")
        if not hwmon_root.exists():
            if strict:
                raise PowerMonitorUnavailable("/sys/class/hwmon not present on host")
            return None

        hint_source = hwmon_name_hint or os.getenv(_RPI5_HWMON_NAME_ENV) or ""
        hints = [part.strip().lower() for part in hint_source.split(",") if part.strip()]

        for entry in sorted(hwmon_root.iterdir()):
            name_file = entry / "name"
            try:
                name_value = name_file.read_text().strip().lower()
            except Exception:
                continue
            if not name_value:
                continue
            if hints:
                if any(hint in name_value for hint in hints):
                    return entry
            else:
                if "rpi" in name_value and (
                    "power" in name_value
                    or "pmic" in name_value
                    or "monitor" in name_value
                    or "volt" in name_value
                ):
                    return entry

        if strict:
            raise PowerMonitorUnavailable("unable to locate Raspberry Pi power hwmon device")
        return None

    def _resolve_channels(
        self,
        voltage_file: Optional[str],
        current_file: Optional[str],
        power_file: Optional[str],
    ) -> tuple[Path, Path, Optional[Path]]:
        search_dirs = [self._hwmon_dir]
        device_dir = self._hwmon_dir / "device"
        if device_dir.is_dir():
            search_dirs.append(device_dir)

        def pick(
            defaults: tuple[str, ...],
            override: Optional[str],
            env_var: str,
            *,
            required: bool,
        ) -> Optional[Path]:
            # Prefer explicit override paths first.
            if override:
                override_path = Path(override)
                if override_path.is_absolute() or override_path.exists():
                    if override_path.exists():
                        return override_path
                    if required:
                        raise PowerMonitorUnavailable(f"override channel path not found: {override_path}")
                else:
                    for base in search_dirs:
                        candidate = base / override
                        if candidate.exists():
                            return candidate
                    if required:
                        raise PowerMonitorUnavailable(f"override channel name not found: {override}")

            env_override = os.getenv(env_var)
            if env_override:
                for token in env_override.split(","):
                    name = token.strip()
                    if not name:
                        continue
                    env_path = Path(name)
                    if env_path.is_absolute() or env_path.exists():
                        if env_path.exists():
                            return env_path
                        continue
                    for base in search_dirs:
                        candidate = base / name
                        if candidate.exists():
                            return candidate

            for name in defaults:
                for base in search_dirs:
                    candidate = base / name
                    if candidate.exists():
                        return candidate

            if required:
                raise PowerMonitorUnavailable(f"missing required hwmon channel {defaults[0] if defaults else 'unknown'}")
            return None

        voltage_path = pick(_RPI5_VOLTAGE_CANDIDATES, voltage_file, _RPI5_VOLTAGE_FILE_ENV, required=True)
        current_path = pick(_RPI5_CURRENT_CANDIDATES, current_file, _RPI5_CURRENT_FILE_ENV, required=True)
        power_path = pick(_RPI5_POWER_CANDIDATES, power_file, _RPI5_POWER_FILE_ENV, required=False)
        if voltage_path is None or current_path is None:
            raise PowerMonitorUnavailable("incomplete hwmon channel mapping")
        return voltage_path, current_path, power_path

    def _read_measurements(self) -> tuple[float, float, float]:
        voltage_v = self._read_channel(self._voltage_path, self._voltage_scale)
        current_a = self._read_channel(self._current_path, self._current_scale)
        if self._power_path is not None:
            power_w = self._read_channel(self._power_path, self._power_scale)
        else:
            power_w = voltage_v * current_a
        return current_a, voltage_v, power_w

    def _read_channel(self, path: Path, scale: float) -> float:
        try:
            raw = path.read_text().strip()
        except FileNotFoundError as exc:
            raise PowerMonitorUnavailable(f"hwmon channel missing: {path}") from exc
        except PermissionError as exc:  # pragma: no cover - depends on host permissions
            raise PowerMonitorUnavailable(f"insufficient permissions for {path}") from exc
        if not raw:
            raise PowerMonitorUnavailable(f"empty hwmon reading from {path}")
        try:
            value = float(raw)
        except ValueError as exc:
            raise PowerMonitorUnavailable(f"invalid hwmon reading from {path}: {raw!r}") from exc
        return value * scale

    def _resolve_scale(self, explicit: Optional[float], env_name: str, default: float) -> float:
        if explicit is not None:
            return explicit
        raw = os.getenv(env_name)
        if raw is None or raw == "":
            return default
        try:
            return float(raw)
        except ValueError as exc:
            raise PowerMonitorUnavailable(f"invalid {env_name} value: {raw!r}") from exc


class Rpi5PmicPowerMonitor:
    """Power monitor backend using Raspberry Pi 5 PMIC telemetry via `vcgencmd`."""

    _RAIL_PATTERN = re.compile(
        r"^\s*(?P<name>[A-Z0-9_]+)\s+(?P<kind>current|volt)\(\d+\)=(?P<value>[0-9.]+)(?P<unit>A|V)\s*$"
    )

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = 10,
        sign_mode: str = "auto",
    ) -> None:
        del sign_mode  # PMIC telemetry is unsigned
        if sample_hz <= 0 or sample_hz > 20:
            raise PowerMonitorUnavailable("rpi5-pmic sample_hz must be between 1 and 20")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self._sign_factor = 1

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")
        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while (time.perf_counter() - start_perf) < duration_s:
                rails = self._read_once()
                voltage_v = self._choose_voltage(rails)
                power_w = self._sum_power(rails)
                current_a = self._derive_current(power_w, voltage_v)

                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}" if not math.isnan(current_a) else "nan",
                    f"{voltage_v:.6f}" if not math.isnan(voltage_v) else "nan",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 10 == 0:
                    handle.flush()

                if not math.isnan(current_a):
                    sum_current += current_a
                if not math.isnan(voltage_v):
                    sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick = start_perf + samples * dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed = max(end_perf - start_perf, 1e-9)
        avg_current = (sum_current / samples) if samples else 0.0
        avg_voltage = (sum_voltage / samples) if samples else 0.0
        avg_power = (sum_power / samples) if samples else 0.0
        energy_j = avg_power * elapsed
        sample_rate = samples / elapsed if elapsed > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        start_perf = time.perf_counter()
        samples = 0
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            rails = self._read_once()
            voltage_v = self._choose_voltage(rails)
            power_w = self._sum_power(rails)
            current_a = self._derive_current(power_w, voltage_v)

            yield PowerSample(
                timestamp_ns=time.time_ns(),
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )

            samples += 1
            next_tick = start_perf + samples * dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    def _read_once(self) -> dict[str, dict[str, Optional[float]]]:
        try:
            output = subprocess.check_output(["vcgencmd", "pmic_read_adc"], text=True, timeout=1.0)
        except FileNotFoundError as exc:
            raise PowerMonitorUnavailable("vcgencmd not found; install raspberrypi-userland") from exc
        except subprocess.SubprocessError as exc:
            raise PowerMonitorUnavailable(f"vcgencmd pmic_read_adc failed: {exc}") from exc

        rails: dict[str, dict[str, Optional[float]]] = {}
        for line in output.splitlines():
            match = self._RAIL_PATTERN.match(line)
            if not match:
                continue
            name = match.group("name")
            kind = match.group("kind")
            value = float(match.group("value"))
            rail = rails.setdefault(name, {"current_a": None, "voltage_v": None})
            if kind == "current":
                rail["current_a"] = value
            else:
                rail["voltage_v"] = value
        if not rails:
            raise PowerMonitorUnavailable("pmic_read_adc returned no rail telemetry")
        return rails

    def _sum_power(self, rails: dict[str, dict[str, Optional[float]]]) -> float:
        total = 0.0
        for rail in rails.values():
            current_a = rail.get("current_a")
            voltage_v = rail.get("voltage_v")
            if current_a is None or voltage_v is None:
                continue
            total += current_a * voltage_v
        return total

    def _choose_voltage(self, rails: dict[str, dict[str, Optional[float]]]) -> float:
        ext5 = rails.get("EXT5V_V", {}).get("voltage_v") if "EXT5V_V" in rails else None
        if ext5 is not None and ext5 > 0:
            return ext5
        return max((rail.get("voltage_v") or float("nan") for rail in rails.values()), default=float("nan"))

    def _derive_current(self, power_w: float, voltage_v: float) -> float:
        if math.isnan(voltage_v) or voltage_v <= 0:
            return float("nan")
        return power_w / voltage_v


def create_power_monitor(
    output_dir: Path,
    *,
    backend: str = "auto",
    sample_hz: Optional[int] = None,
    sign_mode: Optional[str] = None,
    shunt_ohm: Optional[float] = None,
    i2c_bus: Optional[int] = None,
    address: Optional[int] = None,
    hwmon_path: Optional[str] = None,
    hwmon_name_hint: Optional[str] = None,
    voltage_file: Optional[str] = None,
    current_file: Optional[str] = None,
    power_file: Optional[str] = None,
    voltage_scale: Optional[float] = None,
    current_scale: Optional[float] = None,
    power_scale: Optional[float] = None,
) -> PowerMonitor:
    resolved_backend = (backend or "auto").lower()
    env_backend = os.getenv("POWER_MONITOR_BACKEND")
    if resolved_backend == "auto" and env_backend:
        resolved_backend = env_backend.lower()

    resolved_sample_hz = int(sample_hz if sample_hz is not None else _DEFAULT_SAMPLE_HZ)
    resolved_sign_mode = (sign_mode or _DEFAULT_SIGN_MODE).lower()
    resolved_shunt = float(shunt_ohm if shunt_ohm is not None else _DEFAULT_SHUNT_OHM)
    resolved_i2c_bus = int(i2c_bus if i2c_bus is not None else _DEFAULT_I2C_BUS)
    resolved_address = address if address is not None else _DEFAULT_ADDR
    if isinstance(resolved_address, str):
        resolved_address = int(resolved_address, 0)

    ina_kwargs = {
        "i2c_bus": resolved_i2c_bus,
        "address": resolved_address,
        "shunt_ohm": resolved_shunt,
        "sample_hz": resolved_sample_hz,
        "sign_mode": resolved_sign_mode,
    }
    rpi_kwargs = {
        "sample_hz": resolved_sample_hz,
        "sign_mode": resolved_sign_mode,
        "hwmon_path": hwmon_path,
        "hwmon_name_hint": hwmon_name_hint,
        "voltage_file": voltage_file,
        "current_file": current_file,
        "power_file": power_file,
        "voltage_scale": voltage_scale,
        "current_scale": current_scale,
        "power_scale": power_scale,
    }

    if resolved_backend == "ina219":
        return Ina219PowerMonitor(output_dir, **ina_kwargs)
    if resolved_backend == "rpi5":
        return Rpi5PowerMonitor(output_dir, **rpi_kwargs)
    if resolved_backend == "rpi5-pmic":
        return Rpi5PmicPowerMonitor(output_dir, sample_hz=resolved_sample_hz, sign_mode=resolved_sign_mode)
    if resolved_backend != "auto":
        raise ValueError(f"unknown power monitor backend: {backend}")

    rpi_error: Optional[PowerMonitorUnavailable] = None
    pmic_error: Optional[PowerMonitorUnavailable] = None
    if Rpi5PowerMonitor.is_supported(hwmon_path=hwmon_path, hwmon_name_hint=hwmon_name_hint):
        try:
            return Rpi5PowerMonitor(output_dir, **rpi_kwargs)
        except PowerMonitorUnavailable as exc:
            rpi_error = exc

    if shutil.which("vcgencmd"):
        try:
            return Rpi5PmicPowerMonitor(output_dir, sample_hz=resolved_sample_hz, sign_mode=resolved_sign_mode)
        except PowerMonitorUnavailable as exc:
            pmic_error = exc

    try:
        return Ina219PowerMonitor(output_dir, **ina_kwargs)
    except PowerMonitorUnavailable as exc:
        if pmic_error is not None:
            raise pmic_error
        if rpi_error is not None:
            raise rpi_error
        raise


__all__ = [
    "Ina219PowerMonitor",
    "Rpi5PowerMonitor",
    "Rpi5PmicPowerMonitor",
    "PowerMonitor",
    "PowerSummary",
    "PowerSample",
    "PowerMonitorUnavailable",
    "create_power_monitor",
]

============================================================

FILE 9/12: project_config.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\project_config.py
Size: 168 bytes
Modified: 2025-09-24 15:32:25
------------------------------------------------------------
# Thin shim so planned path 'project_config.py' exists without breaking tests.
# Source of truth remains core/config.py
from .config import CONFIG
__all__ = ["CONFIG"]

============================================================

FILE 10/12: run_proxy.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\run_proxy.py
Size: 20,487 bytes
Modified: 2025-09-27 01:00:21
------------------------------------------------------------
"""
Unified CLI entrypoint for the PQC drone-GCS proxy.

Supports subcommands:
- init-identity: Create persistent GCS signing identity
- gcs: Start GCS proxy (requires secret key by default)  
- drone: Start drone proxy (requires GCS public key)

Uses persistent file-based keys by default for production security.
"""

import sys
import argparse
import signal
import os
import json
import time
import logging
from pathlib import Path
from typing import Optional

from core.config import CONFIG
from core.suites import get_suite, build_suite_id
from core.logging_utils import get_logger, configure_file_logger

logger = get_logger("pqc")


def _require_signature_class():
    """Lazily import oqs Signature and provide a friendly error if missing."""

    try:
        from oqs.oqs import Signature  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - exercised via CLI
        if exc.name in {"oqs", "oqs.oqs"}:
            print(
                "Error: oqs-python is required for cryptographic operations. "
                "Install it with 'pip install oqs-python' or activate the project environment."
            )
            sys.exit(1)
        raise

    return Signature


def _require_run_proxy():
    """Import run_proxy only when needed, surfacing helpful guidance on failure."""

    try:
        from core.async_proxy import run_proxy as _run_proxy  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - exercised via CLI
        if exc.name in {"oqs", "oqs.oqs"}:
            print(
                "Error: oqs-python is required to start the proxy. "
                "Install it with 'pip install oqs-python' or activate the project environment."
            )
            sys.exit(1)
        raise

    return _run_proxy


def signal_handler(signum, frame):
    """Handle interrupt signals gracefully."""
    print("\nReceived interrupt signal. Shutting down...")
    sys.exit(0)


def create_secrets_dir():
    """Create secrets directory if it doesn't exist."""
    secrets_dir = Path("secrets")
    secrets_dir.mkdir(exist_ok=True)
    return secrets_dir


def write_json_report(json_path: Optional[str], payload: dict, *, quiet: bool = False) -> None:
    """Persist counters payload to JSON if a path is provided."""

    if not json_path:
        return

    try:
        path = Path(json_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        if not quiet:
            print(f"Wrote JSON report to {path}")
    except Exception as exc:
        print(f"Warning: Failed to write JSON output to {json_path}: {exc}")


def _resolve_suite(args, role_label: str) -> dict:
    """Resolve suite via legacy --suite or new --kem/--aead/--sig components."""

    suite_arg = getattr(args, "suite", None)
    kem = getattr(args, "kem", None)
    sig = getattr(args, "sig", None)
    aead = getattr(args, "aead", None)

    if suite_arg and any(v is not None for v in (kem, sig, aead)):
        print("Error: --suite cannot be combined with --kem/--sig/--aead")
        sys.exit(1)

    try:
        if suite_arg:
            suite = get_suite(suite_arg)
        elif any(v is not None for v in (kem, sig, aead)):
            if not all(v is not None for v in (kem, sig, aead)):
                print("Error: --kem, --sig, and --aead must be provided together")
                sys.exit(1)
            suite_id = build_suite_id(kem, aead, sig)
            suite = get_suite(suite_id)
        else:
            print(f"Error: {role_label} requires --suite or --kem/--sig/--aead")
            sys.exit(1)
    except NotImplementedError as exc:
        print(f"Error: {exc}")
        sys.exit(1)

    # Normalize suite argument for downstream logging
    setattr(args, "suite", suite.get("suite_id", getattr(args, "suite", None)))
    return suite


def init_identity_command(args):
    """Create GCS signing identity and save to persistent files."""
    # Use custom output_dir if provided, otherwise default secrets directory
    if hasattr(args, 'output_dir') and args.output_dir:
        secrets_dir = Path(args.output_dir)
        secrets_dir.mkdir(parents=True, exist_ok=True)
    else:
        secrets_dir = create_secrets_dir()
    
    try:
        suite = get_suite(args.suite) if hasattr(args, 'suite') and args.suite else get_suite("cs-kyber768-aesgcm-dilithium3")
    except KeyError as e:
        print(f"Error: Unknown suite: {args.suite if hasattr(args, 'suite') else 'default'}")
        sys.exit(1)
    
    secret_path = secrets_dir / "gcs_signing.key"
    public_path = secrets_dir / "gcs_signing.pub"
    
    if secret_path.exists() or public_path.exists():
        print("Warning: Identity files already exist. Overwriting with a new keypair.")
    
    Signature = _require_signature_class()

    try:
        sig = Signature(suite["sig_name"])
        if hasattr(sig, 'export_secret_key'):
            gcs_sig_public = sig.generate_keypair()
            gcs_sig_secret = sig.export_secret_key()
            
            # Write files with appropriate permissions
            secret_path.write_bytes(gcs_sig_secret)
            public_path.write_bytes(gcs_sig_public)
            
            # Secure the secret file
            try:
                os.chmod(secret_path, 0o600)
            except Exception:
                pass  # Best effort on Windows
                
            print(f"Created GCS signing identity:")
            print(f"  Secret: {secret_path}")
            print(f"  Public: {public_path}")
            print(f"  Public key (hex): {gcs_sig_public.hex()}")
            return 0  # Success
            
        else:
            print("Error: oqs build lacks key import/export; use --ephemeral or upgrade oqs-python.")
            sys.exit(1)
            
    except Exception as e:
        print(f"Error creating identity: {e}")
        sys.exit(1)


def gcs_command(args):
    """Start GCS proxy."""
    suite = _resolve_suite(args, "GCS proxy")
    suite_id = suite["suite_id"]
    
    Signature = _require_signature_class()
    proxy_runner = _require_run_proxy()

    gcs_sig_secret = None
    gcs_sig_public = None
    json_out_path = getattr(args, "json_out", None)
    quiet = getattr(args, "quiet", False)
    status_file = getattr(args, "status_file", None)

    def info(msg: str) -> None:
        if not quiet:
            print(msg)
    
    if args.ephemeral:
        info("⚠️  WARNING: Using EPHEMERAL keys - not suitable for production!")
        info("⚠️  Key will be lost when process exits.")
        if not quiet:
            print()
        
        # Generate ephemeral keypair
        sig = Signature(suite["sig_name"])
        gcs_sig_public = sig.generate_keypair()
        gcs_sig_secret = sig
        info("Generated ephemeral GCS signing keypair:")
        if not quiet:
            print(f"Public key (hex): {gcs_sig_public.hex()}")
            print("Provide this to the drone via --gcs-pub-hex or --peer-pubkey-file")
            print()
        
    else:
        # Load persistent key
        if args.gcs_secret_file:
            secret_path = Path(args.gcs_secret_file)
        else:
            secret_path = Path("secrets/gcs_signing.key")
            
        if not secret_path.exists():
            print(f"Error: Secret key file not found: {secret_path}")
            print("Run 'python -m core.run_proxy init-identity' to create one,")
            print("or use --ephemeral for development only.")
            sys.exit(1)
            
        secret_bytes = None
        try:
            secret_bytes = secret_path.read_bytes()
        except Exception as exc:
            print(f"Error reading secret key file: {exc}")
            sys.exit(1)

        load_errors = []
        imported_public: Optional[bytes] = None
        load_method: Optional[str] = None

        try:
            primary_sig = Signature(suite["sig_name"])
        except Exception as exc:
            load_errors.append(f"Signature ctor failed: {exc}")
            primary_sig = None  # type: ignore

        if primary_sig is not None and hasattr(primary_sig, "import_secret_key"):
            try:
                imported_public = primary_sig.import_secret_key(secret_bytes)
                gcs_sig_secret = primary_sig
                load_method = "import_secret_key"
            except Exception as exc:
                load_errors.append(f"import_secret_key failed: {exc}")

        if gcs_sig_secret is None:
            try:
                fallback_sig = Signature(suite["sig_name"], secret_key=secret_bytes)
                gcs_sig_secret = fallback_sig
                load_method = "ctor_secret_key"
            except TypeError as exc:
                load_errors.append(f"ctor secret_key unsupported: {exc}")
            except Exception as exc:
                load_errors.append(f"ctor secret_key failed: {exc}")

        if gcs_sig_secret is None:
            print("Error: oqs build lacks usable key import. Tried import_secret_key and constructor fallback without success.")
            if load_errors:
                print("Details:")
                for err in load_errors:
                    print(f"  - {err}")
            print("Consider running with --ephemeral or upgrading oqs-python/liboqs with key import support.")
            sys.exit(1)

        info("Loaded GCS signing key from file.")
        if load_method == "ctor_secret_key":
            info("Using constructor-based fallback because import/export APIs are unavailable.")

        gcs_sig_public = imported_public
        if gcs_sig_public is None:
            public_candidates = []
            if secret_path.suffix:
                public_candidates.append(secret_path.with_suffix(".pub"))
            public_candidates.append(secret_path.parent / "gcs_signing.pub")
            seen = set()
            for candidate in public_candidates:
                key = str(candidate.resolve()) if candidate.exists() else str(candidate)
                if key in seen:
                    continue
                seen.add(key)
                if candidate.exists():
                    try:
                        gcs_sig_public = candidate.read_bytes()
                        info(f"Loaded public key from {candidate}.")
                    except Exception as exc:
                        load_errors.append(f"public key read failed ({candidate}): {exc}")
                    break

        if gcs_sig_public is not None and not quiet:
            print(f"Public key (hex): {gcs_sig_public.hex()}")
        elif gcs_sig_public is None and not quiet:
            print("Warning: Could not locate public key file for display. Ensure the drone has the matching public key.")
        if not quiet:
            print()
    
    try:
        log_path = configure_file_logger("gcs", logger)
        if not quiet:
            print(f"Log file: {log_path}")

        info(f"Starting GCS proxy with suite {suite_id}")
        if args.stop_seconds:
            info(f"Will auto-stop after {args.stop_seconds} seconds")
        if not quiet:
            print()
        
        counters = proxy_runner(
            role="gcs",
            suite=suite,
            cfg=CONFIG,
            gcs_sig_secret=gcs_sig_secret,
            gcs_sig_public=None,
            stop_after_seconds=args.stop_seconds,
            manual_control=getattr(args, "control_manual", False),
            quiet=quiet,
            status_file=status_file,
        )
        
        # Log final counters as JSON
        logger.info("GCS proxy shutdown", extra={"counters": counters})
        
        if not quiet:
            print("GCS proxy stopped. Final counters:")
            for key, value in counters.items():
                print(f"  {key}: {value}")

        payload = {
            "role": "gcs",
            "suite": suite_id,
            "counters": counters,
            "ts_stop_ns": time.time_ns(),
        }
        write_json_report(json_out_path, payload, quiet=quiet)
            
    except KeyboardInterrupt:
        if not quiet:
            print("\nGCS proxy stopped by user.")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


def drone_command(args):
    """Start drone proxy."""
    suite = _resolve_suite(args, "Drone proxy")
    suite_id = suite["suite_id"]
    
    proxy_runner = _require_run_proxy()

    # Get GCS public key
    gcs_sig_public = None
    json_out_path = getattr(args, "json_out", None)
    quiet = getattr(args, "quiet", False)
    status_file = getattr(args, "status_file", None)

    def info(msg: str) -> None:
        if not quiet:
            print(msg)
    
    try:
        if args.peer_pubkey_file:
            pub_path = Path(args.peer_pubkey_file)
            if not pub_path.exists():
                raise FileNotFoundError(f"Public key file not found: {pub_path}")
            gcs_sig_public = pub_path.read_bytes()
        elif args.gcs_pub_hex:
            gcs_sig_public = bytes.fromhex(args.gcs_pub_hex)
        else:
            # Try default location
            default_pub = Path("secrets/gcs_signing.pub")
            if default_pub.exists():
                gcs_sig_public = default_pub.read_bytes()
                info(f"Using GCS public key from: {default_pub}")
            else:
                raise ValueError("No GCS public key provided. Use --peer-pubkey-file, --gcs-pub-hex, or ensure secrets/gcs_signing.pub exists.")
                
    except Exception as e:
        print(f"Error loading GCS public key: {e}")
        sys.exit(1)
    
    try:
        log_path = configure_file_logger("drone", logger)
        if not quiet:
            print(f"Log file: {log_path}")

        info(f"Starting drone proxy with suite {suite_id}")
        if args.stop_seconds:
            info(f"Will auto-stop after {args.stop_seconds} seconds")
        if not quiet:
            print()
        
        counters = proxy_runner(
            role="drone",
            suite=suite,
            cfg=CONFIG,
            gcs_sig_secret=None,
            gcs_sig_public=gcs_sig_public,
            stop_after_seconds=args.stop_seconds,
            manual_control=False,
            quiet=quiet,
            status_file=status_file,
        )
        
        # Log final counters as JSON
        logger.info("Drone proxy shutdown", extra={"counters": counters})
        
        if not quiet:
            print("Drone proxy stopped. Final counters:")
            for key, value in counters.items():
                print(f"  {key}: {value}")

        payload = {
            "role": "drone",
            "suite": suite_id,
            "counters": counters,
            "ts_stop_ns": time.time_ns(),
        }
        write_json_report(json_out_path, payload, quiet=quiet)
            
    except KeyboardInterrupt:
        if not quiet:
            print("\nDrone proxy stopped by user.")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


def main():
    """Main CLI entrypoint with subcommands."""
    # Set up signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)
    
    parser = argparse.ArgumentParser(description="PQC Drone-GCS Secure Proxy")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # init-identity subcommand
    init_parser = subparsers.add_parser('init-identity', 
                                       help='Create persistent GCS signing identity')
    init_parser.add_argument("--suite", default="cs-kyber768-aesgcm-dilithium3",
                            help="Cryptographic suite ID (default: cs-kyber768-aesgcm-dilithium3)")
    init_parser.add_argument("--output-dir", 
                            help="Directory for key files (default: secrets/)")
    
    # gcs subcommand
    gcs_parser = subparsers.add_parser('gcs', help='Start GCS proxy')
    gcs_parser.add_argument("--suite",
                           help="Cryptographic suite ID (e.g., cs-kyber768-aesgcm-dilithium3)")
    gcs_parser.add_argument("--kem",
                           help="KEM alias (e.g., ML-KEM-768, kyber768)")
    gcs_parser.add_argument("--aead",
                           help="AEAD alias (e.g., AES-GCM)")
    gcs_parser.add_argument("--sig",
                           help="Signature alias (e.g., ML-DSA-65, dilithium3)")
    gcs_parser.add_argument("--gcs-secret-file",
                           help="Path to GCS secret key file (default: secrets/gcs_signing.key)")
    gcs_parser.add_argument("--ephemeral", action='store_true',
                           help="Use ephemeral keys (development only - prints warning)")
    gcs_parser.add_argument("--stop-seconds", type=float,
                           help="Auto-stop after N seconds (for testing)")
    gcs_parser.add_argument("--quiet", action="store_true",
                           help="Suppress informational prints (warnings/errors still shown)")
    gcs_parser.add_argument("--json-out",
                           help="Optional path to write counters JSON on shutdown")
    gcs_parser.add_argument("--control-manual", action="store_true",
                           help="Enable interactive manual in-band rekey control thread")
    gcs_parser.add_argument("--status-file",
                           help="Path to write proxy status JSON updates (handshake/rekey)")
    
    # drone subcommand
    drone_parser = subparsers.add_parser('drone', help='Start drone proxy')
    drone_parser.add_argument("--suite",
                             help="Cryptographic suite ID (e.g., cs-kyber768-aesgcm-dilithium3)")
    drone_parser.add_argument("--kem",
                             help="KEM alias (e.g., ML-KEM-768, kyber768)")
    drone_parser.add_argument("--aead",
                             help="AEAD alias (e.g., AES-GCM)")
    drone_parser.add_argument("--sig",
                             help="Signature alias (e.g., ML-DSA-65, dilithium3)")
    drone_parser.add_argument("--peer-pubkey-file",
                             help="Path to GCS public key file (default: secrets/gcs_signing.pub)")
    drone_parser.add_argument("--gcs-pub-hex",
                             help="GCS public key as hex string")
    drone_parser.add_argument("--stop-seconds", type=float,
                             help="Auto-stop after N seconds (for testing)")
    drone_parser.add_argument("--quiet", action="store_true",
                              help="Suppress informational prints (warnings/errors still shown)")
    drone_parser.add_argument("--json-out",
                              help="Optional path to write counters JSON on shutdown")
    drone_parser.add_argument("--status-file",
                              help="Path to write proxy status JSON updates (handshake/rekey)")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Validate required CONFIG keys
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX", 
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX", 
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW"
    ]
    
    missing_keys = [key for key in required_keys if key not in CONFIG]
    if missing_keys:
        print(f"Error: CONFIG missing required keys: {', '.join(missing_keys)}")
        sys.exit(1)
    
    # Route to appropriate command handler
    if args.command == 'init-identity':
        init_identity_command(args)
    elif args.command == 'gcs':
        if getattr(args, "quiet", False):
            logger.setLevel(logging.WARNING)
        gcs_command(args)
    elif args.command == 'drone':
        if getattr(args, "quiet", False):
            logger.setLevel(logging.WARNING)
        drone_command(args)


if __name__ == "__main__":
    main()

============================================================

FILE 11/12: suites.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\suites.py
Size: 11,618 bytes
Modified: 2025-09-28 03:39:31
------------------------------------------------------------
"""PQC cryptographic suite registry and algorithm ID mapping.

Provides a composable {KEM × AEAD × SIG} registry with synonym resolution and
helpers for querying oqs availability.
"""

from __future__ import annotations

from itertools import product
from types import MappingProxyType
from typing import Dict, Iterable, Tuple


def _normalize_alias(value: str) -> str:
    """Normalize alias strings for case- and punctuation-insensitive matching."""

    return "".join(ch for ch in value.lower() if ch.isalnum())


_KEM_REGISTRY = {
    "mlkem512": {
        "oqs_name": "ML-KEM-512",
        "token": "mlkem512",
        "nist_level": "L1",
        "kem_id": 1,
        "kem_param_id": 1,
        "aliases": (
            "ML-KEM-512",
            "ml-kem-512",
            "mlkem512",
            "kyber512",
            "kyber-512",
            "kyber_512",
        ),
    },
    "mlkem768": {
        "oqs_name": "ML-KEM-768",
        "token": "mlkem768",
        "nist_level": "L3",
        "kem_id": 1,
        "kem_param_id": 2,
        "aliases": (
            "ML-KEM-768",
            "ml-kem-768",
            "mlkem768",
            "kyber768",
            "kyber-768",
            "kyber_768",
        ),
    },
    "mlkem1024": {
        "oqs_name": "ML-KEM-1024",
        "token": "mlkem1024",
        "nist_level": "L5",
        "kem_id": 1,
        "kem_param_id": 3,
        "aliases": (
            "ML-KEM-1024",
            "ml-kem-1024",
            "mlkem1024",
            "kyber1024",
            "kyber-1024",
            "kyber_1024",
        ),
    },
}


_SIG_REGISTRY = {
    "mldsa44": {
        "oqs_name": "ML-DSA-44",
        "token": "mldsa44",
        "sig_id": 1,
        "sig_param_id": 1,
        "aliases": (
            "ML-DSA-44",
            "ml-dsa-44",
            "mldsa44",
            "dilithium2",
            "dilithium-2",
        ),
    },
    "mldsa65": {
        "oqs_name": "ML-DSA-65",
        "token": "mldsa65",
        "sig_id": 1,
        "sig_param_id": 2,
        "aliases": (
            "ML-DSA-65",
            "ml-dsa-65",
            "mldsa65",
            "dilithium3",
            "dilithium-3",
        ),
    },
    "mldsa87": {
        "oqs_name": "ML-DSA-87",
        "token": "mldsa87",
        "sig_id": 1,
        "sig_param_id": 3,
        "aliases": (
            "ML-DSA-87",
            "ml-dsa-87",
            "mldsa87",
            "dilithium5",
            "dilithium-5",
        ),
    },
    "falcon512": {
        "oqs_name": "Falcon-512",
        "token": "falcon512",
        "sig_id": 2,
        "sig_param_id": 1,
        "aliases": (
            "Falcon-512",
            "falcon512",
            "falcon-512",
        ),
    },
    "falcon1024": {
        "oqs_name": "Falcon-1024",
        "token": "falcon1024",
        "sig_id": 2,
        "sig_param_id": 2,
        "aliases": (
            "Falcon-1024",
            "falcon1024",
            "falcon-1024",
        ),
    },
    "sphincs128fsha2": {
        "oqs_name": "SPHINCS+-SHA2-128f-simple",
        "token": "sphincs128fsha2",
        "sig_id": 3,
        "sig_param_id": 1,
        "aliases": (
            "SLH-DSA-SHA2-128f",
            "sphincs+-sha2-128f-simple",
            "sphincs128fsha2",
            "sphincs128f_sha2",
        ),
    },
    "sphincs256fsha2": {
        "oqs_name": "SPHINCS+-SHA2-256f-simple",
        "token": "sphincs256fsha2",
        "sig_id": 3,
        "sig_param_id": 2,
        "aliases": (
            "SLH-DSA-SHA2-256f",
            "sphincs+-sha2-256f-simple",
            "sphincs256fsha2",
            "sphincs256f_sha2",
        ),
    },
}


_AEAD_REGISTRY = {
    "aesgcm": {
        "display_name": "AES-256-GCM",
        "token": "aesgcm",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "AES-256-GCM",
            "aes-256-gcm",
            "aesgcm",
            "aes256gcm",
            "aes-gcm",
        ),
    },
}


def _build_alias_map(registry: Dict[str, Dict]) -> Dict[str, str]:
    alias_map: Dict[str, str] = {}
    for key, entry in registry.items():
        for alias in entry["aliases"]:
            normalized = _normalize_alias(alias)
            alias_map[normalized] = key
        alias_map[_normalize_alias(entry["oqs_name"]) if "oqs_name" in entry else _normalize_alias(entry["display_name"])] = key
        alias_map[_normalize_alias(entry["token"])] = key
    return alias_map


_KEM_ALIASES = _build_alias_map(_KEM_REGISTRY)
_SIG_ALIASES = _build_alias_map(_SIG_REGISTRY)
_AEAD_ALIASES = _build_alias_map(_AEAD_REGISTRY)


def _resolve_kem_key(name: str) -> str:
    lookup = _KEM_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise NotImplementedError(f"unknown KEM: {name}")
    return lookup


def _resolve_sig_key(name: str) -> str:
    lookup = _SIG_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise NotImplementedError(f"unknown signature: {name}")
    return lookup


def _resolve_aead_key(name: str) -> str:
    lookup = _AEAD_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise NotImplementedError(f"unknown AEAD: {name}")
    return lookup


def build_suite_id(kem: str, aead: str, sig: str) -> str:
    """Build canonical suite identifier from component aliases."""

    kem_key = _resolve_kem_key(kem)
    aead_key = _resolve_aead_key(aead)
    sig_key = _resolve_sig_key(sig)

    kem_entry = _KEM_REGISTRY[kem_key]
    aead_entry = _AEAD_REGISTRY[aead_key]
    sig_entry = _SIG_REGISTRY[sig_key]

    return f"cs-{kem_entry['token']}-{aead_entry['token']}-{sig_entry['token']}"


_LEGACY_SUITE_ALIASES: Tuple[Tuple[str, str, str], ...] = (
    ("ML-KEM-512", "AES-256-GCM", "ML-DSA-44"),
    ("ML-KEM-768", "AES-256-GCM", "ML-DSA-65"),
    ("ML-KEM-1024", "AES-256-GCM", "ML-DSA-87"),
    ("ML-KEM-768", "AES-256-GCM", "Falcon-512"),
    ("ML-KEM-1024", "AES-256-GCM", "Falcon-1024"),
    ("ML-KEM-512", "AES-256-GCM", "SLH-DSA-SHA2-128f"),
    ("ML-KEM-1024", "AES-256-GCM", "SLH-DSA-SHA2-256f"),
)


_SUITE_ALIASES = {
    legacy_id: build_suite_id(*components)
    for legacy_id, components in {
        "cs-kyber512-aesgcm-dilithium2": _LEGACY_SUITE_ALIASES[0],
        "cs-kyber768-aesgcm-dilithium3": _LEGACY_SUITE_ALIASES[1],
        "cs-kyber1024-aesgcm-dilithium5": _LEGACY_SUITE_ALIASES[2],
        "cs-kyber768-aesgcm-falcon512": _LEGACY_SUITE_ALIASES[3],
        "cs-kyber1024-aesgcm-falcon1024": _LEGACY_SUITE_ALIASES[4],
        "cs-kyber512-aesgcm-sphincs128f_sha2": _LEGACY_SUITE_ALIASES[5],
        "cs-kyber1024-aesgcm-sphincs256f_sha2": _LEGACY_SUITE_ALIASES[6],
    }.items()
}


def _compose_suite(kem_key: str, aead_key: str, sig_key: str) -> Dict[str, object]:
    kem_entry = _KEM_REGISTRY[kem_key]
    aead_entry = _AEAD_REGISTRY[aead_key]
    sig_entry = _SIG_REGISTRY[sig_key]

    suite_id = f"cs-{kem_entry['token']}-{aead_entry['token']}-{sig_entry['token']}"

    return {
        "suite_id": suite_id,
        "kem_name": kem_entry["oqs_name"],
        "kem_id": kem_entry["kem_id"],
        "kem_param_id": kem_entry["kem_param_id"],
        "sig_name": sig_entry["oqs_name"],
        "sig_id": sig_entry["sig_id"],
        "sig_param_id": sig_entry["sig_param_id"],
        "nist_level": kem_entry["nist_level"],
        "aead": aead_entry["display_name"],
        "kdf": aead_entry["kdf"],
    }


def _canonicalize_suite_id(suite_id: str) -> str:
    if not suite_id:
        raise NotImplementedError("suite_id cannot be empty")

    candidate = suite_id.strip()
    if candidate in _SUITE_ALIASES:
        return _SUITE_ALIASES[candidate]

    if not candidate.startswith("cs-"):
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    parts = candidate[3:].split("-")
    if len(parts) < 3:
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    kem_part = parts[0]
    aead_part = parts[1]
    sig_part = "-".join(parts[2:])

    try:
        return build_suite_id(kem_part, aead_part, sig_part)
    except NotImplementedError as exc:
        raise NotImplementedError(f"unknown suite_id: {suite_id}") from exc


def _generate_suite_registry() -> MappingProxyType:
    suites: Dict[str, MappingProxyType] = {}
    for kem_key, sig_key in product(_KEM_REGISTRY.keys(), _SIG_REGISTRY.keys()):
        suite_dict = _compose_suite(kem_key, "aesgcm", sig_key)
        suites[suite_dict["suite_id"]] = MappingProxyType(suite_dict)
    return MappingProxyType(suites)


SUITES = _generate_suite_registry()


def list_suites() -> Dict[str, Dict]:
    """Return all available suites as immutable mapping."""

    return {suite_id: dict(config) for suite_id, config in SUITES.items()}


def get_suite(suite_id: str) -> Dict:
    """Get suite configuration by ID, resolving legacy aliases and synonyms."""

    canonical_id = _canonicalize_suite_id(suite_id)

    if canonical_id not in SUITES:
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    suite = SUITES[canonical_id]

    required_fields = {"kem_name", "sig_name", "aead", "kdf", "nist_level"}
    missing_fields = required_fields - set(suite.keys())
    if missing_fields:
        raise NotImplementedError(f"malformed suite {suite_id}: missing fields {missing_fields}")

    return dict(suite)


def _safe_get_enabled_kem_mechanisms() -> Iterable[str]:
    from oqs.oqs import get_enabled_KEM_mechanisms

    return get_enabled_KEM_mechanisms()


def _safe_get_enabled_sig_mechanisms() -> Iterable[str]:
    from oqs.oqs import get_enabled_sig_mechanisms

    return get_enabled_sig_mechanisms()


def enabled_kems() -> Tuple[str, ...]:
    """Return tuple of oqs KEM mechanism names supported by the runtime."""

    mechanisms = {_normalize_alias(name) for name in _safe_get_enabled_kem_mechanisms()}
    result = [
        entry["oqs_name"]
        for entry in _KEM_REGISTRY.values()
        if _normalize_alias(entry["oqs_name"]) in mechanisms
    ]
    return tuple(result)


def enabled_sigs() -> Tuple[str, ...]:
    """Return tuple of oqs signature mechanism names supported by the runtime."""

    mechanisms = {_normalize_alias(name) for name in _safe_get_enabled_sig_mechanisms()}
    result = [
        entry["oqs_name"]
        for entry in _SIG_REGISTRY.values()
        if _normalize_alias(entry["oqs_name"]) in mechanisms
    ]
    return tuple(result)


def header_ids_for_suite(suite: Dict) -> Tuple[int, int, int, int]:
    """Return embedded header ID bytes for provided suite dict copy."""

    try:
        return (
            suite["kem_id"],
            suite["kem_param_id"],
            suite["sig_id"],
            suite["sig_param_id"],
        )
    except KeyError as e:
        raise NotImplementedError(f"suite missing embedded id field: {e}")


def suite_bytes_for_hkdf(suite: Dict) -> bytes:
    """Generate deterministic bytes from suite for HKDF info parameter."""

    if "suite_id" in suite:
        return suite["suite_id"].encode("utf-8")

    try:
        suite_id = build_suite_id(suite["kem_name"], suite["aead"], suite["sig_name"])
    except (KeyError, NotImplementedError) as exc:
        raise NotImplementedError("Suite configuration not found in registry") from exc

    return suite_id.encode("utf-8")

============================================================

FILE 12/12: temp-file.py
============================================================
Full Path: C:\Users\burak\Desktop\research\core\temp-file.py
Size: 18,859 bytes
Modified: 2025-09-25 18:06:34
------------------------------------------------------------
"""
Selectors-based network transport proxy.

Responsibilities:
1. Perform authenticated TCP handshake (PQC KEM + signature) using `core.handshake`.
2. Bridge plaintext UDP <-> encrypted UDP (AEAD framing) both directions.
3. Enforce replay window and per-direction sequence via `core.aead`.

Note: This module uses the low-level `selectors` stdlib facility—not `asyncio`—to
remain dependency-light and fully deterministic for test harnesses. The filename
is retained for backward compatibility; a future refactor may rename it to
`selector_proxy.py` and/or introduce an asyncio variant.
"""

from __future__ import annotations

import socket
import selectors
import threading
import time
import struct
from typing import Optional, Dict, Tuple
from contextlib import contextmanager

from core.config import CONFIG
from core.suites import SUITES, header_ids_for_suite
try:
    # Optional helper (if you implemented it)
    from core.suites import header_ids_from_names  # type: ignore
except Exception:
    header_ids_from_names = None  # type: ignore

from core.handshake import server_gcs_handshake, client_drone_handshake
from core.logging_utils import get_logger

from core.aead import (
    Sender,
    Receiver,
    HeaderMismatch,
    ReplayError,
    AeadAuthError,
    AeadIds,
)

from core.policy_engine import handle_control

logger = get_logger("pqc")


class ProxyCounters:
    """Simple counters for proxy statistics."""

    def __init__(self) -> None:
        self.ptx_out = 0      # plaintext packets sent out to app
        self.ptx_in = 0       # plaintext packets received from app
        self.enc_out = 0      # encrypted packets sent to peer
        self.enc_in = 0       # encrypted packets received from peer
        self.drops = 0        # total drops
        # Granular drop reasons
        self.drop_replay = 0
        self.drop_auth = 0
        self.drop_header = 0
        self.drop_session_epoch = 0
        self.drop_other = 0

    def to_dict(self) -> Dict[str, int]:
        return {
            "ptx_out": self.ptx_out,
            "ptx_in": self.ptx_in,
            "enc_out": self.enc_out,
            "enc_in": self.enc_in,
            "drops": self.drops,
            "drop_replay": self.drop_replay,
            "drop_auth": self.drop_auth,
            "drop_header": self.drop_header,
            "drop_session_epoch": self.drop_session_epoch,
            "drop_other": self.drop_other,
        }


def _dscp_to_tos(dscp: Optional[int]) -> Optional[int]:
    """Convert DSCP value to TOS byte for socket options."""
    if dscp is None:
        return None
    try:
        d = int(dscp)
        if 0 <= d <= 63:
            return d << 2  # DSCP occupies high 6 bits of TOS/Traffic Class
    except Exception:
        pass
    return None


def _parse_header_fields(
    expected_version: int,
    aead_ids: AeadIds,
    session_id: bytes,
    wire: bytes,
) -> Tuple[str, Optional[int]]:
    """
    Try to unpack the header and classify the most likely drop reason *without* AEAD work.
    Returns (reason, seq_if_available).
    """
    HEADER_STRUCT = "!BBBBB8sQB"
    HEADER_LEN = struct.calcsize(HEADER_STRUCT)
    if len(wire) < HEADER_LEN:
        return ("header_too_short", None)
    try:
        (version, kem_id, kem_param, sig_id, sig_param, sess, seq, epoch) = struct.unpack(
            HEADER_STRUCT, wire[:HEADER_LEN]
        )
    except struct.error:
        return ("header_unpack_error", None)
    if version != expected_version:
        return ("version_mismatch", seq)
    if (kem_id, kem_param, sig_id, sig_param) != (
        aead_ids.kem_id,
        aead_ids.kem_param,
        aead_ids.sig_id,
        aead_ids.sig_param,
    ):
        return ("crypto_id_mismatch", seq)
    if sess != session_id:
        return ("session_mismatch", seq)
    # If we got here, header matches; any decrypt failure that returns None is auth/tag failure.
    return ("auth_fail_or_replay", seq)


class _TokenBucket:
    """Per-IP rate limiter using token bucket algorithm."""
    def __init__(self, capacity: int, refill_per_sec: float) -> None:
        self.capacity = max(1, capacity)
        self.refill = max(0.01, float(refill_per_sec))
        self.tokens: Dict[str, float] = {}      # ip -> tokens
        self.last: Dict[str, float] = {}        # ip -> last timestamp

    def allow(self, ip: str) -> bool:
        """Check if request from IP should be allowed."""
        now = time.monotonic()
        t = self.tokens.get(ip, self.capacity)
        last = self.last.get(ip, now)
        # refill
        t = min(self.capacity, t + (now - last) * self.refill)
        self.last[ip] = now
        if t >= 1.0:
            t -= 1.0
            self.tokens[ip] = t
            return True
        self.tokens[ip] = t
        return False


def _validate_config(cfg: dict) -> None:
    """Validate required configuration keys are present."""
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX",
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX",
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW",
    ]
    for key in required_keys:
        if key not in cfg:
            raise NotImplementedError(f"CONFIG missing: {key}")


def _perform_handshake(
    role: str,
    suite: dict,
    gcs_sig_secret: Optional[bytes],
    gcs_sig_public: Optional[bytes],
    cfg: dict,
    stop_after_seconds: Optional[float] = None,
    ready_event: Optional[threading.Event] = None,
) -> Tuple[bytes, bytes, bytes, bytes, bytes, Optional[str], Optional[str]]:
    """Perform TCP handshake and return derived keys, session_id, and optionally kem/sig names."""
    if role == "gcs":
        if gcs_sig_secret is None:
            raise NotImplementedError("GCS signature secret not provided")

        server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_sock.bind(("0.0.0.0", cfg["TCP_HANDSHAKE_PORT"]))
        server_sock.listen(32)

        if ready_event:
            ready_event.set()

        timeout = stop_after_seconds if stop_after_seconds is not None else 30.0
        server_sock.settimeout(timeout)

        gate = _TokenBucket(
            cfg.get("HANDSHAKE_RL_BURST", 5),
            cfg.get("HANDSHAKE_RL_REFILL_PER_SEC", 1),
        )

        try:
            try:
                conn, addr = server_sock.accept()
                try:
                    ip, _port = addr
                    if not gate.allow(ip):
                        try:
                            conn.settimeout(0.2)
                            conn.sendall(b"\x00")
                        except Exception:
                            pass
                        finally:
                            conn.close()
                        raise NotImplementedError("Handshake rate-limit: too many attempts")

                    result = server_gcs_handshake(conn, suite, gcs_sig_secret)
                    # Support either 5-tuple or 7-tuple
                    if len(result) >= 7:
                        k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
                    else:
                        k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                        kem_name = sig_name = None
                    return k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name
                finally:
                    conn.close()
            except socket.timeout:
                raise NotImplementedError("No drone connection received within timeout")
        finally:
            server_sock.close()

    elif role == "drone":
        if gcs_sig_public is None:
            raise NotImplementedError("GCS signature public key not provided")

        client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            client_sock.connect((cfg["GCS_HOST"], cfg["TCP_HANDSHAKE_PORT"]))
            result = client_drone_handshake(client_sock, suite, gcs_sig_public)
            if len(result) >= 7:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
            else:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                kem_name = sig_name = None
            return k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name
        finally:
            client_sock.close()
    else:
        raise ValueError(f"Invalid role: {role}")


@contextmanager
def _setup_sockets(role: str, cfg: dict):
    """Setup and cleanup all UDP sockets for the proxy."""
    sockets = {}
    try:
        if role == "drone":
            # Encrypted socket - receive from GCS
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_DRONE_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind(("127.0.0.1", cfg["DRONE_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (no bind needed)
            ptx_out_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sockets["plaintext_out"] = ptx_out_sock

            # Peer addresses
            sockets["encrypted_peer"] = (cfg["GCS_HOST"], cfg["UDP_GCS_RX"])
            sockets["plaintext_peer"] = ("127.0.0.1", cfg["DRONE_PLAINTEXT_RX"])

        elif role == "gcs":
            # Encrypted socket - receive from Drone
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_GCS_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind(("127.0.0.1", cfg["GCS_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (no bind needed)
            ptx_out_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sockets["plaintext_out"] = ptx_out_sock

            # Peer addresses
            sockets["encrypted_peer"] = (cfg["DRONE_HOST"], cfg["UDP_DRONE_RX"])
            sockets["plaintext_peer"] = ("127.0.0.1", cfg["GCS_PLAINTEXT_RX"])
        else:
            raise ValueError(f"Invalid role: {role}")

        yield sockets
    finally:
        for sock in list(sockets.values()):
            if isinstance(sock, socket.socket):
                try:
                    sock.close()
                except Exception:
                    pass


def run_proxy(
    *,
    role: str,
    suite: dict,
    cfg: dict,
    gcs_sig_secret: Optional[bytes] = None,
    gcs_sig_public: Optional[bytes] = None,
    stop_after_seconds: Optional[float] = None,
    ready_event: Optional[threading.Event] = None,
) -> Dict[str, int]:
    """
    Start a blocking proxy process for `role` in {"drone","gcs"}.

    - Performs TCP handshake (server on GCS, client on Drone).
    - Bridges plaintext UDP <-> encrypted UDP in both directions.
    - Returns a dict of simple counters on clean exit:
      {"ptx_out": int, "ptx_in": int, "enc_out": int, "enc_in": int, "drops": int}
    """
    if role not in {"drone", "gcs"}:
        raise ValueError(f"Invalid role: {role}")

    _validate_config(cfg)

    counters = ProxyCounters()
    start_time = time.time()

    # Perform handshake and get session keys (+ optional kem/sig names)
    k_d2g, k_g2d, _nseed_d2g, _nseed_g2d, session_id, kem_name, sig_name = _perform_handshake(
        role, suite, gcs_sig_secret, gcs_sig_public, cfg, stop_after_seconds, ready_event
    )

    # Log successful handshake
    try:
        suite_id = next((sid for sid, s in SUITES.items() if dict(s) == suite), "unknown")
    except Exception:
        suite_id = "unknown"
    logger.info(
        "PQC handshake completed successfully",
        extra={"suite_id": suite_id, "peer_role": ("drone" if role == "gcs" else "gcs"), "session_id": session_id.hex()},
    )

    # Setup AEAD header IDs
    if kem_name and sig_name and header_ids_from_names:
        ids_tuple = header_ids_from_names(kem_name, sig_name)  # type: ignore
    else:
        ids_tuple = header_ids_for_suite(suite)
    aead_ids = AeadIds(*ids_tuple)

    # Role-based key directions
    if role == "drone":
        sender = Sender(CONFIG["WIRE_VERSION"], aead_ids, session_id, 0, k_d2g)
        receiver = Receiver(CONFIG["WIRE_VERSION"], aead_ids, session_id, 0, k_g2d, cfg["REPLAY_WINDOW"])
    else:  # gcs
        sender = Sender(CONFIG["WIRE_VERSION"], aead_ids, session_id, 0, k_g2d)
        receiver = Receiver(CONFIG["WIRE_VERSION"], aead_ids, session_id, 0, k_d2g, cfg["REPLAY_WINDOW"])

    # UDP bridge loop
    with _setup_sockets(role, cfg) as sockets:
        selector = selectors.DefaultSelector()
        selector.register(sockets["encrypted"], selectors.EVENT_READ, data="encrypted")
        selector.register(sockets["plaintext_in"], selectors.EVENT_READ, data="plaintext_in")

        try:
            while True:
                if stop_after_seconds is not None and (time.time() - start_time) >= stop_after_seconds:
                    break

                events = selector.select(timeout=0.1)
                for key, _mask in events:
                    sock = key.fileobj
                    data_type = key.data

                    if data_type == "plaintext_in":
                        # Plaintext ingress: encrypt and forward
                        try:
                            payload, _addr = sock.recvfrom(2048)
                            if not payload:
                                continue
                            counters.ptx_in += 1

                            payload_out = (b"\x01" + payload) if cfg.get("ENABLE_PACKET_TYPE") else payload
                            wire = sender.encrypt(payload_out)

                            try:
                                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                                counters.enc_out += 1
                            except socket.error:
                                counters.drops += 1
                        except socket.error:
                            continue

                    elif data_type == "encrypted":
                        try:
                            wire, _addr = sock.recvfrom(2048)
                            if not wire:
                                continue
                            counters.enc_in += 1

                            try:
                                plaintext = receiver.decrypt(wire)
                                if plaintext is None:
                                    reason, _seq = _parse_header_fields(
                                        CONFIG["WIRE_VERSION"], receiver.ids, receiver.session_id, wire
                                    )
                                    counters.drops += 1
                                    if reason in ("version_mismatch", "crypto_id_mismatch", "header_too_short", "header_unpack_error"):
                                        counters.drop_header += 1
                                    elif reason == "session_mismatch":
                                        counters.drop_session_epoch += 1
                                    else:
                                        counters.drop_auth += 1
                                    continue
                            except ReplayError:
                                counters.drops += 1
                                counters.drop_replay += 1
                                continue
                            except HeaderMismatch:
                                counters.drops += 1
                                counters.drop_header += 1
                                continue
                            except AeadAuthError:
                                counters.drops += 1
                                counters.drop_auth += 1
                                continue
                            except Exception:
                                counters.drops += 1
                                counters.drop_other += 1
                                continue

                            try:
                                out_bytes = plaintext
                                if cfg.get("ENABLE_PACKET_TYPE") and plaintext:
                                    ptype = plaintext[0]
                                    if ptype == 0x01:
                                        out_bytes = plaintext[1:]  # deliver to app
                                    elif ptype == 0x02:
                                        _ = handle_control(plaintext[1:])
                                        continue
                                    else:
                                        counters.drops += 1
                                        counters.drop_other += 1
                                        continue

                                sockets["plaintext_out"].sendto(out_bytes, sockets["plaintext_peer"])
                                counters.ptx_out += 1
                            except socket.error:
                                counters.drops += 1
                                counters.drop_other += 1
                        except socket.error:
                            continue
        except KeyboardInterrupt:
            pass
        finally:
            selector.close()

    return counters.to_dict()

============================================================

================================================================================
END OF LOG
================================================================================

----------------------
#!/usr/bin/env python3
"""GCS scheduler that drives rekeys and UDP traffic using central configuration."""

from __future__ import annotations

import bisect
import csv
import json
import math
import os
import shutil
import socket
import struct
import subprocess
import sys
import threading
import time
from collections import deque
from copy import deepcopy
from pathlib import Path
from typing import Dict, IO, Iterable, List, Optional, Set, Tuple

try:
    from openpyxl import Workbook
except ImportError:  # pragma: no cover
    Workbook = None

# Ensure project root is on sys.path so `import core` works when running this file
ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from core import suites as suites_mod
from core.config import CONFIG


DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_SEND_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
APP_RECV_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))

OUTDIR = ROOT / "logs/auto/gcs"
SUITES_OUTDIR = OUTDIR / "suites"
SECRETS_DIR = ROOT / "secrets/matrix"

EXCEL_OUTPUT_DIR = ROOT / Path(
    CONFIG.get("GCS_EXCEL_OUTPUT")
    or os.getenv("GCS_EXCEL_OUTPUT", "output/gcs")
)

COMBINED_OUTPUT_DIR = ROOT / Path(
    CONFIG.get("GCS_COMBINED_OUTPUT_BASE")
    or os.getenv("GCS_COMBINED_OUTPUT_BASE", "output/gcs")
)

DRONE_MONITOR_BASE = ROOT / Path(
    CONFIG.get("DRONE_MONITOR_OUTPUT_BASE")
    or os.getenv("DRONE_MONITOR_OUTPUT_BASE", "output/drone")
)

TELEMETRY_BIND_HOST = CONFIG.get("GCS_TELEMETRY_BIND", "0.0.0.0")
TELEMETRY_PORT = int(
    CONFIG.get("GCS_TELEMETRY_PORT")
    or CONFIG.get("DRONE_TELEMETRY_PORT")
    or 52080
)

PROXY_STATUS_PATH = OUTDIR / "gcs_status.json"
PROXY_SUMMARY_PATH = OUTDIR / "gcs_summary.json"
SUMMARY_CSV = OUTDIR / "summary.csv"
EVENTS_FILENAME = "blaster_events.jsonl"

SEQ_TS_OVERHEAD_BYTES = 12
UDP_HEADER_BYTES = 8
IPV4_HEADER_BYTES = 20
IPV6_HEADER_BYTES = 40
MIN_DELAY_SAMPLES = 30
HYSTERESIS_WINDOW = 3
MAX_BISECT_STEPS = 3
WARMUP_FRACTION = 0.1
MAX_WARMUP_SECONDS = 1.0
SATURATION_COARSE_RATES = [5, 25, 50, 75, 100, 125, 150, 175, 200]
SATURATION_LINEAR_RATES = [
    5,
    10,
    15,
    20,
    25,
    30,
    35,
    40,
    45,
    50,
    60,
    70,
    80,
    90,
    100,
    125,
    150,
    175,
    200,
]
SATURATION_SIGNALS = ("owd_p95_spike", "delivery_degraded", "loss_excess")


class P2Quantile:
    def __init__(self, p: float) -> None:
        if not 0.0 < p < 1.0:
            raise ValueError("p must be between 0 and 1")
        self.p = p
        self._initial: List[float] = []
        self._q: List[float] = []
        self._n: List[int] = []
        self._np: List[float] = []
        self._dn = [0.0, p / 2.0, p, (1.0 + p) / 2.0, 1.0]
        self.count = 0

    def add(self, sample: float) -> None:
        x = float(sample)
        self.count += 1
        if self.count <= 5:
            bisect.insort(self._initial, x)
            if self.count == 5:
                self._q = list(self._initial)
                self._n = [1, 2, 3, 4, 5]
                self._np = [1.0, 1.0 + 2.0 * self.p, 1.0 + 4.0 * self.p, 3.0 + 2.0 * self.p, 5.0]
            return

        if not self._q:
            # Should not happen, but guard for consistency
            self._q = list(self._initial)
            self._n = [1, 2, 3, 4, 5]
            self._np = [1.0, 1.0 + 2.0 * self.p, 1.0 + 4.0 * self.p, 3.0 + 2.0 * self.p, 5.0]

        if x < self._q[0]:
            self._q[0] = x
            k = 0
        elif x >= self._q[4]:
            self._q[4] = x
            k = 3
        else:
            k = 0
            for idx in range(4):
                if self._q[idx] <= x < self._q[idx + 1]:
                    k = idx
                    break

        for idx in range(k + 1, 5):
            self._n[idx] += 1

        for idx in range(5):
            self._np[idx] += self._dn[idx]

        for idx in range(1, 4):
            d = self._np[idx] - self._n[idx]
            if (d >= 1 and self._n[idx + 1] - self._n[idx] > 1) or (d <= -1 and self._n[idx - 1] - self._n[idx] < -1):
                step = 1 if d > 0 else -1
                candidate = self._parabolic(idx, step)
                if self._q[idx - 1] < candidate < self._q[idx + 1]:
                    self._q[idx] = candidate
                else:
                    self._q[idx] = self._linear(idx, step)
                self._n[idx] += step

    def value(self) -> float:
        if self.count == 0:
            return 0.0
        if self.count <= 5 and self._initial:
            rank = (self.count - 1) * self.p
            idx = max(0, min(len(self._initial) - 1, int(round(rank))))
            return float(self._initial[idx])
        if not self._q:
            return 0.0
        return float(self._q[2])

    def _parabolic(self, idx: int, step: int) -> float:
        numerator_left = self._n[idx] - self._n[idx - 1] + step
        numerator_right = self._n[idx + 1] - self._n[idx] - step
        denominator = self._n[idx + 1] - self._n[idx - 1]
        if denominator == 0:
            return self._q[idx]
        return self._q[idx] + (step / denominator) * (
            numerator_left * (self._q[idx + 1] - self._q[idx]) / max(self._n[idx + 1] - self._n[idx], 1)
            + numerator_right * (self._q[idx] - self._q[idx - 1]) / max(self._n[idx] - self._n[idx - 1], 1)
        )

    def _linear(self, idx: int, step: int) -> float:
        target = idx + step
        denominator = self._n[target] - self._n[idx]
        if denominator == 0:
            return self._q[idx]
        return self._q[idx] + step * (self._q[target] - self._q[idx]) / denominator


def wilson_interval(successes: int, n: int, z: float = 1.96) -> Tuple[float, float]:
    if n <= 0:
        return (0.0, 1.0)
    proportion = successes / n
    z2 = z * z
    denom = 1.0 + z2 / n
    center = (proportion + z2 / (2.0 * n)) / denom
    margin = (z * math.sqrt((proportion * (1.0 - proportion) / n) + (z2 / (4.0 * n * n)))) / denom
    return (max(0.0, center - margin), min(1.0, center + margin))


def ip_header_bytes_for_host(host: str) -> int:
    return IPV6_HEADER_BYTES if ":" in host else IPV4_HEADER_BYTES


APP_IP_HEADER_BYTES = ip_header_bytes_for_host(APP_SEND_HOST)


def ts() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def _merge_defaults(defaults: dict, override: Optional[dict]) -> dict:
    result = deepcopy(defaults)
    if isinstance(override, dict):
        for key, value in override.items():
            if isinstance(value, dict) and isinstance(result.get(key), dict):
                merged = result[key].copy()
                merged.update(value)
                result[key] = merged
            else:
                result[key] = value
    return result


AUTO_GCS_DEFAULTS = {
    "session_prefix": "session",
    "traffic": "blast",
    "duration_s": 45.0,
    "pre_gap_s": 1.0,
    "inter_gap_s": 15.0,
    "payload_bytes": 256,
    "event_sample": 100,
    "passes": 1,
    "rate_pps": 0,
    "bandwidth_mbps": 0.0,
    "max_rate_mbps": 200.0,
    "sat_search": "auto",
    "sat_delivery_threshold": 0.85,
    "sat_loss_threshold_pct": 5.0,
    "sat_rtt_spike_factor": 1.6,
    "suites": None,
    "launch_proxy": True,
    "monitors_enabled": True,
    "telemetry_enabled": True,
    "telemetry_bind_host": TELEMETRY_BIND_HOST,
    "telemetry_port": TELEMETRY_PORT,
    "export_combined_excel": True,
    "power_capture": True,
}

AUTO_GCS_CONFIG = _merge_defaults(AUTO_GCS_DEFAULTS, CONFIG.get("AUTO_GCS"))

SATURATION_SEARCH_MODE = str(AUTO_GCS_CONFIG.get("sat_search") or "auto").lower()
SATURATION_RTT_SPIKE = float(AUTO_GCS_CONFIG.get("sat_rtt_spike_factor") or 1.6)
SATURATION_DELIVERY_THRESHOLD = float(AUTO_GCS_CONFIG.get("sat_delivery_threshold") or 0.85)
SATURATION_LOSS_THRESHOLD = float(AUTO_GCS_CONFIG.get("sat_loss_threshold_pct") or 5.0)


def mkdirp(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def suite_outdir(suite: str) -> Path:
    target = SUITES_OUTDIR / suite
    mkdirp(target)
    return target


def resolve_suites(requested: Optional[Iterable[str]]) -> List[str]:
    available = list(suites_mod.list_suites())
    if not available:
        raise RuntimeError("No suites registered in core.suites; cannot proceed")

    if not requested:
        return available

    resolved: List[str] = []
    seen: Set[str] = set()
    for name in requested:
        info = suites_mod.get_suite(name)
        suite_id = info["suite_id"]
        if suite_id not in available:
            raise RuntimeError(f"Suite {name} not present in core registry")
        if suite_id not in seen:
            resolved.append(suite_id)
            seen.add(suite_id)
    return resolved


def preferred_initial_suite(candidates: List[str]) -> Optional[str]:
    configured = CONFIG.get("SIMPLE_INITIAL_SUITE")
    if not configured:
        return None
    try:
        suite_id = suites_mod.get_suite(configured)["suite_id"]
    except NotImplementedError:
        return None
    return suite_id if suite_id in candidates else None


def ctl_send(obj: dict, timeout: float = 2.0, retries: int = 4, backoff: float = 0.5) -> dict:
    last_exc: Optional[Exception] = None
    for attempt in range(1, retries + 1):
        try:
            with socket.create_connection((DRONE_HOST, CONTROL_PORT), timeout=timeout) as sock:
                sock.sendall((json.dumps(obj) + "\n").encode())
                sock.shutdown(socket.SHUT_WR)
                line = sock.makefile().readline()
                return json.loads(line.strip()) if line else {}
        except Exception as exc:
            last_exc = exc
            if attempt < retries:
                time.sleep(backoff * attempt)
                continue
            raise
    if last_exc:
        raise last_exc
    return {}


def request_power_capture(suite: str, duration_s: float, start_ns: Optional[int]) -> dict:
    payload = {
        "cmd": "power_capture",
        "suite": suite,
        "duration_s": duration_s,
    }
    if start_ns is not None:
        payload["start_ns"] = int(start_ns)
    try:
        resp = ctl_send(payload, timeout=1.5, retries=2, backoff=0.4)
    except Exception as exc:
        print(f"[WARN] power_capture request failed: {exc}", file=sys.stderr)
        return {"ok": False, "error": str(exc)}
    return resp


def poll_power_status(max_wait_s: float = 12.0, poll_s: float = 0.6) -> dict:
    deadline = time.time() + max_wait_s
    last: dict = {}
    while time.time() < deadline:
        try:
            resp = ctl_send({"cmd": "power_status"}, timeout=1.5, retries=1, backoff=0.3)
        except Exception as exc:
            last = {"ok": False, "error": str(exc)}
            time.sleep(poll_s)
            continue
        last = resp
        if not resp.get("ok"):
            break
        if not resp.get("available", True):
            break
        if not resp.get("busy", False):
            break
        time.sleep(poll_s)
    return last


class Blaster:
    """High-rate UDP blaster with RTT sampling and throughput accounting."""

    def __init__(
        self,
        send_host: str,
        send_port: int,
    recv_host: str,
    recv_port: int,
    events_path: Optional[Path],
        payload_bytes: int,
        sample_every: int,
        offset_ns: int,
    ) -> None:
        self.send_addr = (send_host, send_port)
        self.recv_addr = (recv_host, recv_port)
        self.payload_bytes = max(12, int(payload_bytes))
        self.sample_every = max(0, int(sample_every))
        self.offset_ns = offset_ns
        self.tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx.bind(self.recv_addr)
        self.rx.settimeout(0.001)
        self.rx_burst = max(1, int(os.getenv("GCS_RX_BURST", "32")))
        try:
            # Allow overriding socket buffer sizes via environment variables
            # Use GCS_SOCK_SNDBUF and GCS_SOCK_RCVBUF if present, otherwise default to 1 MiB
            sndbuf = int(os.getenv("GCS_SOCK_SNDBUF", str(1 << 20)))
            rcvbuf = int(os.getenv("GCS_SOCK_RCVBUF", str(1 << 20)))
            self.tx.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, sndbuf)
            self.rx.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, rcvbuf)
            actual_snd = self.tx.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF)
            actual_rcv = self.rx.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)
            print(
                f"[{ts()}] blaster UDP socket buffers: snd={actual_snd} rcv={actual_rcv}",
                flush=True,
            )
        except Exception:
            # best-effort; continue even if setting buffers fails
            pass

        self.events_path = events_path
        self.events: Optional[IO[str]] = None
        if events_path is not None:
            mkdirp(events_path.parent)
            self.events = open(events_path, "w", encoding="utf-8")

        self.sent = 0
        self.rcvd = 0
        self.sent_bytes = 0
        self.rcvd_bytes = 0
        self.rtt_sum_ns = 0
        self.rtt_samples = 0
        self.rtt_max_ns = 0
        self.rtt_min_ns: Optional[int] = None
        self.pending: Dict[int, int] = {}
        self.rtt_p50 = P2Quantile(0.5)
        self.rtt_p95 = P2Quantile(0.95)
        self.owd_p50 = P2Quantile(0.5)
        self.owd_p95 = P2Quantile(0.95)
        self.owd_samples = 0
        self.owd_p50_ns = 0.0
        self.owd_p95_ns = 0.0
        self.rtt_p50_ns = 0.0
        self.rtt_p95_ns = 0.0

    def _log_event(self, payload: dict) -> None:
        # Buffered write; caller flushes at end of run()
        if self.events is None:
            return
        self.events.write(json.dumps(payload) + "\n")

    def _now(self) -> int:
        return time.time_ns() + self.offset_ns

    def _maybe_log(self, kind: str, seq: int, t_ns: int) -> None:
        if self.sample_every == 0:
            return
        if kind == "send":
            if seq % self.sample_every:
                return
        else:
            if self.rcvd % self.sample_every:
                return
        self._log_event({"event": kind, "seq": seq, "t_ns": t_ns})

    def run(self, duration_s: float, rate_pps: int, max_packets: Optional[int] = None) -> None:
        stop_at = self._now() + int(max(0.0, duration_s) * 1e9)
        payload_pad = b"\x00" * (self.payload_bytes - 12)
        interval = 0.0 if rate_pps <= 0 else 1.0 / max(1, rate_pps)
        stop_event = threading.Event()
        rx_thread = threading.Thread(target=self._rx_loop, args=(stop_event,), daemon=True)
        rx_thread.start()

        seq = 0
        burst = 32 if interval == 0.0 else 1
        while self._now() < stop_at:
            loop_progress = False
            sends_this_loop = burst
            while sends_this_loop > 0:
                if self._now() >= stop_at:
                    break
                t_send = self._now()
                packet = seq.to_bytes(4, "big") + int(t_send).to_bytes(8, "big") + payload_pad
                try:
                    self.tx.sendto(packet, self.send_addr)
                    if self.sample_every and (seq % self.sample_every == 0):
                        self.pending[seq] = int(t_send)
                    self.sent += 1
                    self.sent_bytes += len(packet)
                    loop_progress = True
                    self._maybe_log("send", seq, int(t_send))
                except Exception as exc:
                    self._log_event({"event": "send_error", "err": str(exc), "ts": ts()})
                seq += 1
                sends_this_loop -= 1
            if max_packets is not None and self.sent >= max_packets:
                break
            if interval > 0.0:
                time.sleep(interval)
            elif (seq & 0x3FFF) == 0:
                time.sleep(0)
            if not loop_progress:
                time.sleep(0.0005)

        tail_deadline = self._now() + int(0.25 * 1e9)
        while self._now() < tail_deadline:
            if not self._rx_once():
                time.sleep(0)
        stop_event.set()
        rx_thread.join(timeout=0.2)
        self.owd_p50_ns = self.owd_p50.value()
        self.owd_p95_ns = self.owd_p95.value()
        self.rtt_p50_ns = self.rtt_p50.value()
        self.rtt_p95_ns = self.rtt_p95.value()
        # Bug #5 fix: Ensure cleanup happens even on exceptions
        try:
            if self.events:
                try:
                    self.events.flush()
                except Exception:
                    pass
        finally:
            if self.events:
                try:
                    self.events.close()
                except Exception:
                    pass
                self.events = None
            try:
                self.tx.close()
            except Exception:
                pass
            try:
                self.rx.close()
            except Exception:
                pass
    def _rx_loop(self, stop_event: threading.Event) -> None:
        while not stop_event.is_set():
            progressed = False
            for _ in range(self.rx_burst):
                if self._rx_once():
                    progressed = True
                else:
                    break
            if not progressed:
                time.sleep(0.0005)

    def _rx_once(self) -> bool:
        try:
            data, _ = self.rx.recvfrom(65535)
        except socket.timeout:
            return False
        except (socket.error, OSError) as exc:
            # Bug #4 fix: Catch specific exceptions, log unexpected errors
            if not isinstance(exc, (ConnectionResetError, ConnectionRefusedError)):
                self._log_event({"event": "rx_error", "err": str(exc), "ts": ts()})
            return False
        t_recv = self._now()
        self.rcvd += 1
        self.rcvd_bytes += len(data)
        if len(data) < 4:
            return True

        seq = int.from_bytes(data[:4], "big")
        t_send = self.pending.pop(seq, None)
        header_t_send = int.from_bytes(data[4:12], "big") if len(data) >= 12 else None
        if t_send is None:
            t_send = header_t_send

        drone_recv_ns = int.from_bytes(data[-8:], "big") if len(data) >= 20 else None

        if t_send is not None:
            rtt = t_recv - t_send
            if rtt >= 0:
                self.rtt_sum_ns += rtt
                self.rtt_samples += 1
                if rtt > self.rtt_max_ns:
                    self.rtt_max_ns = rtt
                if self.rtt_min_ns is None or rtt < self.rtt_min_ns:
                    self.rtt_min_ns = rtt
                self.rtt_p50.add(rtt)
                self.rtt_p95.add(rtt)
            self._maybe_log("recv", seq, int(t_recv))

        if t_send is not None and drone_recv_ns is not None:
            owd_up_ns = drone_recv_ns - t_send
            if owd_up_ns >= 0:
                self.owd_samples += 1
                self.owd_p50.add(owd_up_ns)
                self.owd_p95.add(owd_up_ns)
        return True


def wait_handshake(timeout: float = 20.0) -> bool:
    deadline = time.time() + timeout
    while time.time() < deadline:
        if PROXY_STATUS_PATH.exists():
            try:
                with open(PROXY_STATUS_PATH, encoding="utf-8") as handle:
                    js = json.load(handle)
            except Exception:
                js = {}
            state = js.get("state") or js.get("status")
            if state in {"running", "completed", "ready", "handshake_ok"}:
                return True
        time.sleep(0.3)
    return False


def wait_active_suite(target: str, timeout: float = 10.0) -> bool:
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            status = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status = {}
        if status.get("suite") == target:
            return True
        time.sleep(0.2)
    return False


def timesync() -> dict:
    t1 = time.time_ns()
    resp = ctl_send({"cmd": "timesync", "t1_ns": t1})
    t4 = time.time_ns()
    t2 = int(resp.get("t2_ns", t1))
    t3 = int(resp.get("t3_ns", t4))
    delay_ns = (t4 - t1) - (t3 - t2)
    offset_ns = ((t2 - t1) + (t3 - t4)) // 2
    return {"offset_ns": offset_ns, "rtt_ns": delay_ns}


def snapshot_proxy_artifacts(suite: str) -> None:
    target_dir = suite_outdir(suite)
    if PROXY_STATUS_PATH.exists():
        shutil.copy(PROXY_STATUS_PATH, target_dir / "gcs_status.json")
    if PROXY_SUMMARY_PATH.exists():
        shutil.copy(PROXY_SUMMARY_PATH, target_dir / "gcs_summary.json")


def start_gcs_proxy(initial_suite: str) -> tuple[subprocess.Popen, IO[str]]:
    key_path = SECRETS_DIR / initial_suite / "gcs_signing.key"
    if not key_path.exists():
        raise FileNotFoundError(f"Missing GCS signing key for suite {initial_suite}: {key_path}")

    mkdirp(OUTDIR)
    log_path = OUTDIR / f"gcs_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_handle: IO[str] = open(log_path, "w", encoding="utf-8", errors="replace")

    env = os.environ.copy()
    env["DRONE_HOST"] = DRONE_HOST
    env["GCS_HOST"] = GCS_HOST
    env["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    env["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"

    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str

    proc = subprocess.Popen(
        [
            sys.executable,
            "-m",
            "core.run_proxy",
            "gcs",
            "--suite",
            initial_suite,
            "--gcs-secret-file",
            str(key_path),
            "--control-manual",
            "--status-file",
            str(PROXY_STATUS_PATH),
            "--json-out",
            str(PROXY_SUMMARY_PATH),
        ],
        stdin=subprocess.PIPE,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
        env=env,
        cwd=str(ROOT),
    )
    return proc, log_handle


def read_proxy_stats_live() -> dict:
    try:
        with open(PROXY_STATUS_PATH, encoding="utf-8") as handle:
            js = json.load(handle)
    except Exception:
        return {}
    if isinstance(js, dict):
        counters = js.get("counters")
        if isinstance(counters, dict):
            return counters
        if any(k in js for k in ("enc_out", "enc_in")):
            return js
    return {}


def read_proxy_summary() -> dict:
    if not PROXY_SUMMARY_PATH.exists():
        return {}
    try:
        with open(PROXY_SUMMARY_PATH, encoding="utf-8") as handle:
            return json.load(handle)
    except Exception:
        return {}



def _read_proxy_counters() -> dict:

    counters = read_proxy_stats_live()

    if isinstance(counters, dict) and counters:

        return counters

    summary = read_proxy_summary()

    if isinstance(summary, dict):

        summary_counters = summary.get("counters")

        if isinstance(summary_counters, dict):

            return summary_counters

        if any(key in summary for key in ("enc_out", "enc_in", "rekeys_ok", "rekeys_fail", "last_rekey_suite")):

            return summary

    return {}





def wait_proxy_rekey(

    target_suite: str,

    baseline: Dict[str, object],

    *,

    timeout: float = 20.0,

    poll_interval: float = 0.4,

    proc: subprocess.Popen,

) -> str:

    start = time.time()

    baseline_ok = int(baseline.get("rekeys_ok", 0) or 0)

    baseline_fail = int(baseline.get("rekeys_fail", 0) or 0)

    while time.time() - start < timeout:

        if proc.poll() is not None:

            raise RuntimeError("GCS proxy exited during rekey")

        counters = _read_proxy_counters()

        if counters:

            rekeys_ok = int(counters.get("rekeys_ok", 0) or 0)

            rekeys_fail = int(counters.get("rekeys_fail", 0) or 0)

            last_suite = counters.get("last_rekey_suite") or counters.get("suite") or ""

            if rekeys_fail > baseline_fail:

                return "fail"

            if rekeys_ok > baseline_ok and (not last_suite or last_suite == target_suite):

                return "ok"

        time.sleep(poll_interval)

    return "timeout"


def activate_suite(gcs: subprocess.Popen, suite: str, is_first: bool) -> float:

    if gcs.poll() is not None:

        raise RuntimeError("GCS proxy is not running; cannot continue")

    start_ns = time.time_ns()

    if is_first:

        try:

            ctl_send({"cmd": "mark", "suite": suite})

        except Exception as exc:

            print(f"[WARN] control mark failed for {suite}: {exc}", file=sys.stderr)

        finally:

            try:

                ctl_send({"cmd": "rekey_complete", "suite": suite, "status": "ok"})

            except Exception:

                pass

    else:

        assert gcs.stdin is not None

        print(f"[{ts()}] rekey -> {suite}")

        gcs.stdin.write(suite + "\n")
        gcs.stdin.flush()

        baseline = _read_proxy_counters()

        try:
            ctl_send({"cmd": "mark", "suite": suite})
        except Exception as exc:
            print(f"[WARN] control mark failed for {suite}: {exc}", file=sys.stderr)
        try:
            follower_ready = wait_active_suite(suite, timeout=5.0)
            if not follower_ready:
                print(f"[WARN] follower did not report suite {suite} active before timeout", file=sys.stderr)
        except Exception:
            print(f"[WARN] follower status check failed for suite {suite}", file=sys.stderr)

        rekey_status = "timeout"

        try:

            result = wait_proxy_rekey(suite, baseline, timeout=15.0, proc=gcs)

            rekey_status = result

            if result == "timeout":

                print(f"[WARN] timed out waiting for proxy to activate suite {suite}", file=sys.stderr)

            elif result == "fail":

                print(f"[WARN] proxy reported failed rekey for suite {suite}", file=sys.stderr)

        except RuntimeError as exc:

            rekey_status = "error"

            raise

        except Exception as exc:

            rekey_status = "error"

            print(f"[WARN] error while waiting for proxy rekey {suite}: {exc}", file=sys.stderr)

        finally:

            try:

                ctl_send({"cmd": "rekey_complete", "suite": suite, "status": rekey_status})

            except Exception as exc:

                print(f"[WARN] rekey_complete failed for {suite}: {exc}", file=sys.stderr)

    return (time.time_ns() - start_ns) / 1_000_000




def run_suite(
    gcs: subprocess.Popen,
    suite: str,
    is_first: bool,
    duration_s: float,
    payload_bytes: int,
    event_sample: int,
    offset_ns: int,
    pass_index: int,
    traffic_mode: str,
    pre_gap: float,
    rate_pps: int,
    power_capture_enabled: bool,
) -> dict:
    rekey_duration_ms = activate_suite(gcs, suite, is_first)

    events_path = suite_outdir(suite) / EVENTS_FILENAME
    start_mark_ns = time.time_ns() + offset_ns + int(0.150 * 1e9) + int(max(pre_gap, 0.0) * 1e9)
    try:
        ctl_send({"cmd": "schedule_mark", "suite": suite, "t0_ns": start_mark_ns})
    except Exception as exc:
        print(f"[WARN] schedule_mark failed for {suite}: {exc}", file=sys.stderr)

    power_request_ok = False
    power_request_error: Optional[str] = None
    power_status: dict = {}
    if power_capture_enabled:
        power_start_ns = time.time_ns() + offset_ns + int(max(pre_gap, 0.0) * 1e9)
        power_resp = request_power_capture(suite, duration_s, power_start_ns)
        power_request_ok = bool(power_resp.get("ok"))
        power_request_error = power_resp.get("error") if not power_request_ok else None
        if not power_request_ok and power_request_error:
            print(f"[WARN] power capture not scheduled: {power_request_error}", file=sys.stderr)
        banner = f"[{ts()}] ===== POWER: START in {pre_gap:.1f}s | suite={suite} | duration={duration_s:.1f}s mode={traffic_mode} ====="
    else:
        power_request_error = "disabled"
        banner = (
            f"[{ts()}] ===== TRAFFIC: START in {pre_gap:.1f}s | suite={suite} | duration={duration_s:.1f}s "
            f"mode={traffic_mode} (power capture disabled) ====="
        )
    print(banner)
    if pre_gap > 0:
        time.sleep(pre_gap)

    warmup_s = min(MAX_WARMUP_SECONDS, duration_s * WARMUP_FRACTION)
    start_wall_ns = time.time_ns()
    start_perf_ns = time.perf_counter_ns()
    sent_packets = 0
    rcvd_packets = 0
    rcvd_bytes = 0
    avg_rtt_ns = 0
    max_rtt_ns = 0
    rtt_samples = 0
    blaster_sent_bytes = 0

    if traffic_mode == "blast":
        if warmup_s > 0:
            warmup_blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path=None,
                payload_bytes=payload_bytes,
                sample_every=0,
                offset_ns=offset_ns,
            )
            warmup_blaster.run(duration_s=warmup_s, rate_pps=rate_pps)
        start_wall_ns = time.time_ns()
        start_perf_ns = time.perf_counter_ns()
        blaster = Blaster(
            APP_SEND_HOST,
            APP_SEND_PORT,
            APP_RECV_HOST,
            APP_RECV_PORT,
            events_path,
            payload_bytes=payload_bytes,
            sample_every=event_sample,
            offset_ns=offset_ns,
        )
        blaster.run(duration_s=duration_s, rate_pps=rate_pps)
        sent_packets = blaster.sent
        rcvd_packets = blaster.rcvd
        rcvd_bytes = blaster.rcvd_bytes
        blaster_sent_bytes = blaster.sent_bytes
        sample_count = max(1, blaster.rtt_samples)
        avg_rtt_ns = blaster.rtt_sum_ns // sample_count
        max_rtt_ns = blaster.rtt_max_ns
        rtt_samples = blaster.rtt_samples
    else:
        time.sleep(duration_s)

    end_wall_ns = time.time_ns()
    end_perf_ns = time.perf_counter_ns()
    if power_capture_enabled:
        print(f"[{ts()}] ===== POWER: STOP | suite={suite} =====")
    else:
        print(f"[{ts()}] ===== TRAFFIC: STOP | suite={suite} =====")

    snapshot_proxy_artifacts(suite)
    proxy_stats = read_proxy_stats_live() or read_proxy_summary()

    if power_capture_enabled and power_request_ok:
        power_status = poll_power_status(max_wait_s=max(6.0, duration_s * 0.25))
        if power_status.get("error"):
            print(f"[WARN] power status error: {power_status['error']}", file=sys.stderr)

    power_summary = power_status.get("last_summary") if isinstance(power_status, dict) else None
    power_capture_complete = bool(power_summary)
    power_error = None
    if not power_capture_complete:
        if isinstance(power_status, dict):
            power_error = power_status.get("error")
            if not power_error and power_status.get("busy"):
                power_error = "capture_incomplete"
        if power_error is None:
            power_error = power_request_error

    elapsed_s = max(1e-9, (end_perf_ns - start_perf_ns) / 1e9)
    pps = sent_packets / elapsed_s if elapsed_s > 0 else 0.0
    throughput_mbps = (rcvd_bytes * 8) / (elapsed_s * 1_000_000) if elapsed_s > 0 else 0.0
    sent_mbps = (blaster_sent_bytes * 8) / (elapsed_s * 1_000_000) if blaster_sent_bytes else 0.0
    delivered_ratio = throughput_mbps / sent_mbps if sent_mbps > 0 else 0.0
    avg_rtt_ms = avg_rtt_ns / 1_000_000
    max_rtt_ms = max_rtt_ns / 1_000_000

    app_packet_bytes = payload_bytes + SEQ_TS_OVERHEAD_BYTES
    wire_packet_bytes_est = app_packet_bytes + UDP_HEADER_BYTES + APP_IP_HEADER_BYTES
    goodput_mbps = (rcvd_packets * payload_bytes * 8) / (elapsed_s * 1_000_000) if elapsed_s > 0 else 0.0
    wire_throughput_mbps_est = (
        (rcvd_packets * wire_packet_bytes_est * 8) / (elapsed_s * 1_000_000)
        if elapsed_s > 0
        else 0.0
    )
    if sent_mbps > 0:
        goodput_ratio = goodput_mbps / sent_mbps
        goodput_ratio = max(0.0, min(1.0, goodput_ratio))
    else:
        goodput_ratio = 0.0

    owd_p50_ms = 0.0
    owd_p95_ms = 0.0
    rtt_p50_ms = 0.0
    rtt_p95_ms = 0.0
    sample_quality = "low"
    owd_samples = 0

    if traffic_mode == "blast":
        owd_p50_ms = blaster.owd_p50_ns / 1_000_000
        owd_p95_ms = blaster.owd_p95_ns / 1_000_000
        rtt_p50_ms = blaster.rtt_p50_ns / 1_000_000
        rtt_p95_ms = blaster.rtt_p95_ns / 1_000_000
        owd_samples = blaster.owd_samples
        if blaster.rtt_samples >= MIN_DELAY_SAMPLES and blaster.owd_samples >= MIN_DELAY_SAMPLES:
            sample_quality = "ok"

    loss_pct = 0.0
    if sent_packets:
        loss_pct = max(0.0, (sent_packets - rcvd_packets) * 100.0 / sent_packets)
    loss_successes = max(0, sent_packets - rcvd_packets)
    loss_low, loss_high = wilson_interval(loss_successes, sent_packets)

    row = {
        "pass": pass_index,
        "suite": suite,
        "duration_s": round(elapsed_s, 3),
        "sent": sent_packets,
        "rcvd": rcvd_packets,
        "pps": round(pps, 1),
        "throughput_mbps": round(throughput_mbps, 3),
        "sent_mbps": round(sent_mbps, 3),
        "delivered_ratio": round(delivered_ratio, 3) if sent_mbps > 0 else 0.0,
        "goodput_mbps": round(goodput_mbps, 3),
        "wire_throughput_mbps_est": round(wire_throughput_mbps_est, 3),
        "app_packet_bytes": app_packet_bytes,
        "wire_packet_bytes_est": wire_packet_bytes_est,
        "goodput_ratio": round(goodput_ratio, 3),
        "rtt_avg_ms": round(avg_rtt_ms, 3),
        "rtt_max_ms": round(max_rtt_ms, 3),
        "rtt_p50_ms": round(rtt_p50_ms, 3),
        "rtt_p95_ms": round(rtt_p95_ms, 3),
        "owd_p50_ms": round(owd_p50_ms, 3),
        "owd_p95_ms": round(owd_p95_ms, 3),
        "rtt_samples": rtt_samples,
        "owd_samples": owd_samples,
        "sample_quality": sample_quality,
        "loss_pct": round(loss_pct, 3),
        "loss_pct_wilson_low": round(loss_low * 100.0, 3),
        "loss_pct_wilson_high": round(loss_high * 100.0, 3),
        "enc_out": proxy_stats.get("enc_out", 0),
        "enc_in": proxy_stats.get("enc_in", 0),
        "drops": proxy_stats.get("drops", 0),
        "rekeys_ok": proxy_stats.get("rekeys_ok", 0),
        "rekeys_fail": proxy_stats.get("rekeys_fail", 0),
        "start_ns": start_wall_ns,
        "end_ns": end_wall_ns,
        "rekey_ms": round(rekey_duration_ms, 3),
        "power_request_ok": power_request_ok,
        "power_capture_ok": power_capture_complete,
        "power_error": power_error,
        "power_avg_w": round(power_summary.get("avg_power_w", 0.0), 6) if power_summary else 0.0,
        "power_energy_j": round(power_summary.get("energy_j", 0.0), 6) if power_summary else 0.0,
        "power_samples": power_summary.get("samples") if power_summary else 0,
        "power_avg_current_a": round(power_summary.get("avg_current_a", 0.0), 6) if power_summary else 0.0,
        "power_avg_voltage_v": round(power_summary.get("avg_voltage_v", 0.0), 6) if power_summary else 0.0,
        "power_sample_rate_hz": round(power_summary.get("sample_rate_hz", 0.0), 3) if power_summary else 0.0,
        "power_duration_s": round(power_summary.get("duration_s", 0.0), 3) if power_summary else 0.0,
        "power_csv_path": power_summary.get("csv_path") if power_summary else "",
        "power_summary_path": power_summary.get("summary_json_path") if power_summary else "",
    }

    print(
        f"[{ts()}] <<< FINISH suite={suite} mode={traffic_mode} sent={sent_packets} rcvd={rcvd_packets} "
        f"pps~{pps:.0f} thr~{throughput_mbps:.2f} Mb/s sent~{sent_mbps:.2f} Mb/s loss={loss_pct:.2f}% "
        f"rtt_avg={avg_rtt_ms:.3f}ms rtt_max={max_rtt_ms:.3f}ms rekey={rekey_duration_ms:.2f}ms "
        f"enc_out={row['enc_out']} enc_in={row['enc_in']} >>>"
    )

    return row


def write_summary(rows: List[dict]) -> None:
    if not rows:
        return
    mkdirp(OUTDIR)
    with open(SUMMARY_CSV, "w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=list(rows[0].keys()))
        writer.writeheader()
        writer.writerows(rows)
    print(f"[{ts()}] wrote {SUMMARY_CSV}")


class SaturationTester:
    def __init__(
        self,
        suite: str,
        payload_bytes: int,
        duration_s: float,
        event_sample: int,
        offset_ns: int,
        output_dir: Path,
        max_rate_mbps: int,
        search_mode: str,
        delivery_threshold: float,
        loss_threshold: float,
        spike_factor: float,
    ) -> None:
        self.suite = suite
        self.payload_bytes = payload_bytes
        self.duration_s = duration_s
        self.event_sample = event_sample
        self.offset_ns = offset_ns
        self.output_dir = output_dir
        self.max_rate_mbps = max_rate_mbps
        self.search_mode = search_mode
        self.delivery_threshold = delivery_threshold
        self.loss_threshold = loss_threshold
        self.spike_factor = spike_factor
        self.records: List[Dict[str, float]] = []
        self._rate_cache: Dict[int, Tuple[Dict[str, float], bool, Optional[str]]] = {}
        self._baseline: Optional[Dict[str, float]] = None
        self._signal_history = {key: deque(maxlen=HYSTERESIS_WINDOW) for key in SATURATION_SIGNALS}
        self._last_ok_rate: Optional[int] = None
        self._first_bad_rate: Optional[int] = None
        self._stop_cause: Optional[str] = None
        self._stop_samples = 0

    def run(self) -> Dict[str, Optional[float]]:
        self.records = []
        self._rate_cache.clear()
        self._baseline = None
        self._signal_history = {key: deque(maxlen=HYSTERESIS_WINDOW) for key in SATURATION_SIGNALS}
        self._last_ok_rate = None
        self._first_bad_rate = None
        self._stop_cause = None
        self._stop_samples = 0

        used_mode = self.search_mode
        if self.search_mode == "linear":
            self._linear_search()
        else:
            self._coarse_search()
            if self._first_bad_rate is not None and self._last_ok_rate is not None:
                self._bisect_search()
            elif self.search_mode == "bisect" and self._first_bad_rate is None:
                self._linear_search()
                used_mode = "linear"

        resolution = None
        if self._first_bad_rate is not None and self._last_ok_rate is not None:
            resolution = max(0, self._first_bad_rate - self._last_ok_rate)
        saturation_point = self._last_ok_rate if self._last_ok_rate is not None else self._first_bad_rate
        confidence = min(1.0, self._stop_samples / 200.0) if self._stop_samples > 0 else 0.0

        baseline = self._baseline or {}
        return {
            "suite": self.suite,
            "baseline_owd_p50_ms": baseline.get("owd_p50_ms"),
            "baseline_owd_p95_ms": baseline.get("owd_p95_ms"),
            "baseline_rtt_p50_ms": baseline.get("rtt_p50_ms"),
            "baseline_rtt_p95_ms": baseline.get("rtt_p95_ms"),
            "saturation_point_mbps": saturation_point,
            "stop_cause": self._stop_cause,
            "confidence": round(confidence, 3),
            "search_mode": used_mode,
            "resolution_mbps": resolution,
        }

    def _linear_search(self) -> None:
        for rate in SATURATION_LINEAR_RATES:
            if rate > self.max_rate_mbps:
                break
            _, is_bad, _ = self._evaluate_rate(rate)
            if is_bad:
                break

    def _coarse_search(self) -> None:
        for rate in SATURATION_COARSE_RATES:
            if rate > self.max_rate_mbps:
                break
            _, is_bad, _ = self._evaluate_rate(rate)
            if is_bad:
                break

    def _bisect_search(self) -> None:
        if self._first_bad_rate is None:
            return
        lo = self._last_ok_rate if self._last_ok_rate is not None else 0
        hi = self._first_bad_rate
        steps = 0
        while hi - lo > 5 and steps < MAX_BISECT_STEPS:
            mid = max(1, int(round((hi + lo) / 2)))
            if mid == hi or mid == lo:
                break
            _, is_bad, _ = self._evaluate_rate(mid)
            steps += 1
            metrics = self._rate_cache[mid][0]
            sample_ok = metrics.get("sample_quality") == "ok"
            if not sample_ok:
                is_bad = True
            if is_bad:
                if mid < hi:
                    hi = mid
                if self._first_bad_rate is None or mid < self._first_bad_rate:
                    self._first_bad_rate = mid
            else:
                if mid > lo:
                    lo = mid
                if self._last_ok_rate is None or mid > self._last_ok_rate:
                    self._last_ok_rate = mid

    def _evaluate_rate(self, rate: int) -> Tuple[Dict[str, float], bool, Optional[str]]:
        cached = self._rate_cache.get(rate)
        if cached:
            return cached

        metrics = self._run_rate(rate)
        metrics["suite"] = self.suite
        self.records.append(metrics)

        if self._baseline is None and metrics.get("sample_quality") == "ok":
            self._baseline = {
                "owd_p50_ms": metrics.get("owd_p50_ms"),
                "owd_p95_ms": metrics.get("owd_p95_ms"),
                "rtt_p50_ms": metrics.get("rtt_p50_ms"),
                "rtt_p95_ms": metrics.get("rtt_p95_ms"),
            }

        signals = self._classify_signals(metrics)
        is_bad = any(signals.values())
        cause = self._update_history(signals, rate, metrics)
        if is_bad:
            if self._first_bad_rate is None or rate < self._first_bad_rate:
                self._first_bad_rate = rate
        else:
            if metrics.get("sample_quality") == "ok":
                if self._last_ok_rate is None or rate > self._last_ok_rate:
                    self._last_ok_rate = rate

        result = (metrics, is_bad, cause)
        self._rate_cache[rate] = result
        return result

    def _classify_signals(self, metrics: Dict[str, float]) -> Dict[str, bool]:
        signals = {key: False for key in SATURATION_SIGNALS}
        baseline = self._baseline
        if baseline and baseline.get("owd_p95_ms"):
            baseline_p95 = baseline.get("owd_p95_ms", 0.0)
            if baseline_p95 > 0:
                signals["owd_p95_spike"] = metrics.get("owd_p95_ms", 0.0) > baseline_p95 * self.spike_factor
        signals["delivery_degraded"] = metrics.get("goodput_ratio", 0.0) < self.delivery_threshold
        signals["loss_excess"] = metrics.get("loss_pct", 0.0) > self.loss_threshold
        return signals

    def _update_history(
        self,
        signals: Dict[str, bool],
        rate: int,
        metrics: Dict[str, float],
    ) -> Optional[str]:
        cause = None
        for key in SATURATION_SIGNALS:
            history = self._signal_history[key]
            history.append(bool(signals.get(key)))
            if self._stop_cause is None and sum(history) >= 2:
                self._stop_cause = key
                self._stop_samples = max(metrics.get("rtt_samples", 0), metrics.get("owd_samples", 0))
                cause = key
        return cause

    def _run_rate(self, rate_mbps: int) -> Dict[str, float]:
        denominator = max(self.payload_bytes * 8, 1)
        rate_pps = int((rate_mbps * 1_000_000) / denominator)
        if rate_pps <= 0:
            rate_pps = 1
        events_path = self.output_dir / f"saturation_{rate_mbps}Mbps.jsonl"
        warmup_s = min(MAX_WARMUP_SECONDS, self.duration_s * WARMUP_FRACTION)
        if warmup_s > 0:
            warmup_blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path=None,
                payload_bytes=self.payload_bytes,
                sample_every=0,
                offset_ns=self.offset_ns,
            )
            warmup_blaster.run(duration_s=warmup_s, rate_pps=rate_pps)
        blaster = Blaster(
            APP_SEND_HOST,
            APP_SEND_PORT,
            APP_RECV_HOST,
            APP_RECV_PORT,
            events_path,
            payload_bytes=self.payload_bytes,
            sample_every=max(1, self.event_sample),
            offset_ns=self.offset_ns,
        )
        start = time.perf_counter()
        blaster.run(duration_s=self.duration_s, rate_pps=rate_pps)
        elapsed = max(1e-9, time.perf_counter() - start)

        sent_packets = blaster.sent
        rcvd_packets = blaster.rcvd
        sent_bytes = blaster.sent_bytes
        rcvd_bytes = blaster.rcvd_bytes

        pps_actual = sent_packets / elapsed if elapsed > 0 else 0.0
        throughput_mbps = (rcvd_bytes * 8) / (elapsed * 1_000_000) if elapsed > 0 else 0.0
        sent_mbps = (sent_bytes * 8) / (elapsed * 1_000_000) if sent_bytes else 0.0
        delivered_ratio = throughput_mbps / sent_mbps if sent_mbps > 0 else 0.0

        avg_rtt_ms = (blaster.rtt_sum_ns / max(1, blaster.rtt_samples)) / 1_000_000 if blaster.rtt_samples else 0.0
        min_rtt_ms = (blaster.rtt_min_ns or 0) / 1_000_000
        max_rtt_ms = blaster.rtt_max_ns / 1_000_000

        app_packet_bytes = self.payload_bytes + SEQ_TS_OVERHEAD_BYTES
        wire_packet_bytes_est = app_packet_bytes + UDP_HEADER_BYTES + APP_IP_HEADER_BYTES
        goodput_mbps = (
            (rcvd_packets * self.payload_bytes * 8) / (elapsed * 1_000_000)
            if elapsed > 0
            else 0.0
        )
        wire_throughput_mbps_est = (
            (rcvd_packets * wire_packet_bytes_est * 8) / (elapsed * 1_000_000)
            if elapsed > 0
            else 0.0
        )
        if sent_mbps > 0:
            goodput_ratio = goodput_mbps / sent_mbps
            goodput_ratio = max(0.0, min(1.0, goodput_ratio))
        else:
            goodput_ratio = 0.0

        loss_pct = 0.0
        if sent_packets:
            loss_pct = max(0.0, (sent_packets - rcvd_packets) * 100.0 / sent_packets)
        loss_low, loss_high = wilson_interval(max(0, sent_packets - rcvd_packets), sent_packets)

        sample_quality = "low"
        if blaster.rtt_samples >= MIN_DELAY_SAMPLES and blaster.owd_samples >= MIN_DELAY_SAMPLES:
            sample_quality = "ok"

        return {
            "rate_mbps": float(rate_mbps),
            "pps": float(rate_pps),
            "pps_actual": round(pps_actual, 1),
            "sent_mbps": round(sent_mbps, 3),
            "throughput_mbps": round(throughput_mbps, 3),
            "goodput_mbps": round(goodput_mbps, 3),
            "wire_throughput_mbps_est": round(wire_throughput_mbps_est, 3),
            "goodput_ratio": round(goodput_ratio, 3),
            "loss_pct": round(loss_pct, 3),
            "loss_pct_wilson_low": round(loss_low * 100.0, 3),
            "loss_pct_wilson_high": round(loss_high * 100.0, 3),
            "delivered_ratio": round(delivered_ratio, 3) if sent_mbps > 0 else 0.0,
            "avg_rtt_ms": round(avg_rtt_ms, 3),
            "min_rtt_ms": round(min_rtt_ms, 3),
            "max_rtt_ms": round(max_rtt_ms, 3),
            "rtt_p50_ms": round(blaster.rtt_p50_ns / 1_000_000, 3),
            "rtt_p95_ms": round(blaster.rtt_p95_ns / 1_000_000, 3),
            "owd_p50_ms": round(blaster.owd_p50_ns / 1_000_000, 3),
            "owd_p95_ms": round(blaster.owd_p95_ns / 1_000_000, 3),
            "rtt_samples": blaster.rtt_samples,
            "owd_samples": blaster.owd_samples,
            "sample_quality": sample_quality,
            "app_packet_bytes": app_packet_bytes,
            "wire_packet_bytes_est": wire_packet_bytes_est,
        }

    def export_excel(self, session_id: str) -> Optional[Path]:
        if Workbook is None:
            print("[WARN] openpyxl not available; skipping Excel export")
            return None
        EXCEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        path = EXCEL_OUTPUT_DIR / f"saturation_{self.suite}_{session_id}.xlsx"
        wb = Workbook()
        ws = wb.active
        ws.title = "Saturation"
        ws.append([
            "rate_mbps",
            "pps",
            "pps_actual",
            "sent_mbps",
            "throughput_mbps",
            "goodput_mbps",
            "wire_throughput_mbps_est",
            "goodput_ratio",
            "loss_pct",
            "loss_pct_wilson_low",
            "loss_pct_wilson_high",
            "delivered_ratio",
            "avg_rtt_ms",
            "min_rtt_ms",
            "max_rtt_ms",
            "rtt_p50_ms",
            "rtt_p95_ms",
            "owd_p50_ms",
            "owd_p95_ms",
            "rtt_samples",
            "owd_samples",
            "sample_quality",
            "app_packet_bytes",
            "wire_packet_bytes_est",
        ])
        for record in self.records:
            ws.append([
                record.get("rate_mbps", 0.0),
                record.get("pps", 0.0),
                record.get("pps_actual", 0.0),
                record.get("sent_mbps", 0.0),
                record.get("throughput_mbps", 0.0),
                record.get("goodput_mbps", 0.0),
                record.get("wire_throughput_mbps_est", 0.0),
                record.get("goodput_ratio", 0.0),
                record.get("loss_pct", 0.0),
                record.get("loss_pct_wilson_low", 0.0),
                record.get("loss_pct_wilson_high", 0.0),
                record.get("delivered_ratio", 0.0),
                record.get("avg_rtt_ms", 0.0),
                record.get("min_rtt_ms", 0.0),
                record.get("max_rtt_ms", 0.0),
                record.get("rtt_p50_ms", 0.0),
                record.get("rtt_p95_ms", 0.0),
                record.get("owd_p50_ms", 0.0),
                record.get("owd_p95_ms", 0.0),
                record.get("rtt_samples", 0),
                record.get("owd_samples", 0),
                record.get("sample_quality", "low"),
                record.get("app_packet_bytes", 0),
                record.get("wire_packet_bytes_est", 0),
            ])
        wb.save(path)
        return path


class TelemetryCollector:
    def __init__(self, host: str, port: int) -> None:
        self.host = host
        self.port = port
        self.stop_event = threading.Event()
        self.server: Optional[socket.socket] = None
        self.accept_thread: Optional[threading.Thread] = None
        self.client_threads: List[threading.Thread] = []
        # Bug #9 fix: Use deque with maxlen to prevent unbounded memory growth
        from collections import deque

        env_maxlen = os.getenv("GCS_TELEM_MAXLEN")
        maxlen = 100000
        if env_maxlen is not None:
            try:
                maxlen = max(1000, int(env_maxlen))
            except (TypeError, ValueError):
                maxlen = 100000
        self.samples: deque = deque(maxlen=maxlen)  # ~10MB limit for long tests
        self.lock = threading.Lock()
        self.enabled = True

    def start(self) -> None:
        try:
            srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            srv.bind((self.host, self.port))
            srv.listen(8)
            srv.settimeout(0.5)
            self.server = srv
            self.accept_thread = threading.Thread(target=self._accept_loop, daemon=True)
            self.accept_thread.start()
            print(f"[{ts()}] telemetry collector listening on {self.host}:{self.port}")
        except Exception as exc:
            print(f"[WARN] telemetry collector disabled: {exc}", file=sys.stderr)
            self.enabled = False
            if self.server:
                try:
                    self.server.close()
                except Exception:
                    pass
            self.server = None

    def _accept_loop(self) -> None:
        assert self.server is not None
        while not self.stop_event.is_set():
            try:
                conn, addr = self.server.accept()
            except socket.timeout:
                continue
            except OSError:
                break
            except Exception as exc:
                if not self.stop_event.is_set():
                    print(f"[WARN] telemetry accept error: {exc}", file=sys.stderr)
                continue
            thread = threading.Thread(target=self._client_loop, args=(conn, addr), daemon=True)
            thread.start()
            self.client_threads.append(thread)

    def _client_loop(self, conn: socket.socket, addr) -> None:
        peer = f"{addr[0]}:{addr[1]}"
        try:
            conn.settimeout(1.0)
            with conn, conn.makefile("r", encoding="utf-8") as reader:
                for line in reader:
                    if self.stop_event.is_set():
                        break
                    data = line.strip()
                    if not data:
                        continue
                    try:
                        payload = json.loads(data)
                    except json.JSONDecodeError:
                        continue
                    payload.setdefault("collector_ts_ns", time.time_ns())
                    payload.setdefault("source", "drone")
                    payload.setdefault("peer", peer)
                    with self.lock:
                        self.samples.append(payload)
        except Exception:
            # drop connection silently
            pass

    def snapshot(self) -> List[dict]:
        with self.lock:
            # Convert deque to list for compatibility
            return list(self.samples)

    def stop(self) -> None:
        self.stop_event.set()
        if self.server:
            try:
                self.server.close()
            except Exception:
                pass
        if self.accept_thread and self.accept_thread.is_alive():
            self.accept_thread.join(timeout=1.5)
        for thread in self.client_threads:
            if thread.is_alive():
                thread.join(timeout=1.0)

def resolve_under_root(path: Path) -> Path:
    expanded = path.expanduser()
    return expanded if expanded.is_absolute() else ROOT / expanded


def safe_sheet_name(name: str) -> str:
    sanitized = "".join("_" if ch in '[]:*?/\\' else ch for ch in name).strip()
    if not sanitized:
        sanitized = "Sheet"
    return sanitized[:31]


def unique_sheet_name(workbook, base_name: str) -> str:
    base = safe_sheet_name(base_name)
    if base not in workbook.sheetnames:
        return base
    index = 1
    while True:
        suffix = f"_{index}"
        name = base[: 31 - len(suffix)] + suffix
        if name not in workbook.sheetnames:
            return name
        index += 1


def append_dict_sheet(workbook, title: str, rows: List[dict]) -> None:
    if not rows:
        return
    sheet_name = unique_sheet_name(workbook, title)
    ws = workbook.create_sheet(sheet_name)
    headers: List[str] = []
    for row in rows:
        for key in row.keys():
            if key not in headers:
                headers.append(key)
    ws.append(headers)
    for row in rows:
        ws.append([row.get(header, "") for header in headers])


def append_csv_sheet(workbook, path: Path, title: str) -> None:
    if not path.exists():
        return
    try:
        with open(path, newline="", encoding="utf-8") as handle:
            reader = csv.reader(handle)
            rows = list(reader)
    except Exception as exc:
        print(f"[WARN] failed to read CSV {path}: {exc}", file=sys.stderr)
        return
    if not rows:
        return
    sheet_name = unique_sheet_name(workbook, title)
    ws = workbook.create_sheet(sheet_name)
    for row in rows:
        ws.append(row)


def locate_drone_session_dir(session_id: str) -> Optional[Path]:
    candidates = []
    try:
        candidates.append(resolve_under_root(DRONE_MONITOR_BASE) / session_id)
    except Exception:
        pass
    fallback = Path("/home/dev/research/output/drone") / session_id
    candidates.append(fallback)
    repo_default = ROOT / "output" / "drone" / session_id
    candidates.append(repo_default)
    seen = set()
    for candidate in candidates:
        if candidate in seen:
            continue
        seen.add(candidate)
        try:
            if candidate.exists():
                return candidate
        except Exception:
            continue
    return None


def export_combined_excel(
    session_id: str,
    summary_rows: List[dict],
    saturation_overview: List[dict],
    saturation_samples: List[dict],
    telemetry_samples: List[dict],
) -> Optional[Path]:
    if Workbook is None:
        print("[WARN] openpyxl not available; skipping combined Excel export", file=sys.stderr)
        return None

    workbook = Workbook()
    info_sheet = workbook.active
    info_sheet.title = "run_info"
    info_sheet.append(["generated_utc", ts()])
    info_sheet.append(["session_id", session_id])

    append_dict_sheet(workbook, "gcs_summary", summary_rows)
    append_dict_sheet(workbook, "saturation_overview", saturation_overview)
    append_dict_sheet(workbook, "saturation_samples", saturation_samples)
    append_dict_sheet(workbook, "telemetry_samples", telemetry_samples)

    if SUMMARY_CSV.exists():
        append_csv_sheet(workbook, SUMMARY_CSV, "gcs_summary_csv")

    drone_session_dir = locate_drone_session_dir(session_id)
    if drone_session_dir:
        info_sheet.append(["drone_session_dir", str(drone_session_dir)])
        for csv_path in sorted(drone_session_dir.glob("*.csv")):
            append_csv_sheet(workbook, csv_path, csv_path.stem[:31])
    else:
        info_sheet.append(["drone_session_dir", "not_found"])

    combined_dir = resolve_under_root(COMBINED_OUTPUT_DIR)
    combined_dir.mkdir(parents=True, exist_ok=True)
    target_path = combined_dir / f"{session_id}_combined.xlsx"
    workbook.save(target_path)
    return target_path


def main() -> None:
    OUTDIR.mkdir(parents=True, exist_ok=True)

    auto = AUTO_GCS_CONFIG

    traffic_mode = str(auto.get("traffic") or "blast").lower()
    pre_gap = float(auto.get("pre_gap_s") or 1.0)
    inter_gap = float(auto.get("inter_gap_s") or 15.0)
    duration = float(auto.get("duration_s") or 45.0)
    payload_bytes = int(auto.get("payload_bytes") or 256)
    event_sample = int(auto.get("event_sample") or 100)
    passes = int(auto.get("passes") or 1)
    rate_pps = int(auto.get("rate_pps") or 0)
    bandwidth_mbps = float(auto.get("bandwidth_mbps") or 0.0)
    max_rate_mbps = float(auto.get("max_rate_mbps") or 200.0)
    if bandwidth_mbps > 0:
        denominator = max(payload_bytes * 8, 1)
        rate_pps = max(1, int((bandwidth_mbps * 1_000_000) / denominator))

    sat_search_cfg = str(auto.get("sat_search") or SATURATION_SEARCH_MODE).lower()
    if sat_search_cfg not in {"auto", "linear", "bisect"}:
        sat_search_cfg = SATURATION_SEARCH_MODE
    sat_delivery_threshold = float(auto.get("sat_delivery_threshold") or SATURATION_DELIVERY_THRESHOLD)
    sat_loss_threshold = float(auto.get("sat_loss_threshold_pct") or SATURATION_LOSS_THRESHOLD)
    sat_spike_factor = float(auto.get("sat_rtt_spike_factor") or SATURATION_RTT_SPIKE)

    if duration <= 0:
        raise ValueError("AUTO_GCS.duration_s must be positive")
    if pre_gap < 0:
        raise ValueError("AUTO_GCS.pre_gap_s must be >= 0")
    if inter_gap < 0:
        raise ValueError("AUTO_GCS.inter_gap_s must be >= 0")
    if rate_pps < 0:
        raise ValueError("AUTO_GCS.rate_pps must be >= 0")
    if passes <= 0:
        raise ValueError("AUTO_GCS.passes must be >= 1")

    if traffic_mode not in {"blast", "mavproxy", "saturation"}:
        raise ValueError(f"Unsupported traffic mode: {traffic_mode}")

    suites_override = auto.get("suites")
    suites = resolve_suites(suites_override)
    if not suites:
        raise RuntimeError("No suites selected for execution")

    session_prefix = str(auto.get("session_prefix") or "session")
    session_id = os.environ.get("GCS_SESSION_ID") or f"{session_prefix}_{int(time.time())}"
    print(f"[{ts()}] session_id={session_id}")

    initial_suite = preferred_initial_suite(suites)
    if initial_suite and suites[0] != initial_suite:
        suites = [initial_suite] + [s for s in suites if s != initial_suite]
        print(f"[{ts()}] reordered suites to start with {initial_suite} (from CONFIG)")

    power_capture_enabled = bool(auto.get("power_capture", True))

    telemetry_enabled = bool(auto.get("telemetry_enabled", True))
    telemetry_bind_host = auto.get("telemetry_bind_host") or TELEMETRY_BIND_HOST
    telemetry_port_cfg = auto.get("telemetry_port")
    telemetry_port = TELEMETRY_PORT if telemetry_port_cfg in (None, "") else int(telemetry_port_cfg)

    print(
        f"[{ts()}] traffic={traffic_mode} duration={duration:.1f}s pre_gap={pre_gap:.1f}s "
        f"inter_gap={inter_gap:.1f}s payload={payload_bytes}B event_sample={event_sample} passes={passes} "
        f"rate_pps={rate_pps} sat_search={sat_search_cfg}"
    )
    if bandwidth_mbps > 0:
        print(f"[{ts()}] bandwidth target {bandwidth_mbps:.2f} Mbps -> approx {rate_pps} pps")
    print(f"[{ts()}] power capture: {'enabled' if power_capture_enabled else 'disabled'}")

    reachable = False
    for attempt in range(8):
        try:
            resp = ctl_send({"cmd": "ping"}, timeout=1.0, retries=1)
            if resp.get("ok"):
                reachable = True
                break
        except Exception:
            pass
        time.sleep(0.5)
    if reachable:
        print(f"[{ts()}] follower reachable at {DRONE_HOST}:{CONTROL_PORT}")
    else:
        print(f"[WARN] follower not reachable at {DRONE_HOST}:{CONTROL_PORT}", file=sys.stderr)

    offset_ns = 0
    try:
        sync = timesync()
        offset_ns = sync["offset_ns"]
        print(f"[{ts()}] clocks synced: offset_ns={offset_ns} ns, link_rtt~{sync['rtt_ns']} ns")
    except Exception as exc:
        print(f"[WARN] timesync failed: {exc}", file=sys.stderr)

    telemetry_collector: Optional[TelemetryCollector] = None
    if telemetry_enabled:
        telemetry_collector = TelemetryCollector(telemetry_bind_host, telemetry_port)
        telemetry_collector.start()
        print(f"[{ts()}] telemetry collector -> {telemetry_bind_host}:{telemetry_port}")
    else:
        print(f"[{ts()}] telemetry collector disabled via AUTO_GCS configuration")

    if not bool(auto.get("launch_proxy", True)):
        raise NotImplementedError("AUTO_GCS.launch_proxy=False is not supported")

    gcs_proc: Optional[subprocess.Popen] = None
    log_handle = None
    gcs_proc, log_handle = start_gcs_proxy(suites[0])

    try:
        ready = wait_handshake(timeout=20.0)
        print(f"[{ts()}] initial handshake ready? {ready}")

        summary_rows: List[dict] = []
        saturation_reports: List[dict] = []
        all_rate_samples: List[dict] = []
        telemetry_samples: List[dict] = []

        if traffic_mode == "saturation":
            for idx, suite in enumerate(suites):
                rekey_ms = activate_suite(gcs_proc, suite, is_first=(idx == 0))
                outdir = suite_outdir(suite)
                tester = SaturationTester(
                    suite=suite,
                    payload_bytes=payload_bytes,
                    duration_s=duration,
                    event_sample=event_sample,
                    offset_ns=offset_ns,
                    output_dir=outdir,
                    max_rate_mbps=int(max_rate_mbps),
                    search_mode=sat_search_cfg,
                    delivery_threshold=sat_delivery_threshold,
                    loss_threshold=sat_loss_threshold,
                    spike_factor=sat_spike_factor,
                )
                summary = tester.run()
                summary["rekey_ms"] = rekey_ms
                excel_path = tester.export_excel(session_id)
                if excel_path:
                    summary["excel_path"] = str(excel_path)
                saturation_reports.append(summary)
                all_rate_samples.extend(dict(record) for record in tester.records)
                if inter_gap > 0 and idx < len(suites) - 1:
                    time.sleep(inter_gap)
            report_path = OUTDIR / f"saturation_summary_{session_id}.json"
            with open(report_path, "w", encoding="utf-8") as handle:
                json.dump(saturation_reports, handle, indent=2)
            print(f"[{ts()}] saturation summary written to {report_path}")
        else:
            for pass_index in range(passes):
                for idx, suite in enumerate(suites):
                    row = run_suite(
                        gcs_proc,
                        suite,
                        is_first=(pass_index == 0 and idx == 0),
                        duration_s=duration,
                        payload_bytes=payload_bytes,
                        event_sample=event_sample,
                        offset_ns=offset_ns,
                        pass_index=pass_index,
                        traffic_mode=traffic_mode,
                        pre_gap=pre_gap,
                        rate_pps=rate_pps,
                        power_capture_enabled=power_capture_enabled,
                    )
                    summary_rows.append(row)
                    is_last_suite = idx == len(suites) - 1
                    is_last_pass = pass_index == passes - 1
                    if inter_gap > 0 and not (is_last_suite and is_last_pass):
                        time.sleep(inter_gap)

            write_summary(summary_rows)

        if telemetry_collector and telemetry_collector.enabled:
            telemetry_samples = telemetry_collector.snapshot()

        if auto.get("export_combined_excel", True):
            combined_path = export_combined_excel(
                session_id=session_id,
                summary_rows=summary_rows,
                saturation_overview=saturation_reports,
                saturation_samples=all_rate_samples,
                telemetry_samples=telemetry_samples,
            )
            if combined_path:
                print(f"[{ts()}] combined workbook written to {combined_path}")

    finally:
        try:
            ctl_send({"cmd": "stop"})
        except Exception:
            pass

        if gcs_proc and gcs_proc.stdin:
            try:
                gcs_proc.stdin.write("quit\n")
                gcs_proc.stdin.flush()
            except Exception:
                pass
        if gcs_proc:
            try:
                gcs_proc.wait(timeout=5)
            except Exception:
                gcs_proc.kill()

        if log_handle:
            try:
                log_handle.close()
            except Exception:
                pass

        if telemetry_collector:
            telemetry_collector.stop()


if __name__ == "__main__":
    main()

----------------
#!/usr/bin/env python3
"""GCS follower that exposes the control channel for a drone-side scheduler."""

from __future__ import annotations

import argparse
import json
import os
import queue
import signal
import socket
import subprocess
import sys
import threading
import time
from pathlib import Path
from typing import Dict, Optional

# Ensure project root is on sys.path so `import core` works when running this file
ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from core import suites as suites_mod
from core.config import CONFIG


DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

CONTROL_HOST = CONFIG.get("GCS_CONTROL_HOST", "0.0.0.0")
CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", CONFIG.get("DRONE_CONTROL_PORT", 48080)))

APP_BIND_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))
APP_SEND_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))

TELEMETRY_DEFAULT_HOST = (
    CONFIG.get("DRONE_TELEMETRY_HOST")
    or CONFIG.get("DRONE_HOST")
    or "127.0.0.1"
)
TELEMETRY_DEFAULT_PORT = int(
    CONFIG.get("DRONE_TELEMETRY_PORT")
    or CONFIG.get("GCS_TELEMETRY_PORT")
    or 52080
)

OUTDIR = Path("logs/auto/gcs_follower")
MARK_DIR = OUTDIR / "marks"
SECRETS_DIR = Path("secrets/matrix")

PROXY_STATUS_PATH = OUTDIR / "gcs_status.json"
PROXY_SUMMARY_PATH = OUTDIR / "gcs_summary.json"


def ts() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


class TelemetryPublisher:
    """Best-effort telemetry transport towards the drone scheduler."""

    def __init__(self, host: str, port: int, session_id: str) -> None:
        self.host = host
        self.port = port
        self.session_id = session_id
        self.queue: "queue.Queue[dict]" = queue.Queue(maxsize=5000)
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.sock: Optional[socket.socket] = None
        self.writer = None

    def start(self) -> None:
        self.thread.start()

    def publish(self, kind: str, payload: Dict[str, object]) -> None:
        if self.stop_event.is_set():
            return
        message = {"session_id": self.session_id, "kind": kind, **payload}
        try:
            self.queue.put_nowait(message)
        except queue.Full:
            try:
                self.queue.get_nowait()
            except queue.Empty:
                pass
            try:
                self.queue.put_nowait(message)
            except queue.Full:
                pass

    def stop(self) -> None:
        self.stop_event.set()
        if self.thread.is_alive():
            self.thread.join(timeout=2.0)
        self._close_socket()

    def _close_socket(self) -> None:
        if self.writer is not None:
            try:
                self.writer.close()
            except Exception:
                pass
            self.writer = None
        if self.sock is not None:
            try:
                self.sock.close()
            except Exception:
                pass
            self.sock = None

    def _ensure_connection(self) -> bool:
        if self.sock is not None and self.writer is not None:
            return True
        try:
            sock = socket.create_connection((self.host, self.port), timeout=3.0)
            self.sock = sock
            self.writer = sock.makefile("w", encoding="utf-8", buffering=1)
            hello = {
                "session_id": self.session_id,
                "kind": "telemetry_hello",
                "timestamp_ns": time.time_ns(),
                "source": "gcs-follower",
            }
            self.writer.write(json.dumps(hello) + "\n")
            self.writer.flush()
            return True
        except Exception:
            self._close_socket()
            return False

    def _run(self) -> None:
        backoff = 1.0
        while not self.stop_event.is_set():
            if not self._ensure_connection():
                time.sleep(min(backoff, 5.0))
                backoff = min(backoff * 1.5, 5.0)
                continue
            backoff = 1.0
            try:
                item = self.queue.get(timeout=0.5)
            except queue.Empty:
                continue
            try:
                if self.writer is None:
                    continue
                if "timestamp_ns" not in item:
                    item["timestamp_ns"] = time.time_ns()
                self.writer.write(json.dumps(item) + "\n")
                self.writer.flush()
            except Exception:
                self._close_socket()


def popen(cmd, **kw) -> subprocess.Popen:
    if isinstance(cmd, (list, tuple)):
        display = " ".join(str(part) for part in cmd)
    else:
        display = str(cmd)
    print(f"[{ts()}] exec: {display}", flush=True)
    return subprocess.Popen(cmd, **kw)


def killtree(proc: Optional[subprocess.Popen]) -> None:
    if not proc or proc.poll() is not None:
        return
    try:
        proc.terminate()
        proc.wait(timeout=3)
    except Exception:
        try:
            proc.kill()
        except Exception:
            pass


def discover_initial_suite() -> str:
    configured = CONFIG.get("SIMPLE_INITIAL_SUITE")
    if configured:
        return configured
    suite_map = suites_mod.list_suites()
    if suite_map:
        first = sorted(suite_map.keys())[0]
        return suites_mod.get_suite(first)["suite_id"]
    if SECRETS_DIR.exists():
        for path in sorted(SECRETS_DIR.iterdir()):
            if (path / "gcs_signing.key").exists():
                return path.name
    return "cs-mlkem768-aesgcm-mldsa65"


def suite_outdir(suite: str) -> Path:
    path = OUTDIR / "suites" / suite
    path.mkdir(parents=True, exist_ok=True)
    return path


def suite_secrets_dir(suite: str) -> Path:
    return SECRETS_DIR / suite


def write_marker(suite: str) -> None:
    MARK_DIR.mkdir(parents=True, exist_ok=True)
    marker = MARK_DIR / f"{int(time.time())}_{suite}.json"
    with open(marker, "w", encoding="utf-8") as handle:
        json.dump({"ts": ts(), "suite": suite}, handle)


def read_json(path: Path) -> dict:
    try:
        with open(path, encoding="utf-8") as handle:
            return json.load(handle)
    except Exception:
        return {}


def read_proxy_counters() -> dict:
    data = read_json(PROXY_STATUS_PATH)
    counters = data.get("counters") if isinstance(data, dict) else None
    if isinstance(counters, dict) and counters:
        return counters
    summary = read_json(PROXY_SUMMARY_PATH)
    if isinstance(summary, dict):
        summary_counters = summary.get("counters")
        if isinstance(summary_counters, dict) and summary_counters:
            return summary_counters
        if any(key in summary for key in ("enc_out", "enc_in", "rekeys_ok", "rekeys_fail")):
            return summary
    return {}


def read_proxy_status() -> dict:
    data = read_json(PROXY_STATUS_PATH)
    if isinstance(data, dict) and data:
        return data
    return read_json(PROXY_SUMMARY_PATH)


def start_gcs_proxy(suite: str) -> tuple[subprocess.Popen, object]:
    key_path = suite_secrets_dir(suite) / "gcs_signing.key"
    if not key_path.exists():
        raise FileNotFoundError(f"Missing GCS signing key for suite {suite}: {key_path}")

    OUTDIR.mkdir(parents=True, exist_ok=True)
    log_path = OUTDIR / f"gcs_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_handle = open(log_path, "w", encoding="utf-8", errors="replace")

    os.environ["DRONE_HOST"] = DRONE_HOST
    os.environ["GCS_HOST"] = GCS_HOST
    os.environ["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    os.environ["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"

    proc = popen(
        [
            sys.executable,
            "-m",
            "core.run_proxy",
            "gcs",
            "--suite",
            suite,
            "--gcs-secret-file",
            str(key_path),
            "--control-manual",
            "--status-file",
            str(PROXY_STATUS_PATH),
            "--json-out",
            str(PROXY_SUMMARY_PATH),
        ],
        stdin=subprocess.PIPE,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
    )
    if proc.stdin is None:
        raise RuntimeError("GCS proxy did not expose stdin for manual control")
    return proc, log_handle


class UdpEcho(threading.Thread):
    def __init__(
        self,
        bind_host: str,
        recv_port: int,
        send_host: str,
        send_port: int,
        stop_event: threading.Event,
        publisher: Optional[TelemetryPublisher],
    ) -> None:
        super().__init__(daemon=True)
        self.bind_host = bind_host
        self.recv_port = recv_port
        self.send_host = send_host
        self.send_port = send_port
        self.stop_event = stop_event
        self.publisher = publisher
        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        try:
            sndbuf = int(os.getenv("GCS_SOCK_SNDBUF", str(8 << 20)))
            rcvbuf = int(os.getenv("GCS_SOCK_RCVBUF", str(8 << 20)))
            self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, rcvbuf)
            self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, sndbuf)
        except Exception:
            pass
        self.rx_sock.bind((self.bind_host, self.recv_port))

    def run(self) -> None:
        print(
            f"[gcs-follower] UDP echo listening {self.bind_host}:{self.recv_port} -> {self.send_host}:{self.send_port}",
            flush=True,
        )
        self.rx_sock.settimeout(0.001)
        while not self.stop_event.is_set():
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                recv_ns = time.time_ns()
                annotated = self._annotate_packet(data, recv_ns)
                self.tx_sock.sendto(annotated, (self.send_host, self.send_port))
                if self.publisher:
                    self.publisher.publish(
                        "udp_echo",
                        {
                            "recv_timestamp_ns": recv_ns,
                            "payload_len": len(data),
                            "peer": f"{addr[0]}:{addr[1]}",
                        },
                    )
            except socket.timeout:
                continue
            except Exception as exc:
                print(f"[gcs-follower] UDP echo error: {exc}", flush=True)
        self.rx_sock.close()
        self.tx_sock.close()

    @staticmethod
    def _annotate_packet(data: bytes, recv_ns: int) -> bytes:
        if len(data) >= 20:
            return data[:-8] + recv_ns.to_bytes(8, "big")
        return data + recv_ns.to_bytes(8, "big")


class ControlServer(threading.Thread):
    def __init__(self, host: str, port: int, state: dict, lock: threading.RLock):
        super().__init__(daemon=True)
        self.host = host
        self.port = port
        self.state = state
        self.lock = lock
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.sock.bind((self.host, self.port))
        self.sock.listen(5)

    def run(self) -> None:
        print(f"[gcs-follower] control listening on {self.host}:{self.port}", flush=True)
        while not self.state["stop_event"].is_set():
            try:
                self.sock.settimeout(0.5)
                conn, _addr = self.sock.accept()
            except socket.timeout:
                continue
            threading.Thread(target=self.handle, args=(conn,), daemon=True).start()
        self.sock.close()

    def handle(self, conn: socket.socket) -> None:
        try:
            line = conn.makefile().readline()
            request = json.loads(line.strip()) if line else {}
        except Exception:
            request = {}
        try:
            cmd = request.get("cmd")
            if cmd == "ping":
                self._send(conn, {"ok": True, "ts": ts()})
                return
            if cmd == "timesync":
                t1 = int(request.get("t1_ns", 0))
                t2 = time.time_ns()
                t3 = time.time_ns()
                self._send(conn, {"ok": True, "t1_ns": t1, "t2_ns": t2, "t3_ns": t3})
                return
            if cmd == "status":
                with self.lock:
                    proxy: Optional[subprocess.Popen] = self.state.get("proxy")
                    running = bool(proxy and proxy.poll() is None)
                    suite = self.state.get("suite")
                    pending = self.state.get("pending_suite")
                counters = read_proxy_counters()
                status = read_proxy_status()
                self._send(
                    conn,
                    {
                        "ok": True,
                        "suite": suite,
                        "pending_suite": pending,
                        "running": running,
                        "counters": counters,
                        "status": status,
                    },
                )
                telemetry: Optional[TelemetryPublisher] = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "status_reply",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "running": running,
                        },
                    )
                return
            if cmd == "mark":
                suite = request.get("suite")
                if not suite:
                    self._send(conn, {"ok": False, "error": "missing suite"})
                    return
                with self.lock:
                    current = self.state.get("suite")
                    self.state["prev_suite"] = current
                    self.state["pending_suite"] = suite
                    self.state["suite"] = suite
                write_marker(suite)
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "mark",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "prev_suite": current,
                        },
                    )
                self._send(conn, {"ok": True, "marked": suite})
                return
            if cmd == "rekey":
                suite = request.get("suite")
                if not suite:
                    self._send(conn, {"ok": False, "error": "missing suite"})
                    return
                with self.lock:
                    proxy: Optional[subprocess.Popen] = self.state.get("proxy")
                    stdin = self.state.get("proxy_stdin")
                if not proxy or proxy.poll() is not None or stdin is None:
                    self._send(conn, {"ok": False, "error": "proxy_not_running"})
                    return
                try:
                    stdin.write(suite + "\n")
                    stdin.flush()
                except Exception as exc:
                    self._send(conn, {"ok": False, "error": f"stdin_write_failed: {exc}"})
                    return
                with self.lock:
                    self.state["pending_suite"] = suite
                    self.state["last_rekey_started_ns"] = time.time_ns()
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "rekey_initiated",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                        },
                    )
                self._send(conn, {"ok": True, "suite": suite})
                return
            if cmd == "rekey_complete":
                status_value = str(request.get("status", "ok"))
                suite = request.get("suite")
                telemetry = self.state.get("telemetry")
                with self.lock:
                    if status_value.lower() == "ok" and suite:
                        self.state["suite"] = suite
                    self.state.pop("pending_suite", None)
                if telemetry:
                    telemetry.publish(
                        "rekey_complete",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "status": status_value,
                        },
                    )
                self._send(conn, {"ok": True})
                return
            if cmd == "schedule_mark":
                suite = request.get("suite")
                t0_ns = int(request.get("t0_ns", 0))
                if not suite or not t0_ns:
                    self._send(conn, {"ok": False, "error": "missing suite or t0_ns"})
                    return

                def _do_mark() -> None:
                    delay = max(0.0, (t0_ns - time.time_ns()) / 1e9)
                    if delay:
                        time.sleep(delay)
                    with self.lock:
                        current = self.state.get("suite")
                        self.state["prev_suite"] = current
                        self.state["pending_suite"] = suite
                        self.state["suite"] = suite
                    write_marker(suite)
                    telemetry_inner = self.state.get("telemetry")
                    if telemetry_inner:
                        telemetry_inner.publish(
                            "scheduled_mark",
                            {
                                "timestamp_ns": time.time_ns(),
                                "suite": suite,
                                "prev_suite": current,
                            },
                        )

                threading.Thread(target=_do_mark, daemon=True).start()
                self._send(conn, {"ok": True, "scheduled": suite, "t0_ns": t0_ns})
                return
            if cmd == "stop":
                self.state["stop_event"].set()
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish("stop", {"timestamp_ns": time.time_ns()})
                self._send(conn, {"ok": True, "stopping": True})
                return
            self._send(conn, {"ok": False, "error": "unknown_cmd"})
        finally:
            try:
                conn.close()
            except Exception:
                pass

    @staticmethod
    def _send(conn: socket.socket, obj: dict) -> None:
        conn.sendall((json.dumps(obj) + "\n").encode())


def main() -> None:
    OUTDIR.mkdir(parents=True, exist_ok=True)
    MARK_DIR.mkdir(parents=True, exist_ok=True)

    default_suite = discover_initial_suite()

    parser = argparse.ArgumentParser(description="GCS follower driven by core configuration")
    parser.add_argument(
        "--initial-suite",
        default=default_suite,
        help="Initial suite to launch (default: discover from config/secrets)",
    )
    parser.add_argument(
        "--session-id",
        help="Session identifier for telemetry",
    )
    parser.add_argument(
        "--telemetry-host",
        default=TELEMETRY_DEFAULT_HOST,
        help="Telemetry collector host (default: drone host)",
    )
    parser.add_argument(
        "--telemetry-port",
        type=int,
        default=TELEMETRY_DEFAULT_PORT,
        help="Telemetry collector TCP port",
    )
    parser.add_argument(
        "--disable-telemetry",
        action="store_true",
        help="Disable telemetry publisher",
    )
    parser.add_argument(
        "--disable-echo",
        action="store_true",
        help="Disable UDP echo service",
    )
    args = parser.parse_args()

    initial_suite = args.initial_suite
    session_id = args.session_id or f"session_{int(time.time())}"
    stop_event = threading.Event()

    telemetry: Optional[TelemetryPublisher] = None
    if not args.disable_telemetry:
        telemetry = TelemetryPublisher(args.telemetry_host, args.telemetry_port, session_id)
        telemetry.start()

    proxy = None
    log_handle = None
    echo_thread: Optional[UdpEcho] = None
    control_thread: Optional[ControlServer] = None

    try:
        proxy, log_handle = start_gcs_proxy(initial_suite)
        if proxy.poll() is not None:
            raise RuntimeError(f"gcs proxy exited immediately with {proxy.returncode}")

        if not args.disable_telemetry and telemetry:
            telemetry.publish(
                "proxy_started",
                {
                    "timestamp_ns": time.time_ns(),
                    "suite": initial_suite,
                    "proxy_pid": proxy.pid,
                },
            )

        if not args.disable_echo:
            echo_thread = UdpEcho(
                APP_BIND_HOST,
                APP_RECV_PORT,
                APP_SEND_HOST,
                APP_SEND_PORT,
                stop_event,
                telemetry,
            )
            echo_thread.start()

        state = {
            "suite": initial_suite,
            "pending_suite": None,
            "prev_suite": None,
            "proxy": proxy,
            "proxy_stdin": proxy.stdin,
            "stop_event": stop_event,
            "telemetry": telemetry,
        }
        lock = threading.RLock()
        control_thread = ControlServer(CONTROL_HOST, CONTROL_PORT, state, lock)
        control_thread.start()

        print(f"[gcs-follower] awaiting stop signal (session {session_id})", flush=True)
        while not stop_event.is_set():
            if proxy.poll() is not None:
                print(f"[gcs-follower] proxy exited with {proxy.returncode}", flush=True)
                stop_event.set()
                break
            time.sleep(0.5)
    except KeyboardInterrupt:
        stop_event.set()
    finally:
        try:
            ctl_sock = socket.create_connection((CONTROL_HOST, CONTROL_PORT), timeout=1.0)
            ctl_sock.sendall((json.dumps({"cmd": "stop"}) + "\n").encode())
            ctl_sock.close()
        except Exception:
            pass

        if control_thread and control_thread.is_alive():
            control_thread.join(timeout=1.5)

        stop_event.set()
        if echo_thread and echo_thread.is_alive():
            echo_thread.join(timeout=1.0)

        if proxy and proxy.stdin:
            try:
                proxy.stdin.write("quit\n")
                proxy.stdin.flush()
            except Exception:
                pass
        if proxy:
            try:
                proxy.wait(timeout=5)
            except Exception:
                killtree(proxy)

        if log_handle:
            try:
                log_handle.close()
            except Exception:
                pass

        if telemetry:
            telemetry.stop()


if __name__ == "__main__":
    main()
